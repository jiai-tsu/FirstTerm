{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_20210608.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IPDLYbOSrD_7",
        "UDOMYkT1rs9m",
        "BRUzDNKlxROJ",
        "2MbKJY38Puy9",
        "e1_Y75QXJS6h",
        "iYn4MdZnKCey",
        "THY-sZMiQ4UV",
        "0FMYgY_mPfTi",
        "PKY_iPSPNWoj",
        "Jd-3GCUEiKtv",
        "mWtinsGDPJlV",
        "Rw1fkAczTQYh",
        "dZrd4CdjR-Fp",
        "P4M_vIbUi7c0",
        "k6qC-SbjK0yW",
        "TAAp_xGjbj95",
        "Mo3aPa_1bkbN",
        "4j_GRSXJ0_Tm",
        "vgxdCgR60_To",
        "hvX8sKsfMaio",
        "p9M-HoZG0_Tt",
        "aKUZnDiqQrAh",
        "Yc8n8aCfbkha"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnijj-lZg_s"
      },
      "source": [
        "## 49. 敵対的生成ネットワーク（GAN : Generative Adversarial Network）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPDLYbOSrD_7"
      },
      "source": [
        "### <font color=blue>**1.** </font> 深層畳み込みGAN（DCGAN）\n",
        "<font color=red size=5>**GPU推奨**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDOMYkT1rs9m"
      },
      "source": [
        "#### <font color=green>**1.1.** </font> kerasで実装してみた"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REHsliPRFxYQ"
      },
      "source": [
        "## 出典 : https://qiita.com/takuto512/items/c8fd8eb1cdc7b6689798"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwxk31p3hRns"
      },
      "source": [
        "'''Main'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip, datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsdcT-y4hRlo"
      },
      "source": [
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import Grid\n",
        "\n",
        "%matplotlib inline\n",
        "color = sns.color_palette()\n",
        "sns.set(\"talk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0prGM_M4hRji"
      },
      "source": [
        "'''Data Prep and Model Evaluation'''\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
        "\n",
        "#from keras.utils import to_categorical\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf6Nnl0zCSsH"
      },
      "source": [
        "'''Algos'''\n",
        "import lightgbm as lgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l54DkDUXhQ6b"
      },
      "source": [
        "'''TensorFlow and Keras'''\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import BatchNormalization, Input, Lambda\n",
        "from keras.layers import Embedding, Flatten, dot\n",
        "from keras import regularizers\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4PShz3rCSp5"
      },
      "source": [
        "# 学習データとテストデータに分割したデータ\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "\n",
        "# ピクセルの値を 0~1 の間に正規化\n",
        "x_train= x_train / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4H3OrCtCSlQ"
      },
      "source": [
        "#DCGANのクラス\n",
        "class DCGAN(object):\n",
        "  #初期化\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "\n",
        "    #生成ネットワーク\n",
        "    #100*1*1の行列をデータセットの画像と同じ1*28*28にする\n",
        "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8,\n",
        "                  window=5, input_dim=100, output_depth=1):\n",
        "        if self.G:\n",
        "            return self.G\n",
        "        self.G = Sequential()\n",
        "\n",
        "        #100*1*1 → 256*7*7\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "\n",
        "        #256*7*7 → 128*14*14\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        #128*14*14 → 64*28*28\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        #64*28*28→32*28*28\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        #1*28*28\n",
        "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
        "        #各ピクセルを0～1の間の値にする\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "\n",
        "    #識別ネットワーク\n",
        "    #28*28*1の画像が本物かどうかを見分ける\n",
        "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
        "        if self.D:\n",
        "            return self.D\n",
        "\n",
        "        self.D = Sequential()\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "\n",
        "      #28*28*1 → 14*14*64\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "      #14*14*64 → 7*7*128\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "      #7*7*128 → 4*4*256\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        #4*4*512 → 4*4*512 ####ただしあっているか確認###\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        #フラット化してsigmoidで分類\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    #識別モデル\n",
        "    def discriminator_model(self):\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "        optimizer = RMSprop(learning_rate=0.0002, decay=6e-8) ### lr -> learning_rate\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy',\n",
        "                        optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    #生成モデル\n",
        "    def adversarial_model(self):\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "        optimizer = RMSprop(learning_rate=0.0001, decay=3e-8) ### lr -> learning_rate\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy',\n",
        "                        optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.AM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz0KRnFoCSi8"
      },
      "source": [
        "#MNISTのデータにDCGANを適用するクラス\n",
        "class MNIST_DCGAN(object):\n",
        "    #初期化\n",
        "    def __init__(self, x_train):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = x_train\n",
        "\n",
        "        #DCGANの識別、敵対的生成モデルの定義\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    #訓練用の関数\n",
        "    #train_on_batchは各batchごとに学習している。出力はlossとacc\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        noise_input = None\n",
        "\n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "\n",
        "        for i in range(train_steps):\n",
        "            #訓練用のデータをbatch_sizeだけランダムに取り出す\n",
        "            images_train = self.x_train[np.random.randint(0,self.x_train.shape[0], size=batch_size), :, :, :] \n",
        "\n",
        "            # 100*1*1のノイズをbatch sizeだけ生み出して偽画像とする\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "\n",
        "            #生成画像を学習させる\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            #訓練データを1に、生成データを0にする\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "\n",
        "            #識別モデルを学習させる\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "\n",
        "            #生成&識別モデルを学習させる\n",
        "            #生成モデルの学習はここでのみ行われる\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "\n",
        "            #訓練データと生成モデルのlossと精度\n",
        "            #D lossは生成された画像と実際の画像のときのlossとacc\n",
        "            #A lossはadversarialで生み出された画像を1としたときのlossとacc\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            \n",
        "            #if i % 50 == 0:\n",
        "            print(log_mesg)\n",
        "\n",
        "            #save_intervalごとにデータを保存する\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True,\n",
        "                                     samples=noise_input.shape[0],\n",
        "                                     noise=noise_input, step=(i+1))\n",
        "\n",
        "    #訓練結果をプロットする\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16,\n",
        "                    noise=None, step=0):\n",
        "        current_path = os.getcwd()\n",
        "        file = os.path.sep.join([\"\",\"data\", 'images', 'chapter12', 'synthetic_mnist', ''])\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(current_path+file+filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FVpmIeFCSbs"
      },
      "source": [
        "Instance = MNIST_DCGAN(x_train)\n",
        "Instance.train(train_steps=200)\n",
        "\n",
        "## GPUで所要時間2分くらい"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvhFvzoyCSZI"
      },
      "source": [
        "Instance.plot_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMVjlZxoPqXB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHVlT-a_PqQ3"
      },
      "source": [
        "# 出典元のepoch数\n",
        "Instance.train(train_steps=2000)\n",
        "Instance.plot_images()\n",
        "\n",
        "## 8分３６秒かかった"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0d_PZldTHnr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ9-iVC-THg-"
      },
      "source": [
        "## さらに２０００epoch学習させてみる\n",
        "Instance.train(train_steps=2000)\n",
        "Instance.plot_images()\n",
        "\n",
        "## 8分41秒かかった"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_OYqN9jwGwl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpWWmOoawx2b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "#### <font color=green>**1.2.** </font> TensorFlowの公式チュートリアル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-dEqtfQxH3r"
      },
      "source": [
        "## https://www.tensorflow.org/tutorials/generative/dcgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRUzDNKlxROJ"
      },
      "source": [
        "##### What are GANs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVRMOuNAxUyl"
      },
      "source": [
        "[Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A *generator* (\"the artist\") learns to create images that look real, while a *discriminator* (\"the art critic\") learns to tell real images apart from fakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xtffXbxUua"
      },
      "source": [
        "![A diagram of a generator and discriminator](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gan1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atEyIfc2xaZ1"
      },
      "source": [
        "During training, the *generator* progressively becomes better at creating images that look real, while the *discriminator* becomes better at telling them apart. The process reaches equilibrium when the *discriminator* can no longer distinguish real images from fakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MbKJY38Puy9"
      },
      "source": [
        "![A second diagram of a generator and discriminator](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gan2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGM40o8Cxfe8"
      },
      "source": [
        "This notebook demonstrates this process on the MNIST dataset. The following animation shows a series of images produced by the *generator* as it was trained for 50 epochs. The images begin as random noise, and increasingly resemble hand written digits over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu7yyVpDxfKu"
      },
      "source": [
        "![sample output](https://tensorflow.org/images/gan/dcgan.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nOBRlvrxeox"
      },
      "source": [
        "To learn more about GANs, see MIT's [Intro to Deep Learning](http://introtodeeplearning.com/) course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fli6GuIxkPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "##### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZKbyU2-AiY-"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx-zNbLqB4K8"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTlj4YdCip_"
      },
      "source": [
        "# To generate GIFs\n",
        "!pip install imageio\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW5Asv8wxr84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "##### Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4fYMGxGhrna"
      },
      "source": [
        "# You will use the MNIST dataset to train the generator and the discriminator.\n",
        "# The generator will generate handwritten digits resembling the MNIST data.\n",
        "\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFC2ghIdiZYE"
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4PIDhoDLbsZ"
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yKCCQOoJ7cn"
      },
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmFwmAduxvQl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "##### Create the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAJ8ar0Zxxwi"
      },
      "source": [
        "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtVC2dODx1hN"
      },
      "source": [
        "### The Generator\n",
        "# The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise).\n",
        "# Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1.\n",
        "# Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bpTcDqoLWjY"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Use the (as yet untrained) generator to create an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7jcC7TdPTG"
      },
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCKNO9thx6df"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFa6uaSfx7Dk"
      },
      "source": [
        "### The Discriminator\n",
        "# The discriminator is a CNN-based image classifier."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FJ6X3rsyAYC"
      },
      "source": [
        "# Use the (as yet untrained) discriminator to classify the generated images as real or fake.\n",
        "# The model will be trained to output positive values for real images, and negative values for fake images."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDkA05NE6QMs"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsrIdfv0x-j7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "##### Define the loss and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "source": [
        "# Define loss functions and optimizers for both models.\n",
        "\n",
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toVoj89LyIsD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "##### Discriminator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMbHM0gRyKtX"
      },
      "source": [
        "# This method quantifies how well the discriminator is able to distinguish real images from fakes.\n",
        "# It compares the discriminator's predictions on real images to an array of 1s,\n",
        "# and the discriminator's predictions on fake (generated) images to an array of 0s."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fLqWo00yOeH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "##### Generator loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6EW3fhMyPep"
      },
      "source": [
        "# The generator's loss quantifies how well it was able to trick the discriminator.\n",
        "# Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).\n",
        "# Here, compare the discriminators decisions on the generated images to an array of 1s."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhr3A1LZyViM"
      },
      "source": [
        "# The discriminator and the generator optimizers are different since you will train two networks separately."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvpTeSONyPWW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtinsGDPJlV"
      },
      "source": [
        "##### Save checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTUM8FbByZNL"
      },
      "source": [
        "# This notebook also demonstrates how to save and restore models,\n",
        "# which can be helpful in case a long running training task is interrupted."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1w-7s2POEy"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43HY7Tayyafu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "##### Define the training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "source": [
        "#EPOCHS = 50\n",
        "EPOCHS = 1\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yESuUv_QyeCa"
      },
      "source": [
        "# The training loop begins with generator receiving a random seed as input.\n",
        "# That seed is used to produce an image.\n",
        "# The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator).\n",
        "# The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t5ibNo05jCB"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQnlQFxsyoI3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb3LOeSpykGg"
      },
      "source": [
        "## **Generate and save images**"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f_Oovm4yn4t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "##### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVG4p0YWyqil"
      },
      "source": [
        "# Call the `train()` method defined above to train the generator and discriminator simultaneously.\n",
        "# Note, training GANs can be tricky.\n",
        "# It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRgJPc3uyqeA"
      },
      "source": [
        "# At the beginning of the training, the generated images look like random noise.\n",
        "# As training progresses, the generated digits will look increasingly real.\n",
        "# After about 50 epochs, they resemble MNIST digits.\n",
        "# This may take about one minute / epoch with the default settings on Colab."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3UN0SLLY2l"
      },
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90qBGWp5y0JY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhXsd0srPo8c"
      },
      "source": [
        "# Restore the latest checkpoint.\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aPjUXqAyx9y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4M_vIbUi7c0"
      },
      "source": [
        "##### Create a GIF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfO5wCdclHGL"
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x3q9_Oe5q0A"
      },
      "source": [
        "display_image(EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGKQgENQ8lEI"
      },
      "source": [
        "# Use `imageio` to create an animated gif using the images saved during training.\n",
        "\n",
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBwyU6t2Wf3g"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cRst6r4y7EI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6qC-SbjK0yW"
      },
      "source": [
        "##### Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjjkT9KAK6H7"
      },
      "source": [
        "This tutorial has shown the complete code necessary to write and train a GAN. As a next step, you might like to experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset). To learn more about GANs see the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wChvx2-twxvs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAAp_xGjbj95"
      },
      "source": [
        "#### <font color=green>**1.3.** </font> Celeb Faces Attributes (CelebA) datasetでやってみる（kerasのサンプルコード）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR8OSJ3Dbj96"
      },
      "source": [
        "## 出典 : https://keras.io/examples/generative/dcgan_overriding_train_step/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkoNhRBHHdL"
      },
      "source": [
        "## Setup\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jee6JsQXHHdM"
      },
      "source": [
        "## Prepare CelebA data\n",
        "# We'll use face images from the CelebA dataset, resized to 64x64.\n",
        "\n",
        "os.makedirs(\"celeba_gan\")\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slnP0ukJHHdO"
      },
      "source": [
        "# Create a dataset from our folder, and rescale the images to the [0-1] range:\n",
        "\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    \"celeba_gan\", label_mode=None, image_size=(64, 64), batch_size=32\n",
        ")\n",
        "dataset = dataset.map(lambda x: x / 255.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9_UTlj60YoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zn8qz5LHHdP"
      },
      "source": [
        "# Let's display a sample image:\n",
        "\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HTb--NZNifJ"
      },
      "source": [
        "for x in dataset:\n",
        "    plt.figure(figsize=(32, 32))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow((x.numpy()).astype(\"int32\")[i])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RTXsiKa0X6C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjuqsNhwHHdQ"
      },
      "source": [
        "## Create the discriminator\n",
        "# It maps a 64x64 image to a binary classification score.\n",
        "\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQv38Bud0bIm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggttWWhHHdR"
      },
      "source": [
        "## Create the generator\n",
        "# It mirrors the discriminator, replacing `Conv2D` layers with `Conv2DTranspose` layers.\n",
        "\n",
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1t2ciT30e1G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfF9aUPcHHdS"
      },
      "source": [
        "## Override `train_step`\n",
        "\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        # Sample random points in the latent space\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Decode them to fake images\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Combine them with real images\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "        # Assemble labels discriminating real from fake images\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "        # Add random noise to the labels - important trick!\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        # Assemble labels that say \"all real images\"\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEn6aQNE0jyX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edUV8Gq6HHdU"
      },
      "source": [
        "## Create a callback that periodically saves generated images\n",
        "\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.preprocessing.image.array_to_img(generated_images[i])\n",
        "            img.save(\"generated_img_%03d_%d.png\" % (epoch, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5klc-Aa0mJW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT7sucGEHHdV"
      },
      "source": [
        "## Train the end-to-end model\n",
        "\n",
        "epochs = 1  # In practice, use ~100 epochs\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")\n",
        "\n",
        "## GPUでも 1epoch 30分くらいかかるっぽい"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjDjmcWk0tIB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-tYf08j0ttj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEdH9NNaHHdW"
      },
      "source": [
        "Some of the last generated images around epoch 30\n",
        "(results keep improving after that):\n",
        "\n",
        "![results](https://i.imgur.com/h5MtQZ7l.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfGnJRqhbj96"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee2ZE0YJbj96"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo3aPa_1bkbN"
      },
      "source": [
        "### <font color=blue>**2.** </font> スタイル変換（CycleGAN）\n",
        "<font color=red size=5>**GPUでも 1epoch 8分程度かかる**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neKYcyhFbkbO"
      },
      "source": [
        "## 出典 : https://www.tensorflow.org/tutorials/generative/cyclegan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if5N6WtW1YhG"
      },
      "source": [
        "This notebook demonstrates unpaired image to image translation using conditional GAN's, as described in [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593), also known as CycleGAN. The paper proposes a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zatgi_Ev1Xwl"
      },
      "source": [
        "This notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYhXECh1XkO"
      },
      "source": [
        "CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj7QwWti1igI"
      },
      "source": [
        "This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITZuApL56Mny"
      },
      "source": [
        "![Output Image 1](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/horse2zebra_1.png?raw=1)\n",
        "![Output Image 2](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/horse2zebra_2.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUtM0Liy1kXJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j_GRSXJ0_Tm"
      },
      "source": [
        "#### Set up the input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fGHWOKPX4ta"
      },
      "source": [
        "Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ1ROiQxJ-vY"
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhSsUx9Nyb3t"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIid6Zem0_To"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYf2-2fH1nZo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgxdCgR60_To"
      },
      "source": [
        "#### Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRC_a0hm1rUp"
      },
      "source": [
        "This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/catalog/cycle_gan). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU8HaeA01rKP"
      },
      "source": [
        "As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2od0IZQ1qrp"
      },
      "source": [
        "This is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)\n",
        "\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuGVPOo7Cce0"
      },
      "source": [
        "dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
        "                              with_info=True, as_supervised=True)\n",
        "\n",
        "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
        "test_horses, test_zebras = dataset['testA'], dataset['testB']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CbTEt448b4R"
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3IwqhiIszt"
      },
      "source": [
        "def random_crop(image):\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhR2cgbLKWW"
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image / 127.5) - 1\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVQOjcPVLrUc"
      },
      "source": [
        "def random_jitter(image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  image = tf.image.resize(image, [286, 286],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  image = random_crop(image)\n",
        "\n",
        "  # random mirroring\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyaP4hLJ8b4W"
      },
      "source": [
        "def preprocess_image_train(image, label):\n",
        "  image = random_jitter(image)\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3Z6D_zKSru"
      },
      "source": [
        "def preprocess_image_test(image, label):\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsajGXxd5JkZ"
      },
      "source": [
        "train_horses = train_horses.map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "train_zebras = train_zebras.map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_horses = test_horses.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_zebras = test_zebras.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3MhJ3zVLPan"
      },
      "source": [
        "sample_horse = next(iter(train_horses))\n",
        "sample_zebra = next(iter(train_zebras))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pOYjMk_KfIB"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Horse')\n",
        "plt.imshow(sample_horse[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Horse with random jitter')\n",
        "plt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KJyB9ENLb2y"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Zebra')\n",
        "plt.imshow(sample_zebra[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Zebra with random jitter')\n",
        "plt.imshow(random_jitter(sample_zebra[0]) * 0.5 + 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS65whTK1xLi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvX8sKsfMaio"
      },
      "source": [
        "#### Import and reuse the Pix2Pix models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcUK0Hup10Z3"
      },
      "source": [
        "Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvYxBdYI10Ph"
      },
      "source": [
        "The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n",
        "\n",
        "* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n",
        "* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPBg43V710En"
      },
      "source": [
        "There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n",
        "\n",
        "* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n",
        "* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n",
        "* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n",
        "* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGrL73uCd-_M"
      },
      "source": [
        "![Cyclegan model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cyclegan_model.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQJ2pQtd17Hz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ju9Wyw87MRW"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
        "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
        "\n",
        "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
        "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDaGZ3WpZUyw"
      },
      "source": [
        "to_zebra = generator_g(sample_horse)\n",
        "to_horse = generator_f(sample_zebra)\n",
        "plt.figure(figsize=(8, 8))\n",
        "contrast = 8\n",
        "\n",
        "imgs = [sample_horse, to_zebra, sample_zebra, to_horse]\n",
        "title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "  plt.subplot(2, 2, i+1)\n",
        "  plt.title(title[i])\n",
        "  if i % 2 == 0:\n",
        "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
        "  else:\n",
        "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5MhJmxyZiy9"
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Is a real zebra?')\n",
        "plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Is a real horse?')\n",
        "plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUCWr0nH19WL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9M-HoZG0_Tt"
      },
      "source": [
        "#### Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRqt02lupRn8"
      },
      "source": [
        "In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsqljau62HrK"
      },
      "source": [
        "The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#build_the_generator)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqJCha9r2IPH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyhxTuvJyIHV"
      },
      "source": [
        "LAMBDA = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Xbz5OaLj5C"
      },
      "source": [
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnj4Cy-q0_Tu"
      },
      "source": [
        "def discriminator_loss(real, generated):\n",
        "  real_loss = loss_obj(tf.ones_like(real), real)\n",
        "\n",
        "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss * 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grVsXpuC0_Tu"
      },
      "source": [
        "def generator_loss(generated):\n",
        "  return loss_obj(tf.ones_like(generated), generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DYBFwf62Oi-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nk6_9C92QPM"
      },
      "source": [
        "Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-L5ENzH2QE1"
      },
      "source": [
        "In cycle consistency loss, \n",
        "\n",
        "* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n",
        "* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n",
        "* Mean absolute error is calculated between $X$ and $\\hat{X}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKhQhkTK2P5F"
      },
      "source": [
        "$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n",
        "\n",
        "$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iIWQzVF7f9e"
      },
      "source": [
        "![Cycle loss](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cycle_loss.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1I98G32PDa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMpVGj_sW6Vo"
      },
      "source": [
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "  \n",
        "  return LAMBDA * loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9H-XRoZ2Yu7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhV7gPC2ZjS"
      },
      "source": [
        "As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-tJL-fX0Mq7"
      },
      "source": [
        "If you run the zebra-to-horse model on a horse or the horse-to-zebra model on a zebra, it should not modify the image much since the image already contains the target class.\n",
        "\n",
        "$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zflwsNsx2dHy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ywEH680Aud"
      },
      "source": [
        "def identity_loss(real_image, same_image):\n",
        "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "  return LAMBDA * 0.5 * loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyShwoR-0_Tv"
      },
      "source": [
        "# Initialize the optimizers for all the generators and the discriminators.\n",
        "\n",
        "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U8Awi-k2ejp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKUZnDiqQrAh"
      },
      "source": [
        "#### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnftd5sQsv6"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
        "                           generator_f=generator_f,\n",
        "                           discriminator_x=discriminator_x,\n",
        "                           discriminator_y=discriminator_y,\n",
        "                           generator_g_optimizer=generator_g_optimizer,\n",
        "                           generator_f_optimizer=generator_f_optimizer,\n",
        "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_bGGBV-2jtf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN95H5QX0_Tw"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfBwXace2omg"
      },
      "source": [
        "Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. \\\n",
        "Predictions may be less accurate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhPVZLAa2ljc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEjX6LoH0_Tw"
      },
      "source": [
        "EPOCHS = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqX8pwiO0_Tw"
      },
      "source": [
        "def generate_images(model, test_input):\n",
        "  prediction = model(test_input)\n",
        "    \n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  display_list = [test_input[0], prediction[0]]\n",
        "  title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "  for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDO489Cu2tLH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE47ERn5fyLC"
      },
      "source": [
        "Even though the training loop looks complicated, it consists of four basic steps:\n",
        "\n",
        "* Get the predictions.\n",
        "* Calculate the loss.\n",
        "* Calculate the gradients using backpropagation.\n",
        "* Apply the gradients to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBKUV2sKXDbY"
      },
      "source": [
        "@tf.function\n",
        "def train_step(real_x, real_y):\n",
        "  # persistent is set to True because the tape is used more than\n",
        "  # once to calculate the gradients.\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Generator G translates X -> Y\n",
        "    # Generator F translates Y -> X.\n",
        "    \n",
        "    fake_y = generator_g(real_x, training=True)\n",
        "    cycled_x = generator_f(fake_y, training=True)\n",
        "\n",
        "    fake_x = generator_f(real_y, training=True)\n",
        "    cycled_y = generator_g(fake_x, training=True)\n",
        "\n",
        "    # same_x and same_y are used for identity loss.\n",
        "    same_x = generator_f(real_x, training=True)\n",
        "    same_y = generator_g(real_y, training=True)\n",
        "\n",
        "    disc_real_x = discriminator_x(real_x, training=True)\n",
        "    disc_real_y = discriminator_y(real_y, training=True)\n",
        "\n",
        "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
        "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
        "\n",
        "    # calculate the loss\n",
        "    gen_g_loss = generator_loss(disc_fake_y)\n",
        "    gen_f_loss = generator_loss(disc_fake_x)\n",
        "    \n",
        "    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
        "    \n",
        "    # Total generator loss = adversarial loss + cycle loss\n",
        "    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
        "    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
        "\n",
        "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "  \n",
        "  # Calculate the gradients for generator and discriminator\n",
        "  generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
        "                                        generator_g.trainable_variables)\n",
        "  generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
        "                                        generator_f.trainable_variables)\n",
        "  \n",
        "  discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
        "                                            discriminator_x.trainable_variables)\n",
        "  discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
        "                                            discriminator_y.trainable_variables)\n",
        "  \n",
        "  # Apply the gradients to the optimizer\n",
        "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
        "                                            generator_g.trainable_variables))\n",
        "\n",
        "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
        "                                            generator_f.trainable_variables))\n",
        "  \n",
        "  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
        "                                                discriminator_x.trainable_variables))\n",
        "  \n",
        "  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
        "                                                discriminator_y.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvliZ8E0_Tx"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  n = 0\n",
        "  for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n",
        "    train_step(image_x, image_y)\n",
        "    if n % 10 == 0:\n",
        "      print ('.', end='')\n",
        "    n += 1\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  # Using a consistent image (sample_horse) so that the progress of the model\n",
        "  # is clearly visible.\n",
        "  generate_images(generator_g, sample_horse)\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                      time.time()-start))\n",
        "\n",
        "## @GPU : Time taken for epoch 1 is 479.8142921924591 sec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsiuCodh2vVc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RGysMU_BZhx"
      },
      "source": [
        "#### Generate using test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgSnmy2nqSP"
      },
      "source": [
        "# Run the trained model on the test dataset\n",
        "for inp in test_horses.take(5):\n",
        "  generate_images(generator_g, inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkQZ6F1f2waC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABGiHY6fE02b"
      },
      "source": [
        "#### Next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqyhCY7m2zTU"
      },
      "source": [
        "This tutorial has shown how to implement CycleGAN starting from the generator and discriminator implemented in the [Pix2Pix](https://www.tensorflow.org/tutorials/generative/pix2pix) tutorial. As a next step, you could try using a different dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/cycle_gan). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqh7Rqnf21vi"
      },
      "source": [
        "You could also train for a larger number of epochs to improve the results, or you could implement the modified ResNet generator used in the [paper](https://arxiv.org/abs/1703.10593) instead of the U-Net generator used here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaobSYnvbkbP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adoKBeIRbkbQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc8n8aCfbkha"
      },
      "source": [
        "### <font color=blue>**3.** </font> 異常検知（EfficientGAN）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVbSM38o70R3"
      },
      "source": [
        "## 出典 : https://qiita.com/key9asm4/items/bfcdf86d573af47646d0\n",
        "## https://github.com/asm94/EfficientGAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATcmtmNZ7ivP"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.layers import Input, Dense, LeakyReLU, Concatenate, Dropout\n",
        "from tensorflow.python.keras.models import Model\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0fz9IDD68Zz"
      },
      "source": [
        "class EfficientGAN(object):\n",
        "    def __init__(self, input_dim=0, latent_dim=32):\n",
        "        self.input_dim = int(input_dim)\n",
        "        self.latent_dim = int(latent_dim)\n",
        "      \n",
        "    #Train model\n",
        "    def fit(self, X_train, epochs=50, batch_size=50, loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, ### lr -> learning_rate\n",
        "                                               beta_1=0.5), test=tuple(), early_stop_num=50,\n",
        "            verbose=1):\n",
        "        \n",
        "        #Convert training-data to numpy format\n",
        "        X_train = np.array(X_train)\n",
        "        \n",
        "        #If \"input_dim\" is not greater than 1, it is set to the number of features in the training-data\n",
        "        if not self.input_dim >= 1: self.input_dim = X_train.shape[1]\n",
        "        \n",
        "        #Define model for Discriminator\n",
        "        self.dis = self.get_discriminator()\n",
        "        self.dis.compile(loss=loss, optimizer=optimizer)        \n",
        "        \n",
        "        #Define model to train Encoder\n",
        "        self.enc = self.get_encoder()\n",
        "        x = Input(shape=(self.input_dim,))\n",
        "        z_gen = self.enc(x)\n",
        "        valid = self.dis([x, z_gen])\n",
        "        enc_dis = Model(inputs=x, outputs=valid, name='enc_to_dis')\n",
        "        enc_dis.compile(loss=loss, optimizer=optimizer) \n",
        "        \n",
        "        #Define model to train Generator\n",
        "        self.gen = self.get_generator()        \n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        x_gen = self.gen(z)\n",
        "        valid = self.dis([x_gen, z])\n",
        "        gen_dis = Model(inputs=z, outputs=valid, name='gen_to_dis')\n",
        "        gen_dis.compile(loss=loss, optimizer=optimizer)          \n",
        "        \n",
        "        #Training\n",
        "        min_val_loss = float('inf')\n",
        "        stop_count = 0\n",
        "        for i in range(epochs):    \n",
        "            #Unfreeze Discriminator\n",
        "            self.dis.trainable = True\n",
        "                \n",
        "            #From the training data, randomly choice half of the \"batch_size\" as \"real_data\"\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size//2)\n",
        "            real_data = X_train[idx]\n",
        "    \n",
        "            #Generates noise and get the data generated by that noise\n",
        "            noise = np.random.normal(0, 1, (len(idx), self.latent_dim))\n",
        "            gen_data = self.gen.predict(noise)\n",
        "    \n",
        "            #Get noise predicted from \"real_data\"\n",
        "            enc_noise = self.enc.predict(real_data)\n",
        "               \n",
        "            #Train Discriminator\n",
        "            d_enc_loss = self.dis.train_on_batch([real_data, enc_noise], np.ones((len(real_data), 1)))\n",
        "            d_gen_loss = self.dis.train_on_batch([gen_data, noise], np.zeros((len(gen_data), 1)))\n",
        "            d_loss = d_enc_loss + d_gen_loss\n",
        "    \n",
        "            #Freeze Discriminator to train Encoder and Generator\n",
        "            self.dis.trainable = False\n",
        "    \n",
        "            #Train Encoder\n",
        "            e_loss = enc_dis.train_on_batch(real_data, np.zeros((len(real_data), 1)))\n",
        "        \n",
        "            #Train Generator\n",
        "            g_loss = gen_dis.train_on_batch(noise, np.ones((len(noise), 1)))\n",
        "                 \n",
        "            #Calculate test loss\n",
        "            if len(test)>0:\n",
        "                #Get testing-data\n",
        "                X_test = test[0]\n",
        "                y_true = test[1]\n",
        "                \n",
        "                #Predict testing-data\n",
        "                proba = self.predict(X_test)\n",
        "                proba = minmax_scale(proba)\n",
        "                \n",
        "                #As \"val_loss\", calculate binary cross entropy\n",
        "                val_loss = tf.keras.losses.binary_crossentropy(y_true, proba).numpy()\n",
        "                \n",
        "                #If \"val_loss\" is less than \"min_val_loss\"\n",
        "                if min_val_loss > val_loss:                                        \n",
        "                    min_val_loss = val_loss #Update \"min_val_loss\" to \"val_loss\"\n",
        "                    stop_count = 0 #Change \"stop_count\" to 0\n",
        "                #If \"stop_count\" is equal or more than \"early_stop_num\", training is end\n",
        "                elif stop_count >= early_stop_num:\n",
        "                    break\n",
        "                #Else, \"stop_count\" is added 1\n",
        "                else:\n",
        "                    stop_count += 1               \n",
        "                    \n",
        "            #Display learning progress\n",
        "            if verbose==1 and i%100==0:\n",
        "                if len(test)==0: print(f'epoch{i}-> d_loss:{d_loss}  e_loss:{e_loss}  g_loss:{g_loss}')\n",
        "                else: print(f'epoch{i}-> d_loss:{d_loss}  e_loss:{e_loss}  g_loss:{g_loss}  val_loss:{val_loss}')\n",
        "    \n",
        "    #Test model\n",
        "    def predict(self, X_test, weight=0.9, degree=1):\n",
        "        \n",
        "        #Convert testing-data to numpy format\n",
        "        X_test = np.array(X_test)\n",
        "        \n",
        "        #Get noise predicted from testing-data\n",
        "        z_gen = self.enc.predict(X_test)\n",
        "        \n",
        "        #Get the data generated by that noise predicted from testing-data\n",
        "        reconstructs = self.gen.predict(z_gen)\n",
        "                \n",
        "        #As \"gen_score\", calculation of the norm of difference between testing-data and generated data for each features\n",
        "        #The more different the testing-data is from the training-data, the higher the score (=anomality)        \n",
        "        '''\n",
        "            If testing-data is similar to the training-data,\n",
        "            the data generated by a well-trained Encoder and Genrator can reproduce the testing-data.\n",
        "        '''        \n",
        "        delta = X_test - reconstructs\n",
        "        gen_score = tf.norm(delta, ord=degree, axis=1).numpy()\n",
        "        \n",
        "        #Inference of Encoder's input and output in the Discriminator\n",
        "        l_encoder = self.dis.predict([X_test, z_gen])\n",
        "        \n",
        "        #As \"dis_score\", calculate binary cross entropy from the results of the inference\n",
        "        #The more different the testing-data is from the training-data, the higher the score (=anomality)        \n",
        "        '''\n",
        "            If testing-data is similar to the training-data,\n",
        "            result of inference of testing-data and the noise encoded from testing-data in the Discriminator approaches 1.\n",
        "        '''\n",
        "        dis_score = tf.keras.losses.binary_crossentropy(np.ones((len(X_test), 1)), l_encoder).numpy()\n",
        "    \n",
        "        #Return anomality calculated \"gen_score\" and \"dis_score\"\n",
        "        return weight*gen_score + (1-weight)*dis_score      \n",
        "        \n",
        "    ##Encoder\n",
        "    def get_encoder(self, initializer=tf.keras.initializers.GlorotUniform()):\n",
        "        inputs = Input(shape=(self.input_dim,), name='input')\n",
        "        net = inputs\n",
        "        net = Dense(64, activation=LeakyReLU(alpha=0.1), kernel_initializer=initializer,\n",
        "                    name='layer_1')(net)\n",
        "        outputs = Dense(self.latent_dim, activation='linear', kernel_initializer=initializer,\n",
        "                        name='output')(net)\n",
        "    \n",
        "        return Model(inputs=inputs, outputs=outputs, name='Encoder')\n",
        "    \n",
        "    ##Generator\n",
        "    def get_generator(self, initializer=tf.keras.initializers.GlorotUniform()):\n",
        "        inputs = Input(shape=(self.latent_dim,), name='input')\n",
        "        net = inputs\n",
        "        net = Dense(64, activation='relu', kernel_initializer=initializer,\n",
        "                    name='layer_1')(net)\n",
        "        net = Dense(128, activation='relu', kernel_initializer=initializer,\n",
        "                    name='layer_2')(net)\n",
        "        outputs = Dense(self.input_dim, activation='linear', kernel_initializer=initializer,\n",
        "                        name='output')(net)\n",
        "    \n",
        "        return Model(inputs=inputs, outputs=outputs, name='Generator')\n",
        "    \n",
        "    ##Discriminator\n",
        "    def get_discriminator(self, initializer=tf.keras.initializers.GlorotUniform()):\n",
        "        #D(x)\n",
        "        inputs1 = Input(shape=(self.input_dim,), name='real')\n",
        "        net = inputs1\n",
        "        net = Dense(128, activation=LeakyReLU(alpha=0.1), kernel_initializer=initializer,\n",
        "                    name='layer_1')(net)\n",
        "        dx = Dropout(.2)(net)\n",
        "    \n",
        "        #D(z)\n",
        "        inputs2 = Input(shape=(self.latent_dim,), name='noise')\n",
        "        net = inputs2\n",
        "        net = Dense(128, activation=LeakyReLU(alpha=0.1), kernel_initializer=initializer,\n",
        "                    name='layer_2')(net)\n",
        "        dz = Dropout(.2)(net)\n",
        "    \n",
        "        #concat D(x) and D(z)\n",
        "        conet = Concatenate(axis=1)([dx,dz])\n",
        "    \n",
        "        #D(x,z)\n",
        "        conet = Dense(128, activation=LeakyReLU(alpha=0.1), kernel_initializer=initializer,\n",
        "                      name='layer_3')(conet)\n",
        "        conet = Dropout(.2)(conet)\n",
        "        outputs = Dense(1, activation='sigmoid', kernel_initializer=initializer,\n",
        "                        name='output')(conet)\n",
        "\n",
        "        return Model(inputs=[inputs1,inputs2], outputs=outputs, name='Discriminator')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG7uJAB16-B7"
      },
      "source": [
        "#for debug    \n",
        "def run(X_train, X_test, y_true):\n",
        "    model = EfficientGAN()\n",
        "    model.fit(X_train, test=(X_test,y_true))\n",
        "    proba = model.predict(X_test)\n",
        "    \n",
        "from sklearn.datasets import load_iris\n",
        "if __name__ == '__main__':    \n",
        "    iris = load_iris()   \n",
        "    run(iris['data'], iris['data'], iris['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_rUKus86-FE"
      },
      "source": [
        "#from model import EfficientGAN\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_true = (iris['data'], ### 今、150個全て使用\n",
        "                           iris['data'], ### 今、X_trainと同一になっている\n",
        "                           iris['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIs9Opm73GyV"
      },
      "source": [
        "model = EfficientGAN()\n",
        "model.fit(X_train, test=(X_test,y_true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEoXMvxe3pPL"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZuHXMA63nb7"
      },
      "source": [
        "proba = model.predict(X_test, weight=0)\n",
        "plt.scatter(range(len(proba)), proba, c=y_true)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3ZClc0U3nZG"
      },
      "source": [
        "proba = model.predict(X_test, weight=1)\n",
        "plt.scatter(range(len(proba)), proba, c=y_true)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIoH17-r-pMG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhWlVORj8bl7"
      },
      "source": [
        "model = EfficientGAN()\n",
        "model.fit(X_train, \n",
        "          epochs=500, ###\n",
        "          test=(X_test,y_true))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OPUKUde3j42"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbH5LcoS69_w"
      },
      "source": [
        "proba = model.predict(X_test, weight=0)\n",
        "plt.scatter(range(len(proba)), proba, c=y_true)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yn_bNEW9bjw"
      },
      "source": [
        "proba = model.predict(X_test, weight=1)\n",
        "plt.scatter(range(len(proba)), proba, c=y_true)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8EKizf9bg6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJrfKq9jbkhc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saN2i7VtMCrR"
      },
      "source": [
        "### <font color=blue>**4.** </font> 模倣学習（Imitation Learning）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klCdUCpVMV9o"
      },
      "source": [
        "1:05:50〜\n",
        "\n",
        "https://www.youtube.com/watch?v=rOho-2oJFeA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B44-YHvMQe7"
      },
      "source": [
        "https://www.youtube.com/watch?v=cFtnflNe5fM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-CB_BFtMQbg"
      },
      "source": [
        "https://www.youtube.com/watch?v=TSj7E-M3Zig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihLm2PG8MMHB"
      },
      "source": [
        "https://www.youtube.com/watch?v=EDl8SQUNjj0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4yLGe5sMBpJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYGolUkLMBl3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}