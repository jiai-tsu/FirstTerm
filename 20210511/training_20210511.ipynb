{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training_20210511.ipynb","provenance":[],"collapsed_sections":["Itqemv_37PK5","JP6m19lOCdsX","5W1tXBGYBmzW","6BBFcnoUThbJ","YnLDIM5cy2L_","cCkDgCl6SaI8","8vBSo9iASwAm","cICRmvzq6pza","nfppHzioGW-4","0c8n0AP3GWjf","ki9f7MIiunda","gmpaWSCacci_"],"authorship_tag":"ABX9TyO5AU3NVDUXxrZRtjUIttxT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IQnijj-lZg_s"},"source":["## 46. Nonparametric Bayesian Models"]},{"cell_type":"markdown","metadata":{"id":"o-ERMWOCbL5n"},"source":["### <font color=blue>**1.** </font> Dirichlet Process"]},{"cell_type":"markdown","metadata":{"id":"Itqemv_37PK5"},"source":["#### <font color=green>**1.1.** </font> 棒折り過程（SBP : Stick-Breaking Process）"]},{"cell_type":"code","metadata":{"id":"4VSb1ov5dL2o"},"source":["## 出典 : https://github.com/Ma-sa-ue/practice/tree/master/machine%20learning(python)/nonparabayes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FzhGUQa2C3q0"},"source":["import numpy as np\n","import scipy as sp\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXv4O0ce72d2"},"source":["#### base distibution\n","\n","def get_ck(v,_N):\n","  c =[]\n","  for i in range(_N):\n","    first = 1.0\n","    for j in range(i):\n","      first = first*(1 -v[j])\n","    c.append(first*v[i])\n","  return c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5K1MyQaS72WI"},"source":["### Stick breaking process\n","def sbp(alpha, N):\n","  v = np.random.beta(1,alpha,N)\n","  ck = get_ck(v, N)\n","  return ck"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rsU-HQKYC3oR"},"source":["N = int(5e2)\n","\n","plt.figure(figsize=(12,6))\n","plt.subplot(1,2,1)\n","plt.plot(sbp(100.0, N))\n","plt.ylim(0, 0.05)\n","plt.title(\"alpha=100.0\")\n","\n","plt.subplot(1,2,2)\n","plt.plot(sbp(1.0, N))\n","plt.title(\"alpha=1.0\")\n","plt.ylim(0, 0.6)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9YNyW3tccQX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAzwPPNY3ZUy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JP6m19lOCdsX"},"source":["#### <font color=green>**1.2.** </font> SBP 別の実装例"]},{"cell_type":"code","metadata":{"id":"FTcsK16tDMcL"},"source":["## 出典 : https://gist.github.com/tok41/d3548e481d7e4fbb98ccb3ce5d21e075#file-sbp_try1-ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HnpKnWBkDRAQ"},"source":["概要\n","- 棒折り過程（SBP; Stick Breaking Process）を実装してみる\n","- SBPはディリクレ過程の実現れいの一つ\n","- クラス割当（データがどのクラスに属するかを示す変数）を積分消去し、無限次元の混合モデルを考える\n"," - 無限次元の混合比を生成するための確率過程としてSBPが利用できる"]},{"cell_type":"code","metadata":{"id":"mx956fFXDMXL"},"source":["import numpy as np\n","import scipy as sp\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set()\n","np.random.seed(100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCaOBP_iCcmF"},"source":["alphas = [2, 5, 10]\n","\n","xt = np.linspace(0, 1, 100)\n","\n","fig = plt.figure(figsize=(7, 4))\n","ax = fig.subplots(1, 1)\n","for alp in alphas:\n","    p = sp.stats.beta(1, alp).pdf(xt)\n","    ax.plot(xt, p, label = f'(1, {alp})')\n","ax.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaO1DbvSCciZ"},"source":["def SBP(K, alpha):\n","  vi = sp.stats.beta(1, alpha).rvs(size=K)\n","  stick_len = np.concatenate([[1], np.cumprod((1-vi))[:-1]])\n","  pis = vi * stick_len\n","  return pis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qiKHqU0WDHWM"},"source":["K = 100\n","alphas = [2, 10]\n","\n","lst_pis = []\n","for a in alphas:\n","  pis = SBP(K, a)\n","  lst_pis.append(pis)\n","  print(f'alpha={a}, {sum(pis)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_r9E16EcDHTX"},"source":["fig = plt.figure(figsize=(10, 6))\n","ax = fig.subplots(1, 1)\n","\n","for i, pis in enumerate(lst_pis):\n","  ax.plot(np.cumsum(pis), label=f'$\\\\alpha={alphas[i]}$')\n","ax.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5IbN6ZGDHQl"},"source":["fig = plt.figure(figsize=(7, 8))\n","ax = fig.subplots(2, 1)\n","\n","for i, pis in enumerate(lst_pis):\n","  #ax.plot(np.cumsum(pis), label=f'$\\\\alpha={alps[i]}$')\n","  ax[i].bar(np.arange(K), pis, label=f'$\\\\alpha={alphas[i]}$')\n","  ax[i].legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awDTaxUMDgCd"},"source":["$\\alpha$が大きい場合、混合比がそこまで大きくないケースが多数現れる。これが、CRPにおけるテーブルの数が多くなることに対応している。\n","\n","$\\alpha$が小さいと、大きい確率が割り当てられたクラスが少数現れる。実際SBPでは、計算の都合上クラスの上限を決めるため、微小な混合比が割り当てられたクラスがあるが、非常に微小。"]},{"cell_type":"code","metadata":{"id":"oa7a5NEOFVmr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"prEqzBp7FVjk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5W1tXBGYBmzW"},"source":["#### <font color=green>**1.3.** </font> 中華料理店過程（CRP : Chinese Restaurant Process）"]},{"cell_type":"code","metadata":{"id":"3JgeLcB8p7Gn"},"source":["## 出典 : https://learning-with-machine.hatenablog.com/entry/2020/08/26/193000\n","##        https://gist.github.com/tok41/31e18bea891ddb31ebe2c9e0435d5a70#file-crp_try1-ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kY36FwcRC5Mz"},"source":["概要\n","- 中華料理店過程（Chinese Restaurant Process;CRP）を実装して動作を確認してみる\n","- CRPはディリクレ過程の一つの実現例\n","- 分割の仕方（クラス割り当て）の事前分布として利用できる\n","  - n個のデータを分割するとすればどのような分割がどのような確率として起こり得るのかのモデル（「続 わかりやすいパターン認識」, p.226 より）\n","- CRPを使う場合、混合数を積分消去し、無限次元の混合モデルに拡張される\n","  - このときのクラスの割当についての事前分布としてCRPが使われる"]},{"cell_type":"code","metadata":{"id":"MMe1XOvHCcw2"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","sns.set()\n","np.random.seed(100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6HhF3ZCRAHv"},"source":["$\n","  p(z_l | \\overline{\\alpha}) = \\begin{cases}\n","    \\dfrac{n_l(z_l)}{l + \\overline{\\alpha}} & (着席済みテーブルに座る、それぞれの確率) \\\\\n","    \\dfrac{\\overline{\\alpha}}{l + \\overline{\\alpha}} & (新しいテーブルに一人目として座る確率)\n","  \\end{cases}\n","$"]},{"cell_type":"code","metadata":{"id":"2I-4m9UuCElF"},"source":["def prob_table_choice(lst_n, alpha=1.0):\n","  # テーブルの選択確率を算出\n","  n = sum(lst_n) + 1\n","  p = [ni / (n - 1 + alpha) for ni in lst_n]\n","  p.append( alpha / (n - 1 + alpha) )\n","  return p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8tGhO0ZCcuM"},"source":["def CRP(n, alpha=1.0):\n","  n_c = []\n","  history = {\n","      'table_nums':[], \n","      'chosen_tables':[], \n","      }\n","  for i in range(n):\n","    table_ids = np.arange(0, len(n_c)+1)\n","    p = prob_table_choice(n_c, alpha=alpha)\n","    chosen_table = np.random.choice(table_ids, p=p)\n","    if len(n_c) == chosen_table:\n","      n_c.append(1)\n","    else:\n","      n_c[chosen_table] += 1\n","    history['table_nums'].append(len(n_c))\n","    history['chosen_tables'].append(chosen_table)\n","  return n_c, history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZmoU6OXfCcrr"},"source":["N = 1000\n","customer_counts1, history1 = CRP(N, alpha=10)\n","customer_counts2, history2 = CRP(N, alpha=2)\n","\n","fig = plt.figure(figsize=(7, 4))\n","ax = fig.subplots(1, 1)\n","ax.plot(history1['table_nums'], label=f'$\\\\alpha=10$')\n","ax.plot(history2['table_nums'], label=f'$\\\\alpha=2$')\n","ax.legend()\n","ax.set_xlabel('number of customer')\n","ax.set_ylabel('number of table')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QOhguRNwCco0"},"source":["fig = plt.figure(figsize=(12, 4))\n","ax = fig.subplots(1, 2, sharex=True, sharey=True)\n","ax[0].bar(np.arange(1, len(customer_counts1)+1), sorted(customer_counts1, reverse=True), label=f'$\\\\alpha=10$')\n","ax[1].bar(np.arange(1, len(customer_counts2)+1), sorted(customer_counts2, reverse=True), label=f'$\\\\alpha=2$')\n","ax[0].legend()\n","ax[1].legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NNe60foDqPX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlMzer76DqNA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BBFcnoUThbJ"},"source":["#### <font color=green>**1.4.** </font> Pitman-Yor process"]},{"cell_type":"code","metadata":{"id":"sDI9FTVs8oHb"},"source":["## 出典 : https://github.com/Ma-sa-ue/practice/tree/master/machine%20learning(python)/nonparabayes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2HA_mCf8XEg"},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmemfl0u8r_j"},"source":["$\n","  p(z_l | \\overline{\\alpha}) = \\begin{cases}\n","    \\dfrac{n_l(z_l) - d}{\\sum{n_l(z_l)} + \\overline{\\alpha}} & (着席済みテーブルに座る、それぞれの確率) \\\\\n","    \\dfrac{\\overline{\\alpha} + d\\beta}{\\sum{n_l(z_l)} + \\overline{\\alpha}} & (新しいテーブルに一人目として座る確率)\n","  \\end{cases}\n","$"]},{"cell_type":"code","metadata":{"id":"y1T1ZUhzC3gb"},"source":["def alec(x,alpha,d):\n","  xxx = [0]\n","  n = sum(x)\n","  n_k = 0\n","  for i in range(len(x)):\n","    n_k = n_k + x[i]\n","    xxx.append((n_k*1.0 - d*(i+1))/(n+alpha)*1.0)\n","  xxx.append(1)\n","  return xxx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIrEnYQkBMjo"},"source":["#### d=0 : Chinese Restaurant Process\n","def CRP(_N,alpha = 30):\n","  custom_list = [0]\n","  custom_number_list = [1]\n","  custom_judge =[0,1.0/((alpha+1)*1.0),1]\n","  for i in range(_N-1):\n","    judge =  np.random.uniform()\n","    for j in range(len(custom_judge)-1):\n","      if custom_judge[j]< judge < custom_judge[j+1]:\n","        custom_list.append(j)\n","        if j == len(custom_judge)-2:\n","          custom_number_list.append(0)\n","        custom_number_list[j] = custom_number_list[j] +1\n","    custom_judge = alec(custom_number_list, alpha, 0.0)\n","  return custom_number_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjghVxpH9eVf"},"source":["### draw the sample \n","xxx =[]\n","yyy =[]\n","for k in range(20):\n","  x = [len(CRP(100*k,30)) for j in range(5)]\n","  xxx.extend(x)\n","  y = [ k*100 for j in range(5)]\n","  yyy.extend(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dp9QWU_w9eTF"},"source":["####  calulcate the expectaion\n","yy = [ i/4.0 for i in range(8000)]\n","def  expect(_i,alpha=30):\n","  return alpha * np.log(1+ _i/alpha)\n","xx = [ expect(i/4.0) for i in range(8000)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpwzpTC58x2q"},"source":["plt.figure(figsize=(12,6))\n","plt.plot(yy,xx) ## expectation\n","plt.scatter(yyy,xxx,c=\"r\") ## sample\n","plt.title(\"CRP alpha=30.0\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syx6yss18iw6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYUdW_oevkRa"},"source":["### Pitman-Yor process\n","def PYR(_N, alpha=10, d=0.0):\n","  custom_list = [0]\n","  custom_number_list = [1]\n","  custom_judge =[0,1.0/((alpha+1)*1.0),1]\n","  for i in range(_N-1):\n","    judge =  np.random.uniform()\n","    judge =  np.random.uniform()\n","    for j in range(len(custom_judge)-1):\n","      if custom_judge[j]< judge < custom_judge[j+1]:\n","        custom_list.append(j)\n","        if j == len(custom_judge)-2:\n","          custom_number_list.append(0)\n","        custom_number_list[j] = custom_number_list[j] +1\n","    custom_judge = alec(custom_number_list, alpha, d)\n","  return custom_number_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxy7p5xdp8Kz"},"source":["xxx2 =[]\n","yyy2 =[]\n","for k in range(20):\n","  x = [len(PYR(k*100, 10, 0.5)) for j in range(4)]\n","  xxx2.extend(x)\n","  y = [ k*100 for j in range(4)]\n","  yyy2.extend(y)\n","\n","xxx3 =[]\n","yyy3 =[]\n","for k in range(20):\n","  x = [len(PYR(k*100, 10, 0.1)) for j in range(4)]\n","  xxx3.extend(x)\n","  y = [ k*100 for j in range(4)]\n","  yyy3.extend(y)\n","\n","xxx4 =[]\n","yyy4 =[]\n","for k in range(20):\n","  x = [len(PYR(k*100, 10, 0.9)) for j in range(4)]\n","  xxx4.extend(x)\n","  y = [ k*100 for j in range(4)]\n","  yyy4.extend(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjbRt2pZECTC"},"source":["### convert log scale\n","def convert_log(_g):\n","  llog = np.log10(np.array(_g))\n","  return llog"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4syzoxvECQm"},"source":["plt.figure(figsize=(10,6))\n","plt.scatter(convert_log(yyy2),convert_log(xxx2), c=\"blue\")\n","plt.scatter(convert_log(yyy3),convert_log(xxx3), c=\"yellow\")\n","plt.scatter(convert_log(yyy3),convert_log(xxx4), c=\"green\")\n","plt.xlim(0,4)\n","plt.ylim(0,4)\n","plt.title(\"pitman-yor\")\n","plt.xlabel(\"#customers(log scale)\")\n","plt.ylabel(\"#tables (log scale)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_b-21ioF8iuG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofsF5m528irV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CCtivAHtcRl7"},"source":["### <font color=blue>**2.** </font> infiniteGMM"]},{"cell_type":"markdown","metadata":{"id":"YnLDIM5cy2L_"},"source":["#### <font color=green>**2.1.** </font> CRPでクラスタリング"]},{"cell_type":"code","metadata":{"id":"N8CKKAj9dK2u"},"source":["# 出典 : https://github.com/tokky-cpp/machine_learning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t98vmnIodK0p"},"source":["# CRP(中華料理店過程)に基づいてクラス数未定のクラスタリングのための事前分布を決定する。\n","\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kR7tXUDzyWUX"},"source":["alpha = 1.\n","n = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QU_IjJoHufEH"},"source":["def crp(alpha=alpha, n=n):  # alpha:パラメータ  n:全体の人数\n","  s=[] #各人が座るテーブル番号のリスト\n","  table={} #テーブルごとの人数の辞書\n","  for i in range(n):\n","    if i==0: #1人目だったら無条件に0番目のテーブルに着席\n","      s.append(0)\n","      table.setdefault(0,1)\n","      continue\n","    else:\n","      prob = random.random()  # 0-1の範囲\n","      sum = 0.0 # 各テーブルの着席確率を累積していって、probを超えたら着席\n","      # 新規テーブルに対して\n","      new_p = alpha / (i + alpha)   ### i-1 -> i\n","      sum += new_p\n","      if sum >= prob:\n","        s.append(len(table))\n","        table.setdefault(len(table),1)\n","        continue\n","      # 既存テーブルに対して\n","      for t in table.keys():\n","        sit_p = table[t] / (i + alpha)  ### i-1 -> i\n","        sum += sit_p\n","        if sum >= prob:\n","          s.append(t)\n","          table[t] += 1\n","          break\n","  return s, table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f330k2MIyezb"},"source":["$\n","  p(z_l | \\overline{\\alpha}) = \\begin{cases}\n","    \\dfrac{n_l(z_l)}{l + \\overline{\\alpha}} & (着席済みテーブルに座る、それぞれの確率) \\\\\n","    \\dfrac{\\overline{\\alpha}}{l + \\overline{\\alpha}} & (新しいテーブルに一人目として座る確率)\n","  \\end{cases}\n","$"]},{"cell_type":"code","metadata":{"id":"VN1BlhyFIgll"},"source":["def graph(table): # ディクショナリ形式を受け取ってグラフにする\n","  table = sorted(table.items(),key=lambda x: x[1])\n","  table.reverse()\n","  #print(table)\n","  number = []\n","  people = []\n","  for (t,n) in table:\n","    number.append(t)\n","    people.append(n)\n","  x = [i for i in range(len(people))]\n","  y = people\n","  plt.plot(x,y)\n","  plt.show()\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YY5xVq9ue_n"},"source":["s, table = crp(10, 10000)\n","\n","#print(s)\n","#print(table)\n","\n","graph(table)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8j_CPGIGlMX"},"source":["# 崩壊型ギブスサンプリング(続・パタp264)に基づいて記述\n","\n","def cut(A, i):  # 行列Aからi行目i列目の要素をなくした行列を返す i=(0,…,N-1)\n","  if A.shape[0] != A.shape[1]:\n","    print(\"[Error]正方行列ではありません\")\n","    print(A)\n","  return np.delete(np.delete(A, i, 1), i, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jve1sc8-p8S1"},"source":["def clustering(K=None, M=1., W=None, alpha=1., beta=1.):\n","  # K : 類似度行列\n","  # M : 特徴ベクトルの次元数\n","  # W : (N*N行列)中心化したウィシャート分布の共分散行列の事前分布\n","  # alpha : 所属クラスを表す潜在変数\n","  # beta : ナンダコレ？\n","\n","  N = K.shape[0]\n","\n","  # Step1 初期設定 \n","  s, s_i = crp(alpha, N)  # 所属クラスを示す潜在変数を初期化\n","  c = len(s_i)  # 総クラス数\n","  P_max = 0.0 # 事後確率の最大値を初期化\n","\n","  # Step2 所属クラスの更新\n","  prob = []\n","  for i in range(N):\n","    p = 0.0\n","    p += -1*(M/2.0)*math.log(2)\n","    p += -1*((N-1)/4)*math.log(math.pi)\n","    p += -1*math.log(math.gamma((M-N+1)/2))\n","    p += ((M+beta)/2)*math.log((np.linalg.det(K+np.linalg.inv(W)))/(np.linalg.det(cut(K,i)+np.linalg.inv(cut(W,i)))))\n","    p += -1*(beta/2)*math.log(np.linalg.det(W)/np.linalg.det(cut(W,i)))\n","    p += (M-N+1)/2.0*math.log(np.linalg.det(K)/np.linalg.det(cut(K,i)))\n","    p += -1*(1/2.0)*math.log(np.linalg.det(cut(K,i)))\n","    #print(i, p)\n","    prob.append(p)\n","\n","  return prob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pgc5XxLCIgnD"},"source":["# ガウスガンマ分布に基づいてデータ生成\n","# 生成モデルにおいて、クラスごとのガウス分布のパラメータを決定するために使用\n","\n","def normal_gamma(a=5, b=6, mu0=0, beta=2, POINT=4, see=False):\n","  ll = []\n","  mumu = []\n","  for i in range(POINT):\n","    lamb = np.random.gamma(a, 1/b, 1)\n","    ll.append(lamb[0])\n","    mu = np.random.normal(mu0, 1/(beta*lamb))\n","    mumu.append(mu)\n","    \n","  if see:\n","    plt.plot(mumu, ll, \"o\")\n","    plt.show()\n","\n","  return [(m, l) for (m, l) in zip(mumu, ll)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zc--bUhIge7"},"source":["# 生成モデルに基づいた特徴ベクトル生成\n","# 特徴ベクトル間の類似度K(行列)を生成\n","\n","def make_K(N=5, M=8, alpha=1.0, a=5, b=10, visual=False):\n","  # N      data points\n","  # M      dimension of data\n","  # alpha  palameter for CRP\n","  # a      palameter for normal_gamma distribution\n","  # b      palameter for normal_gamma distribution\n","  #print(\"data points = {}\".format(N))\n","  #print(\"dimension of data = {}\".format(M))\n","    \n","  # クラスの事前分布(CRP)\n","  (s, table) = crp(alpha, N)\n","  cls_num = len(table)\n","  print(\"{} clusters\".format(cls_num))\n","    \n","  # パラメータの事前分布(ガウスガンマ分布)\n","  theta = normal_gamma(POINT=cls_num, see=False)\n","    \n","  data = []   # データ点\n","  #data = np.empty((0, M))\n","\n","  # ガウス分布に基づいてデータ点生成\n","  for i in s:   # 全データに対して\n","    d = []\n","    for j in range(M):  # 次元数分だけデータを生成\n","      d.append(np.random.normal(theta[i][0],theta[i][1]))\n","    #print(\"data : {}\".format(data))\n","    #print(\"d : {}\".format(d))\n","    #data = np.append(data, np.array([d]), axis=0)\n","    data.append(d)\n","  data = np.array(data)\n","  #print(\"data : {}\".format(data))\n","\n","  # data visualize\n","  if visual:\n","    for i in range(len(table)):\n","      x = []\n","      y = []\n","      for d in range(len(data)):\n","        if s[d]==i:\n","          x.append(data[d][0])\n","          y.append(data[d][1])\n","      plt.plot(x, y, \"o\")\n","    print(table)\n","    plt.show()\n","  #print(data)\n","  #print(len(data))\n","\n","  #print(\"--------類似度計算-------\")\n","  #print(\"data : {}\".format(data))\n","  K = []\n","  N_ = min(N, len(s))   ###\n","  #K = np.empty((0,N))\n","  for i in range(N_):\n","    ker = []\n","    for j in range(N_):\n","      ker.append(data[i].T @ data[j])\n","    K.append(ker)\n","    #K = np.append(K, np.array([ker]), axis=0)\n","\n","  #print(\"make_K complete\")\n","  return np.array(K)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MprVICfsIgah"},"source":["#------------[start]データ生成〜類似度行列計算----------\n","K = make_K() #オプションがなければデフォルト値(N=5,M=?)で類似度行列生成\n","if len(K)>1:\n","  K = K.reshape(len(K[0]), len(K[1]))\n","#print(\"K : {}\".format(K))\n","#------------[end]データ生成〜類似度行列計算----------\n","\n","#------------クラスタリング----------\n","# 類似度行列とその他のパラメータが与えられたもとで崩壊型ギブスサンプリングを行う\n","M=5\n","clustering(K=K, M=M, W=K, alpha=1, beta=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uikaNpp14b5c"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hV6ukVR4b1c"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCkDgCl6SaI8"},"source":["#### <font color=green>**2.2.** </font> scikit-learnを使用、irisデータで試す"]},{"cell_type":"code","metadata":{"id":"N73nd-JkzH3C"},"source":["import numpy as np\n","from scipy import linalg\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","\n","from sklearn import datasets, mixture"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"juYS9_m1zH0F"},"source":["colors = ['navy', \n","          'c', \n","          'cornflowerblue', \n","          'gold',\n","          'darkorange']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rf4qzIKVzHxs"},"source":["def plot_results(X, Y_, means, covariances, size, index):\n","  splot = plt.subplot(1, 3, 1 + index)\n","  for i, (mean, covar) in enumerate(zip(means, covariances)):\n","    v, w = linalg.eigh(covar)\n","    v = 2. * np.sqrt(2.) * np.sqrt(v)\n","    u = w[0] / linalg.norm(w[0])\n","    if not np.any(Y_ == i):\n","      continue\n","    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1],size,  color=colors[i%5])\n","    angle = np.arctan(u[1] / u[0])\n","    angle = 180. * angle / np.pi  \n","    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=colors[i%5])\n","    ell.set_clip_box(splot.bbox)\n","    ell.set_alpha(0.5)\n","    splot.add_artist(ell)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rb2EOOqZGePb"},"source":["# Number of samples per component\n","n_samples = 500\n","\n","# Generate random sample, two components\n","np.random.seed(0)\n","\n","C = np.array([[0., -0.1], [1.7, .4]])\n","X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n","          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n","\n","plt.figure(figsize=(17,5))\n","\n","# Fit a Gaussian mixture with EM using five components\n","gmm = mixture.GaussianMixture(n_components=5, covariance_type='full').fit(X)\n","plot_results(X, gmm.predict(X), gmm.means_, \n","             gmm.covariances_, 1.0, 0)\n","\n","# Fit a Bayesian Gaussian mixture using five components\n","dpgmm = mixture.BayesianGaussianMixture(n_components=5,\n","                                        weight_concentration_prior_type='dirichlet_distribution', \n","                                        covariance_type='full').fit(X)\n","plot_results(X, dpgmm.predict(X), dpgmm.means_, \n","             dpgmm.covariances_, 1.0, 1)\n","\n","# Fit a Bayesian Gaussian mixture using five components\n","dpgmm = mixture.BayesianGaussianMixture(n_components=n_samples,\n","                                        weight_concentration_prior_type='dirichlet_process', \n","                                        covariance_type='full').fit(X)\n","plot_results(X, dpgmm.predict(X), dpgmm.means_, \n","             dpgmm.covariances_, 1.0, 2)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sz0kpGA0Fsx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1zVW-0N0RDX"},"source":["def plot_results_2(X, Y_, means, covariances, size, index):\n","  splot = plt.subplot(1, 3, 1 + index)\n","  for i, (mean, covar) in enumerate(zip(means, covariances)):\n","    v, w = linalg.eigh(covar)\n","    v = 2. * np.sqrt(2.) * np.sqrt(v)\n","    u = w[0] / linalg.norm(w[0])\n","    if not np.any(Y_ == i):\n","      continue\n","    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], size, color=color(i))\n","    angle = np.arctan(u[1] / u[0])\n","    angle = 180. * angle / np.pi  \n","    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color(i))\n","    ell.set_clip_box(splot.bbox)\n","    ell.set_alpha(0.5)\n","    splot.add_artist(ell)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sM6ZoE9x0Gsx"},"source":["color = plt.get_cmap('tab20')\n","\n","# Number of samples per component\n","n_samples = 150\n","\n","# Generate random sample, two components\n","X = datasets.load_iris().data\n","Y = datasets.load_iris().target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACcx7O7nOi93"},"source":["plt.figure(figsize=(17,5))\n","\n","# Fit a Gaussian mixture with EM using five components\n","gmm = mixture.GaussianMixture(n_components=5, covariance_type='full').fit(X)\n","plot_results_2(X, gmm.predict(X), gmm.means_, \n","               gmm.covariances_, 30.0, 0)\n","\n","# Fit a Bayesian Gaussian mixture using five components\n","dpgmm = mixture.BayesianGaussianMixture(n_components=5,\n","                                        weight_concentration_prior_type='dirichlet_distribution', \n","                                        covariance_type='full').fit(X)\n","plot_results_2(X, dpgmm.predict(X), dpgmm.means_, \n","               dpgmm.covariances_, 30.0, 1)\n","\n","# Fit a Bayesian Gaussian mixture using five components\n","dpgmm = mixture.BayesianGaussianMixture(n_components=n_samples,\n","                                        weight_concentration_prior_type='dirichlet_process', \n","                                        covariance_type='full').fit(X)\n","plot_results_2(X, dpgmm.predict(X), dpgmm.means_, \n","               dpgmm.covariances_, 30.0, 2)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRreUSofGE2-"},"source":["# 正解ラベル\n","plt.figure(figsize=(4.5,4.5))\n","plt.scatter(X[:,0],X[:,1], c=Y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"at3-lj2B3ZSS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXrWsAQ93ZOK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vBSo9iASwAm"},"source":["#### <font color=green>**2.3.** </font> pymc3を使用、irisデータで試す\n","<font color=red>注意：サンプリングに時間がかかる</font>"]},{"cell_type":"code","metadata":{"id":"8ab0FXT5SwAn"},"source":["## 出典 : https://medium.com/@albertoarrigoni/dirichlet-processes-917f376b02d2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"akX6e2h0SwAn"},"source":["from scipy import stats\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3a83zjM_SwAn"},"source":["def DP(h, alpha):\n","  n = max(int(5 * alpha + 2), 500)\n","  pi = stats.beta(1, alpha).rvs(size=n) # sample weights\n","  pi[1:] = pi[1:] * (1 - pi[:-1]).cumprod() # stick-breaking\n","  theta = h(size=n) # random draws from h\n","  return pi, theta # return parameters of G\n","        \n","def plot_normal_dp_approximation(alpha, n=2):\n","  #pi, theta = DP(stats.norm.rvs, alpha)\n","  x = np.linspace(-3, 3, 100)\n","    \n","  plt.figure(figsize=(14, 4))\n","  plt.suptitle(r'Two samples from DP($\\alpha$). $\\alpha$ = {}'.format(alpha))\n","  plt.ylabel(r'$\\pi$')\n","  plt.xlabel(r'$\\theta$')\n","  pltcount = int('1' + str(n) + '0')\n","    \n","  for i in range(n):\n","    pltcount += 1\n","    plt.subplot(pltcount)\n","    #pi, theta = dirichlet_process(stats.norm.rvs, alpha)\n","    pi, theta = DP(stats.norm.rvs, alpha)\n","    pi = pi * (stats.norm.pdf(0) / pi.max())\n","    plt.vlines(theta, 0, pi, alpha=0.5)\n","    plt.ylim(0, 1)\n","    plt.plot(x, stats.norm.pdf(x))\n","  plt.show()  ##\n","\n","np.random.seed(3)\n","for alpha in [1, 10, 100]:\n","  plot_normal_dp_approximation(alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRPrKx9nSwAo"},"source":["import random\n","import matplotlib.pyplot as plt\n","from pylab import rcParams\n","\n","rcParams['figure.figsize'] = 18, 6\n","fig, axs = plt.subplots(1, 3)\n","plot_count = 0\n","fig.suptitle('Chinese Restaurant Process customers distribution')\n","\n","# Play with different concentrations\n","for concentration in [0.1, 1.0, 10]:\n","  # First customer always sits at the first table\n","  tables = [1]\n","  for n in range(2,100):\n","    # Get random number 0~1\n","    rand = random.random()\n","    p_total = 0\n","    existing_table = False\n","    for index, count in enumerate(tables):\n","      prob = count / (n + concentration)\n","      p_total += prob\n","      if rand < p_total:\n","        tables[index] += 1\n","        existing_table = True\n","        break\n","    # New table!!\n","    if not existing_table:\n","      tables.append(1)\n","  axs[plot_count].bar([i for i in range(len(tables))], tables)\n","  axs[plot_count].set_title(r'Concentration ($\\alpha$) = {}'.format(concentration))\n","  plot_count+= 1\n","  for ax in axs.flat:\n","    ax.set(xlabel='Table number', ylabel='N customers')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qba_IyIFSwAp"},"source":["from sklearn.datasets import load_iris\n","import pandas as pd\n","\n","df = pd.DataFrame(load_iris()['data'])\n","y = df.values\n","# Standardize the data\n","y = (y - y.mean(axis=0)) / y.std(axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"baDzlKJKSwAp"},"source":["import seaborn as sns\n","\n","plt.figure(figsize=(12, 6))\n","plt.title('Histogram of the 3rd column of the (standardized) Iris dataset.')\n","plt.xlabel('x')\n","plt.ylabel('count')\n","sns.distplot(y[:, 2], ### 3 -> 2 \n","             bins=20, kde=False, rug=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nH8qGX_DSwAp"},"source":["import pymc3 as pm\n","from theano import tensor as tt\n","\n","def stick_breaking(beta):\n","  portion_remaining = tt.concatenate([[1], tt.extra_ops.cumprod(1 - beta)[:-1]])\n","  return beta * portion_remaining\n","\n","K = 30\n","with pm.Model() as model:\n","  alpha = pm.Gamma('alpha', 1., 1.)\n","  beta = pm.Beta('beta', 1., alpha, shape=K)\n","  w = pm.Deterministic('w', stick_breaking(beta))\n","  tau = pm.Gamma('tau', 1., 1., shape=K)\n","  lambda_ = pm.Uniform('lambda', 0, 5, shape=K)\n","  mu = pm.Normal('mu', 0, tau=lambda_ * tau, shape=K)\n","  obs = pm.NormalMixture('obs', w, mu, tau=lambda_ * tau,\n","                         observed=y[:, 2])\n","\n","with model:\n","  step = None\n","  trace = pm.sample(500, tune=500, init='advi', random_seed=35171, step=step)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1mTnABRSwAq"},"source":["x_plot = np.linspace(-2.4, 2.4, 200)\n","\n","# Calculate pdf for points in x_plot\n","post_pdf_contribs = stats.norm.pdf(np.atleast_3d(x_plot),\n","                                   trace['mu'][:, np.newaxis, :],\n","                                   1. / np.sqrt(trace['lambda'] * trace['tau'])[:, np.newaxis, :])\n","\n","# Weight (Gaussian) posterior probabilities by the posterior of w\n","post_pdfs = (trace['w'][:, np.newaxis, :] * post_pdf_contribs).sum(axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvSMXT5fSwAr"},"source":["import seaborn as sns\n","\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","rcParams['figure.figsize'] = 12, 6\n","sns.distplot(y[:, 2], rug=True, label='Original dataset', bins=20)\n","plt.plot(x_plot, post_pdfs[0],\n","         c='#CD5C5C', label='Posterior samples') # Add this to plot the legend\n","plt.plot(x_plot, post_pdfs[::100].T, c='#CD5C5C')\n","plt.xlabel('Iris dataset (3rd column values)')\n","# plt.yticklabels([]);\n","plt.ylabel('Density')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzWl8k02SwAr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxgV4iOFSwAr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cICRmvzq6pza"},"source":["### <font color=blue>**3.** </font> 構造変化推定"]},{"cell_type":"code","metadata":{"id":"rQdoDD1l4bzf"},"source":["## 出典 : https://gist.github.com/narrowlyapplicable/0922b733fa2cc75167f71eff448bf1a4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oUDKZ6pWIgVD"},"source":["# ライブラリ\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as st\n","import seaborn as sns\n","\n","plt.style.use('ggplot')\n","np.random.seed(1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rv5Opcffsmyi"},"source":["# シミュレーションデータ\n","\n","x = np.arange(1,91,1)\n","y = np.zeros(90)\n","y_tru = np.zeros(90)\n","\n","y[:30] = 1 + 0.5*x[:30] + st.norm.rvs(loc=0, scale=np.sqrt(0.3), size=30)\n","y[30:60] = 25 - 0.3*x[30:60] + st.norm.rvs(loc=0, scale=np.sqrt(0.1), size=30)\n","y[60:] = 1 +0.1*x[60:] + st.norm.rvs(loc=0, scale=np.sqrt(0.2), size=30)\n","y_tru[:30] = 1 + 0.5*x[:30]\n","y_tru[30:60] = 25 - 0.3*x[30:60]\n","y_tru[60:] = 1 +0.1*x[60:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpyakiTPsmui"},"source":["fig, ax = plt.subplots(figsize = (7,5))\n","ax.plot(x, y, marker=\".\", color=\"k\", linewidth=0)\n","ax.plot(x, y_tru, color=\"b\")\n","ax.set_xlabel('x'); ax.set_ylabel('y')\n","ax.set_title('simulation data')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDDrrAcutF3V"},"source":["# 潜在変数z\n","\n","def sampler_z(t, yt, xt, theta, sigma_y, z, alpha, T, mu, sigma, n0, tau):\n","  k_plus, n_t = np.unique(z, return_counts=True)\n","  prob_z = np.empty(k_plus.shape[0]+1)    # P(z_t = k) not normalize\n","  for k in k_plus:\n","    ## culculate n_k^¥t\n","    if(z[t]==k):\n","      n_kt = n_t[k_plus==k] - 1\n","    else:\n","      n_kt = n_t[k_plus==k]\n","    ## culculate probability s.t. z_t = k\n","    prob_z[k] = st.norm.pdf(x=yt, loc = np.dot(theta[k_plus==k], np.r_[xt,1]),\\\n","                            scale=sigma_y[k_plus==k])*(n_kt/(T-1+alpha))\n","\n","  ## create new cluster\n","  theta_new = st.multivariate_normal.rvs(mean=mu, cov=sigma, size=1)\n","  sigma_new = np.sqrt(st.invgamma.rvs(a=n0/2, scale=tau/2, size=1))\n","  prob_z[-1] = st.norm.pdf(x=yt, loc = np.dot(theta_new, np.r_[xt,1]), scale=sigma_new)*(alpha/(T-1+alpha))\n","    \n","  ## sampling\n","  prob_z /= np.sum(prob_z)\n","  z_sample = np.random.choice(np.r_[k_plus, k_plus[-1]+1], size=1, p=prob_z)[0]   # random.choice?\n","  return z_sample, theta_new, sigma_new\n","  ### return new z[t]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCXfm8EZtFw4"},"source":["# パラメータ\n","\n","def sampler_theta(k, y, x, z, sigma_y, mu, sigma_inv):\n","  #print(mu, sigma_inv)\n","  sigma_yk = sigma_y[k]\n","  t_k = np.where(z == k)[0]\n","  x = np.c_[x.copy(), np.ones(x.shape[0])]#X_t = [X_t, 1]\n","  sigma_k_inv = sigma_inv.copy()\n","  mu_k_tmp = np.dot(sigma_inv.copy(), mu) #Sigma^-1 * mu\n","  for tt in t_k: # Sigma_(t in T_k)\n","    x_tt = x[tt][:,np.newaxis]\n","    sigma_k_inv += np.dot(x_tt,x_tt.T) / (sigma_yk**2)\n","    mu_k_tmp += (y[tt]*x[tt]) / (sigma_yk**2)\n","  sigma_k = np.linalg.inv(sigma_k_inv)\n","  mu_k = np.dot(sigma_k, mu_k_tmp)\n","  del mu_k_tmp\n","  return st.multivariate_normal.rvs(mean=mu_k, cov=sigma_k, size=1)\n","  ### return new theta[k]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxEs5VistFvN"},"source":["# 観測時ノイズの分散\n","\n","def sampler_sigma_y(k, y, x, z, theta, n0, tau):\n","  t_k = np.where(z == k)[0]\n","  n_k = t_k.shape[0]\n","  tau_k = tau\n","  for tt in t_k:\n","    resid = y[tt] - np.dot(theta[k],np.r_[x[tt],1])\n","    tau_k += np.dot(resid, resid)\n","  return np.sqrt(st.invgamma.rvs(a=(n0+n_k)/2, scale=tau_k/2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAS0VlwptFtO"},"source":["# μ（θ_new の平均）\n","\n","def sampler_mu(theta, sigma_inv, mu0, v0_inv):\n","  vp = np.linalg.inv(theta.shape[0] * sigma_inv + v0_inv )\n","  mup = np.dot(sigma_inv, np.sum(theta, axis=0)) + np.dot(v0_inv, mu0)\n","  mup = np.dot(vp, mup)\n","  return st.multivariate_normal.rvs(mean=mup, cov=vp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wzDIEZhWtFrj"},"source":["# Σ（θ_new の共分散行列）\n","\n","def sampler_Sigma_inv(theta, mu, nu0, sigma0_inv):\n","  nup = nu0 + theta.shape[0]\n","  sigmap_inv = sigma0_inv\n","  for ii in range(theta.shape[0]):\n","    tmp = (theta[ii] - mu)[:,np.newaxis]\n","    sigmap_inv += np.dot(tmp, tmp.T)\n","  return st.wishart.rvs(df=nup, scale=np.linalg.inv(sigmap_inv))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfMPoV7xtFpS"},"source":["# τ（σy の生成過程のパラメータ）\n","\n","def sampler_tau(sigma_y, n0, m0, tau0):\n","  mp = m0 + n0*sigma_y.shape[0]\n","  taup = tau0 + np.sum(1/sigma_y**2)\n","  return st.gamma.rvs(a=mp/2, scale=taup/2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mNIgXi8tFmu"},"source":["# サンプリング\n","# 初期値設定\n","alpha = 1\n","n0 = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qH1nRa9ssmsq"},"source":["# 事前分布のパラメータ群\n","mu0 = np.array([0, 0])\n","v0_tmp = np.random.uniform(-10, 10, (2, 2))   #np.array([[0.5, 0], [0,4]])\n","v0 = np.dot(v0_tmp, v0_tmp.T)\n","v0_inv = np.linalg.inv(v0)\n","\n","nu0 = 2\n","sigma0_tmp = np.random.uniform(-10, 10, (2, 2))   #np.array([[0.5, 0], [0,10]])   #np.random.uniform(-1, 1, (2, 2))\n","sigma0 = np.dot(sigma0_tmp, sigma0_tmp.T)   # positive definite\n","sigma0_inv = np.linalg.inv(sigma0)\n","\n","m0 = 0.5\n","tau0 = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eO_9LNRHugZq"},"source":["# パラメータの初期値\n","\n","T = x.shape[0]\n","\n","theta = np.array([[0.5,1], [-0.3, 25]])#, [0.1, 1]])\n","sigma_y = np.sqrt(np.array([0.2, 0.2]))#, 0.2]))\n","z = np.repeat(np.array([0,1]), 45)#np.repeat(np.array([0,1,2]), 30) #np.zeros(T, dtype=\"int\")\n","print(z)\n","mu = st.multivariate_normal.rvs(mean=mu0, cov=v0)\n","sigma = st.wishart.rvs(df=nu0, scale=sigma0)\n","sigma_inv = np.linalg.inv(sigma)\n","tau = st.gamma.rvs(a=m0/2, scale=tau0/2)\n","print(\"mu : \", mu)\n","print(\"Sigma : \", sigma)\n","print(\"tau :\", tau)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fzh1bpH5ugW4"},"source":["# ギブスサンプリング\n","# 本では少なくとも12000ステップのサンプリングを行っていたが、ここでは2000ステップ（うちバーンイン1000ステップ）と設定した\n","%%time\n","n_step = 2000\n","burnin = 1000\n","z_sample = np.zeros((n_step, T))\n","theta_sample = np.zeros((n_step, T, theta.shape[1]))\n","\n","n_cluster = theta.shape[0]\n","z_unique = np.unique(z)\n","for step in range(n_step):\n","  #print(step)\n","  for tt in range(T):\n","    z_new, theta_new, sigma_new = sampler_z(tt, y[tt], x[tt], theta, sigma_y, z, alpha, T, mu, sigma, n0, tau)\n","    z[tt] = z_new\n","    if z_unique is not np.unique(z):\n","      z_unique = np.unique(z)\n","      # クラスタ数が増えた場合の処理\n","      if n_cluster < z_unique.shape[0]:\n","        n_cluster += 1\n","        theta = np.r_[theta, [theta_new]]\n","        sigma_y = np.r_[sigma_y, sigma_new]\n","        #print(z_new, theta, sigma_y)\n","      # クラスタ数が減った場合の処理\n","      elif n_cluster > z_unique.shape[0]:\n","        n_cluster -= 1\n","        theta = theta[z_unique]\n","        sigma_y = sigma_y[z_unique]\n","      #Zを0,1,2,...に置き換える処理\n","      for ii, z_val in zip(range(z_unique.shape[0]), z_unique):\n","        z[z==z_val] = ii\n","\n","  for kk in range(n_cluster):\n","    # sampler_thetaに渡すsigma_invは、copy()を取らないと内部で値が変化してしまう\n","    theta[kk] = sampler_theta(kk, y, x, z, sigma_y, mu, sigma_inv.copy())\n","    sigma_y[kk] = sampler_sigma_y(kk, y, x, z, theta, n0, tau)\n","  mu = sampler_mu(theta, sigma_inv, mu0, v0_inv)\n","  sigma_inv = sampler_Sigma_inv(theta, mu, nu0, sigma0_inv.copy())\n","  tau = sampler_tau(sigma_y, n0, m0, tau0)\n","\n","  z_sample[step] = z\n","  theta_sample[step] = np.r_[theta, np.repeat([np.repeat(np.nan,theta.shape[1])], T-theta.shape[0], axis=0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f3xJlcE1ugRK"},"source":["np.unique(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4_OGPR5ugMg"},"source":["# 結果\n","# クラスタ数の分布\n","print(np.unique(np.max(z_sample[burnin:].astype('int')+1, axis=1), return_counts=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4_z8KwBugJ_"},"source":["fig, ax = plt.subplots()\n","ax.hist(np.max(z_sample[burnin:].astype('int'), axis=1)+1)\n","ax.set_xlabel('n_cluster')\n","fig.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vxcaojIsmqG"},"source":["# 傾き（勾配）の事後分布\n","z_after_burnin = z_sample[burnin:,:].astype('int')\n","theta_after_burnin = theta_sample[burnin:,:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1xbmSCkiIgSd"},"source":["# 時刻0における傾きの事後分布\n","grad = [theta_after_burnin[ii,z_after_burnin[ii,0]] for ii in range(z_after_burnin.shape[0])]\n","grad = np.array(grad)\n","grad.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5CES5eHvkqS"},"source":["sns.distplot(grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugxXw0SQvklH"},"source":["# この結果を全時刻で統合する\n","grad_all = np.empty((T,n_step - burnin))\n","for tt in range(T):\n","  grad = [theta_after_burnin[ii,z_after_burnin[ii,tt]] for ii in range(z_after_burnin.shape[0])]\n","  grad_all[tt] = np.array(grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKfy8lxavkbr"},"source":["fig, ax = plt.subplots()\n","ax.plot(x, np.mean(grad_all, axis=1))\n","ax.fill_between(x, np.mean(grad_all, axis=1) +np.std(grad_all, axis=1), np.mean(grad_all, axis=1) -np.std(grad_all, axis=1),\n","               alpha=0.3, color=\"purple\")\n","ax.set_xlabel('x')\n","ax.set_ylabel('grad')\n","fig.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SFmKZoWq4bxi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bv5Y5i1vkjY8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sm2CJ-cdbZaj"},"source":["### <font color=blue>**4.** </font> 階層ディレクレ過程（HDP-LDA : Hierechical Dirichlet Process - Latent Dirichlet Allocation）"]},{"cell_type":"markdown","metadata":{"id":"nfppHzioGW-4"},"source":["#### <font color=green>**4.1.** </font> GENSIM core-tutorials"]},{"cell_type":"code","metadata":{"id":"9KeJs-we1f2t"},"source":["## https://radimrehurek.com/gensim/models/hdpmodel.html\n","## https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7-Jba4j1fxY"},"source":["!pip install gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkpyFw611fva"},"source":["from gensim.test.utils import common_corpus, common_dictionary\n","from gensim.models import HdpModel\n","\n","hdp = HdpModel(common_corpus, common_dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWlQ2GS51ftM"},"source":["# You can then infer topic distributions on new, unseen documents, with\n","unseen_document = [(1, 3.), (2, 4)]\n","doc_hdp = hdp[unseen_document]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ur3M3dI-5US2"},"source":["doc_hdp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSmw6sMI1frI"},"source":["# To print 20 topics with top 10 most probable words.\n","topic_info = hdp.print_topics(num_topics=20, num_words=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sxBD-1Jn1fnX"},"source":["topic_info[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2QYsSEY1cbw"},"source":["# The model can be updated (trained) with new documents via\n","hdp.update([[(1, 2)], [(1, 1), (4, 5)]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1QbZByh1cZd"},"source":["doc_hdp_2 = hdp[unseen_document]\n","doc_hdp_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EpO2RRMN1cVJ"},"source":["topic_info_2 = hdp.print_topics(num_topics=20, num_words=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSf_TQfo1cSR"},"source":["topic_info_2[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e5Ru8pWJ6QNu"},"source":["for i in range(20):\n","  print(i, '\\t', topic_info[i] == topic_info_2[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBtdbyTjGWwu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TgOVUZvGYYb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0c8n0AP3GWjf"},"source":["#### <font color=green>**4.2.** </font> トピックモデルへの適用例"]},{"cell_type":"code","metadata":{"id":"Uv4_pT7FAYcz"},"source":["## 出典 : https://qiita.com/u6k/items/5170b8d8e3f41531f08a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZ8kRYILAYZa"},"source":["import gensim\n","from gensim import corpora"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkPg97i3AYUr"},"source":["documents = [\"Human machine interface for lab abc computer applications\",\n","             \"A survey of user opinion of computer system response time\",\n","             \"The EPS user interface management system\",\n","             \"System and human system engineering testing of EPS\",\n","             \"Relation of user perceived response time to error measurement\",\n","             \"The generation of random binary unordered trees\",\n","             \"The intersection graph of paths in trees\",\n","             \"Graph minors IV Widths of trees and well quasi ordering\",\n","             \"Graph minors A survey\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"163nJO4CAYPS"},"source":["# ストップワードを定義\n","stop_words = set('for a of the and to in'.split())\n","\n","# 文を単語に分割し、ストップワードを除去した配列を作成\n","texts = [[word for word in document.lower().split() if word not in stop_words] for document in documents]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPx86y9eAYNc"},"source":["print(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PejxGzpAYLZ"},"source":["from pprint import pprint\n","pprint(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuK-L1ycBE3X"},"source":["# 単語の出現回数を格納するfrequency変数を定義\n","from collections import defaultdict\n","frequency = defaultdict(int)\n","\n","# 単語の出現回数をfrequency変数でカウント\n","for text in texts:\n","  for token in text:\n","    frequency[token] += 1\n","\n","# frequency変数で1より上の単語のみを配列に構築\n","texts_2 = [[token for token in text if frequency[token] > 1] for text in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMK7_hmuBE0P"},"source":["pprint(texts_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OauQ6C7HBEwb"},"source":["dictionary = corpora.Dictionary(texts_2)\n","\n","# ファイルに保存できます\n","#dictionary.save('/tmp/deerwester.dict')\n","\n","# テキストファイルに保存することもできます\n","#dictionary.save_as_text('/tmp/deerwester.dict.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKa4rUFLBErZ"},"source":["corpus = [dictionary.doc2bow(text) for text in texts_2]\n","\n","# ファイルに保存できる\n","#corpora.MmCorpus.serialize('/tmp/deerwester.mm', corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZAtLBwicBEo8"},"source":["pprint(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SYtwExtyBEmU"},"source":["# num_topics=5で、5個のトピックを持つLDAモデルを作成\n","lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=5, id2word=dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpbuCnbEBy1R"},"source":["pprint(lda.show_topics())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LRWtAzzRByxT"},"source":["# 文を定義\n","test_documents = [\"Computer themselves and software yet to be developed will revolutionize the way we learn\"]\n","\n","# 単語を分割\n","test_texts = [[word for word in document.lower().split()] for document in test_documents]\n","\n","# 既存の辞書を使用して、コーパスを作成\n","test_corpus = [dictionary.doc2bow(text) for text in test_texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gjidk2LBByrO"},"source":["pprint(test_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pzc4qnH4Byou"},"source":["for topics_per_document in lda[test_corpus]:\n","  pprint(topics_per_document)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKrAsKWeGWLp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d457tY7gGYrM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ki9f7MIiunda"},"source":["#### <font color=green>**4.3.** </font> トピックモデルへの適用例　その２"]},{"cell_type":"code","metadata":{"id":"MFs4Oqu-vhKN"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.datasets import fetch_20newsgroups\n","\n","data_samples, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n","                             remove=('headers', 'footers', 'quotes'),\n","                             return_X_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVKKRQjkvhPz"},"source":["stop_words = set('¥n'.split())\n","\n","texts = [[word for word in data_samples.lower().split() if word not in stop_words] for data_samples in data_samples]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJ5_YFMfvhTI"},"source":["from collections import defaultdict\n","frequency = defaultdict(int)\n","\n","for text in texts:\n","  for token in text:\n","    frequency[token] += 1\n","\n","texts = [[token for token in text if frequency[token] >= 1] for text in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSjJ4jATvhNI"},"source":["import gensim\n","from gensim import corpora"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vp-KPpfZyuvH"},"source":["dictionary = corpora.Dictionary(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJUWHOFFyvAE"},"source":["corpus = [dictionary.doc2bow(text) for text in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tun93BSX5CVX"},"source":["num_topics=100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hmMA-qZzDSA"},"source":["test_documents = [\"In a new ad for Omaze, George Clooney pokes fun at the nipples on his Batsuit costume from Joel Schumacher’s notorious Batman & Robin from 1997\"]\n","\n","test_texts = [[word for word in document.lower().split()] for document in test_documents]\n","\n","test_corpus = [dictionary.doc2bow(text) for text in test_texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3tgRoK7zDYX"},"source":["lda = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n","hdp = gensim.models.hdpmodel.HdpModel(corpus=corpus, id2word=dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AB2rlP3VzDhi"},"source":["import matplotlib.pyplot as plt\n","def draw_graph(test_corpus,index):\n","  y = [0 for i in range(num_topics)]\n","  for i in range(num_topics):\n","    for k in range(len(test_corpus)):\n","      if test_corpus[k][0] == i:\n","        y[i] = test_corpus[k][1]\n","  x=[i for i in range(num_topics)]\n","  plt.subplot(1, 2, index)\n","  plt.bar(x,y)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STfYND4iFhuC"},"source":["plt.figure(figsize=(20,5))\n","draw_graph(list(lda[test_corpus])[0],1)\n","draw_graph(list(hdp[test_corpus])[0],2)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"viQypkuq3Yhh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSIC9eSSunD-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gmpaWSCacci_"},"source":["### <font color=blue>**5.** </font> インド料理店過程 : Indian buffet process"]},{"cell_type":"code","metadata":{"id":"7Bbod4ZIAu2C"},"source":["## 出典 : https://github.com/Ma-sa-ue/practice/tree/master/machine%20learning(python)/nonparabayes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5muvIqvIECMZ"},"source":["import numpy as np\n","import scipy as sp\n","import scipy.stats as st\n","import matplotlib.pyplot as plt\n","\n","np.random.seed(100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6WX9KHrEi-b"},"source":["def get_ibp(alpha):\n","  z = np.zeros([15,30])\n","  new = 0\n","  for j in range(15):\n","    ### old phase\n","    to_beru = np.sum(z,0)\n","    for k in range(new):\n","      z[j,k]=np.random.binomial(1,to_beru[k]*1.0/(j+1))\n","    ### new phase\n","    new_alpha = alpha*1.0/(j+1)\n","    new_to_add = np.random.poisson(new_alpha)\n","    z[j,new:new+new_to_add]=1\n","    #### old phase\n","    ##for k in range(len(new_to_add)):\n","    ### update new\n","    new = new+new_to_add\n","  return z"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biB8xQ-CECKG"},"source":["para =[1.0,4.0,9.0]\n","plt.figure(figsize=(16,5))\n","for i in range(3):\n","  plt.subplot(1,3,i+1)\n","  plt.title(\"alpha=\"+str(para[i]))\n","  hoge = get_ibp(para[i])\n","  plt.imshow(hoge,interpolation='none' )\n","plt.savefig(\"IBP.png\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dCJS0C7HieH"},"source":["#### generate sample\n","n = 20\n","true_z = []\n","for i in range(4):\n","  true_z.append(np.zeros(25).reshape(5,5))\n","  if i==0:\n","    true_z[i][0:3,0:3]=1\n","  elif i==1:\n","    true_z[i][0:3,3:5]=1\n","    true_z[i][0,2]=1\n","    true_z[i][3,4]=1\n","  elif i==2:\n","    true_z[i][3:5,0:3]=1\n","    true_z[i][0,0]=1\n","    true_z[i][4,2]=0\n","  elif i==3:\n","    true_z[i][3:5,2:5]=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NFZOjneEHiaJ"},"source":["true_z_flat2 = np.array([i.reshape(1,25) for i in true_z])\n","print(true_z_flat2.shape)\n","true_z_flat2 = np.transpose(true_z_flat2.reshape(4,25))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G8EXdZ0cEQHr"},"source":["for i in range(4):\n","  plt.subplot(2,2,i+1)\n","  plt.imshow(true_z[i],interpolation='none')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"81R3D2CzHpjg"},"source":["plt.imshow(true_z_flat2,interpolation='none')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBhaJ72yEQFA"},"source":["xxx = np.random.normal(0,2.0,4*n).reshape(4,n)\n","yyy = np.dot(true_z_flat2,xxx) + np.random.normal(0,0.1,25*n).reshape((25,n))\n","\n","plt.figure(figsize=(8,8))\n","plt.subplot(1,3,1)\n","plt.tick_params(labelbottom='off')\n","plt.tick_params(labelleft='off')\n","plt.imshow(yyy,interpolation='none')\n","\n","plt.subplot(1,3,2)\n","plt.tick_params(labelbottom='off')\n","plt.tick_params(labelleft='off')\n","plt.imshow(true_z_flat2,interpolation='none')\n","#plt.savefig(\"situation.png\")\n","\n","plt.subplot(1,3,3)\n","plt.tick_params(labelbottom='off')\n","plt.tick_params(labelleft='off')\n","plt.imshow(xxx,interpolation='none')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TZteqkqzEQB7"},"source":["def factorial(n):\n","  if n==0:\n","    return 1\n","  elif n>=1:\n","    return n*factorial(n-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WWowpbAE-43"},"source":["def poisson_pdf(_lambda,x):\n","  return np.power(_lambda,x)*np.exp(-_lambda)*1.0/factorial(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tZDGlLkE-2f"},"source":["def gibbs_for_z(i,k,z,Y,X,sigma_y,alpha,N):\n","  ###### first calculate n_i_k\n","  pesdo_n_i_k = np.sum(z,0)[k]\n","  if z[i,k]==1:\n","    n_i_k = pesdo_n_i_k -1\n","  else:\n","    n_i_k = pesdo_n_i_k\n","    \n","  hoge1 = pesdo_n_i_k*1.0/N\n","  hoge0 = 1-hoge1\n","  ##### calculate likelihood\n","  z1 = np.copy(z[i,:].reshape(z.shape[1]))\n","  z1[k]=1\n","  mean1 = np.dot(z1,X)\n","  z2 = np.copy(z[i,:].reshape(z.shape[1]))\n","  z2[k]=0\n","  mean2 = np.dot(z2,X)\n","  sigma = np.identity(20)*sigma_y\n","  hoge3 = st.multivariate_normal.pdf(Y[i],mean1,sigma)*hoge1\n","  hoge4 = st.multivariate_normal.pdf(Y[i],mean2,sigma)*hoge0\n","  return np.random.binomial(1,hoge3*1.0/(hoge3+hoge4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-TZ3EYJE-zy"},"source":["def gibbs_for_m(i,z,Y,X,sigma_y,sigma_x,alpha,N):\n","  sigma = np.identity(20)*sigma_y\n","  new_x = np.random.normal(0,2.0,100).reshape((5,20))\n","  candidate_list = []\n","  for j in range(4):\n","    p0 = poisson_pdf(alpha*1.0/N,j)\n","    hoge = st.multivariate_normal.pdf(Y[i],np.dot(z[i,:],X)+np.sum(new_x[0:j],0),sigma_y)\n","    candidate_list.append(p0*hoge)\n","  candidate_list = candidate_list/np.sum(np.array(candidate_list))\n","  multi_list = np.random.multinomial(1,candidate_list)\n","  for kkk in range(5):\n","    if multi_list[kkk]==1:\n","      newx = np.concatenate([X,new_x[0:kkk]])\n","      newz = np.concatenate([z,np.zeros(Y.shape[0]*4).reshape((Y.shape[0],4))[:,0:kkk]],1)\n","      if kkk!=0:\n","        newz[i,z.shape[1]:z.shape[1]+kkk] = 1\n","      return [newx,newz]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbt2-GKREP_G"},"source":["def gibbs_for_x(z,Y,X,sigma_y,sigma_x,alpha,N):\n","  newx = []\n","  inverse_vx = np.dot(np.transpose(z),z)+sigma_y*1.0/sigma_x*np.identity(z.shape[1])\n","  vx = np.linalg.inv(inverse_vx)\n","  vxy = sigma_y*vx\n","  for i in range(Y.shape[1]):\n","    yi = Y[:,i]\n","    mean = np.dot(vx,np.dot(np.transpose(z),yi))\n","    variance = vxy\n","    newx.append(np.random.multivariate_normal(mean,vxy))\n","  return np.transpose(np.array(newx))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02bfJICmEP8n"},"source":["x = np.random.normal(0,2.0,1*n).reshape(1,n)\n","true_z_flat = np.random.binomial(1,0.1,25).reshape((25,1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJSVB77iEP2p"},"source":["alpha = 1   ###\n","\n","plt.figure(figsize=(16,5))\n","for sss in range(36):\n","  i,j = true_z_flat.shape\n","  pesdo_z = np.zeros(i*j).reshape((i,j))\n","  #####\n","  for kk1 in range(i):\n","    for kk2 in range(j):\n","      pesdo_z[kk1,kk2] = gibbs_for_z(kk1,kk2,true_z_flat,yyy,x,0.3,alpha,n)\n","  true_z_flat = pesdo_z\n","  #######\n","  for j in range(25):\n","    x,true_z_flat = gibbs_for_m(j,true_z_flat,yyy,x,0.3,2.0,alpha,n)\n","  if sss%3==0:\n","    plt.subplot(1,12,sss/3+1)\n","    plt.imshow(true_z_flat,interpolation='none')\n","    plt.title(str(sss)+\"steps\")\n","    plt.axis('off')\n","  ############\n","  x= gibbs_for_x(true_z_flat,yyy,x,0.3,2.0,alpha,n)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z98zaipVEP0Z"},"source":["plt.imshow(true_z_flat2,interpolation='none')\n","plt.title(\"true z\")\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcWwDQH8rVSw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98-npHnqra2_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IG9_WM-mbXBu"},"source":[""],"execution_count":null,"outputs":[]}]}