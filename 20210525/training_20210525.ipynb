{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_20210525.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IPDLYbOSrD_7",
        "UDOMYkT1rs9m",
        "6JJPOvHVP7yM",
        "-erM88KQ8Zo4",
        "tVOcDp-XCy5v",
        "WSxXv-Cxrs6g",
        "uK64THRg8qdO",
        "XUHcR21urs_7",
        "o-ERMWOCbL5n",
        "rBeMK1kprstg",
        "0-OFs7I7rtCZ",
        "rNTFnWgmrEW5",
        "NA82k7ZqrtEn",
        "JwTCWNUSrtHN",
        "2qwNqG89rEZK",
        "Fe-sXO4yrtJm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnijj-lZg_s"
      },
      "source": [
        "## 48. 強化学習（RL : Reinforcement Learning）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPDLYbOSrD_7"
      },
      "source": [
        "### <font color=blue>**1.** </font> Q学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDOMYkT1rs9m"
      },
      "source": [
        "#### <font color=green>**1.1.** </font> 迷路を解く　その１"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REHsliPRFxYQ"
      },
      "source": [
        "## 出典 : https://book.mynavi.jp/manatee/detail/id=88714"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXIQx9vvHl3Y"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi2ZFF_dHz9f"
      },
      "source": [
        "# 初期位置での迷路の様子\n",
        " \n",
        "# 図を描く大きさと、図の変数名を宣言\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        " \n",
        "# 赤い壁を描く\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        " \n",
        "# 状態を示す文字S0～S8を描く\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        " \n",
        "# 描画範囲の設定と目盛りを消す設定\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
        "                labelbottom='off', right='off', left='off', labelleft='off')\n",
        " \n",
        "# 現在値S0に緑丸を描画する\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaon3LSaHz9g"
      },
      "source": [
        "# 初期の方策を決定するパラメータtheta_0を設定\n",
        " \n",
        "# 行は状態0～7、列は移動方向で↑、→、↓、←を表す\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7\n",
        "                    # ※s8はゴールなので、方策はなし\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBZ7CMllHz9h"
      },
      "source": [
        "# 方策パラメータtheta_0をランダム方策piに変換する関数の定義\n",
        " \n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "  '''単純に割合を計算する'''\n",
        "  [m, n] = theta.shape  # thetaの行列サイズを取得\n",
        "  pi = np.zeros((m, n))\n",
        "  for i in range(0, m):\n",
        "    pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 割合の計算\n",
        "  pi = np.nan_to_num(pi)  # nanを0に変換 \n",
        "  return pi\n",
        " \n",
        "# ランダム行動方策pi_0を求める\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO-dKoSkFxV2"
      },
      "source": [
        "# Q学習による行動価値関数Qの更新\n",
        " \n",
        "def Q_learning(s, a, r, s_next, Q, eta, gamma):\n",
        "  if s_next == 8:  # ゴールした場合\n",
        "    Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        " \n",
        "  else:\n",
        "    Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next,: ]) - Q[s, a])\n",
        "    #Q[s, a] = Q[s, a] + eta * (r + gamma * Q[s_next, a_next] - Q[s, a])\n",
        " \n",
        "  return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyPrOrIYFxTk"
      },
      "source": [
        "# Q学習で迷路を解く関数の定義、状態と行動の履歴および更新したQを出力\n",
        " \n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0):\n",
        "  s = 0  # スタート地点\n",
        "  s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
        " \n",
        "  while (1):  # ゴールするまでループ\n",
        "    [a, s_next] = get_action_and_s_next(s, Q, epsilon, pi_0)\n",
        "    s_a_history[-1][1] = a\n",
        "    # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n",
        " \n",
        "    s_a_history.append([s_next, np.nan])\n",
        "    # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        " \n",
        "    # 報酬を与え,　次の行動を求めます\n",
        "    if s_next == 8:\n",
        "      r = 1  # ゴールにたどり着いたなら報酬を与える\n",
        "      a_next = np.nan\n",
        "    else:\n",
        "      r = 0\n",
        " \n",
        "    # 価値関数を更新\n",
        "    Q = Q_learning(s, a, r, s_next, Q, eta, gamma)\n",
        " \n",
        "    # 終了判定\n",
        "    if s_next == 8:  # ゴール地点なら終了\n",
        "      break\n",
        "    else:\n",
        "      s = s_next\n",
        " \n",
        "  return [s_a_history, Q]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qttFaorjHgVi"
      },
      "source": [
        "# ε-greedy法を実装\n",
        " \n",
        "def get_action_and_s_next(s, Q, epsilon, pi_0):\n",
        "  direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        " \n",
        "  # 行動を決める\n",
        "  if np.random.rand() < epsilon:\n",
        "    # εの確率でランダムに動く\n",
        "    next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
        "  else:\n",
        "    # Qの最大値の行動を採用する\n",
        "    next_direction = direction[np.nanargmax(Q[s, :])]\n",
        " \n",
        "  # 決めた行動で次の状態を決める\n",
        "  if next_direction == \"up\":\n",
        "    action = 0\n",
        "    s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n",
        "  elif next_direction == \"right\":\n",
        "    action = 1\n",
        "    s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
        "  elif next_direction == \"down\":\n",
        "    action = 2\n",
        "    s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n",
        "  elif next_direction == \"left\":\n",
        "    action = 3\n",
        "    s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
        " \n",
        "  return [action, s_next]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYT9N8_jFyGq"
      },
      "source": [
        "# 初期の行動価値関数Qを設定\n",
        " \n",
        "[a, b] = theta_0.shape  # 行と列の数をa, bに格納\n",
        "Q = np.random.rand(a, b) * theta_0 * 0.1\n",
        "# *theta0をすることで要素ごとに掛け算をし、Qの壁方向の値がnanになる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHycHFpXGlEB"
      },
      "source": [
        "# Q学習で迷路を解く\n",
        " \n",
        "eta = 0.1  # 学習率\n",
        "gamma = 0.9  # 時間割引率\n",
        "epsilon = 0.5  # ε-greedy法の初期値\n",
        "v = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\n",
        "is_continue = True\n",
        "episode = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ybmLEwHFyCe"
      },
      "source": [
        "V=[] # エピソードごとの状態価値を格納する\n",
        "V.append(np.nanmax(Q, axis=1))  # 状態ごとに行動価値の最大値を求める\n",
        " \n",
        "while is_continue:  # is_continueがFalseになるまで繰り返す\n",
        "  print(\"エピソード:\" + str(episode))\n",
        " \n",
        "  # ε-greedyの値を少しずつ小さくする\n",
        "  epsilon = epsilon / 2\n",
        " \n",
        "  # Q学習で迷路を解き、移動した履歴と更新したQを求める\n",
        "  [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
        " \n",
        "  # 状態価値の変化\n",
        "  new_v = np.nanmax(Q, axis=1)  # 状態ごとに行動価値の最大値を求める\n",
        "  print(np.sum(np.abs(new_v - v)))  # 状態価値関数の変化を出力\n",
        "  v = new_v\n",
        "  V.append(v) # このエピソード終了時の状態価値関数を追加\n",
        " \n",
        "  print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
        " \n",
        "  # 100エピソード繰り返す\n",
        "  episode = episode + 1\n",
        "  if episode > 100:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSYP8GXGk_2"
      },
      "source": [
        "# 状態価値の変化を可視化\n",
        "# 参考URL http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "import matplotlib.cm as cm  # color map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQFEoEFwISav"
      },
      "source": [
        "def init():\n",
        "  # 背景画像の初期化\n",
        "  line.set_data([], [])\n",
        "  return (line,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTKe00SkISU2"
      },
      "source": [
        "def animate(i):\n",
        "  # フレームごとの描画内容\n",
        "  # 各マスに状態価値の大きさに基づく色付きの四角を描画\n",
        "  line, = ax.plot([0.5], [2.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][0]), markersize=85)  # S0\n",
        "  line, = ax.plot([1.5], [2.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][1]), markersize=85)  # S1\n",
        "  line, = ax.plot([2.5], [2.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][2]), markersize=85)  # S2\n",
        "  line, = ax.plot([0.5], [1.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][3]), markersize=85)  # S3\n",
        "  line, = ax.plot([1.5], [1.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][4]), markersize=85)  # S4\n",
        "  line, = ax.plot([2.5], [1.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][5]), markersize=85)  # S5\n",
        "  line, = ax.plot([0.5], [0.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][6]), markersize=85)  # S6\n",
        "  line, = ax.plot([1.5], [0.5], marker=\"s\",\n",
        "                color=cm.jet(V[i][7]), markersize=85)  # S7\n",
        "  line, = ax.plot([2.5], [0.5], marker=\"s\",\n",
        "                color=cm.jet(1.0), markersize=85)  # S8\n",
        "  return (line,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jq8vqs3FyAM"
      },
      "source": [
        "#　初期化関数とフレームごとの描画関数を用いて動画を作成\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, animate, init_func=init, frames=len(V), interval=200, repeat=False)\n",
        " \n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFRXXc88tgHf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JJPOvHVP7yM"
      },
      "source": [
        "#### <font color=green>**1.2.** </font> Tic Tac Toe（三目並べ）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zejCO_GfTqcs"
      },
      "source": [
        "##　出典 : https://qiita.com/thinking_vecta/items/f5b52311d2c0f6a56dc6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSFltYQiTqZu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import random\n",
        "import time\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAIUUhX3T0NL"
      },
      "source": [
        "def get_player_input(play_area, first_inputter):\n",
        "    \"\"\"プレイヤーから入力を受け付ける関数\n",
        "    ゲームの状況をあらわすリストを受け取り、プレイヤーの入力で更新したリストと入力を返す\n",
        "    \"\"\"\n",
        "    choosable_area = [str(area) for area in play_area if type(area) is int]\n",
        "    while(True):\n",
        "        player_input = input('Choose a number!>>>')\n",
        "        if player_input in choosable_area:\n",
        "            player_input = int(player_input)\n",
        "            break\n",
        "        else:\n",
        "            print('Wrong input!\\nChoose a number from' \\\n",
        "                  '{}'.format(choosable_area))\n",
        "    if first_inputter == 1:\n",
        "        play_area[play_area.index(player_input)] = '○'\n",
        "    elif first_inputter == 2:\n",
        "        play_area[play_area.index(player_input)] = '×'\n",
        "    return play_area, player_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRULkkcIT0KB"
      },
      "source": [
        "def get_ai_input(play_area, first_inputter, mode=0, q_table=None, epsilon=None):\n",
        "    \"\"\"AIの入力を受け付ける関数\n",
        "    ゲームの状況をあらわすリストとAIのモードおよびその他のオプションを受け取り、AIの入力で更新したリストと入力を返す\n",
        "    \"\"\"\n",
        "    choosable_area = [str(area) for area in play_area if type(area) is int]\n",
        "    if mode == 0:\n",
        "        ai_input = int(random.choice(choosable_area))\n",
        "    elif mode == 1:\n",
        "        ai_input = get_ql_action(play_area, choosable_area, q_table, epsilon)\n",
        "    if first_inputter == 1:\n",
        "        play_area[play_area.index(ai_input)] = '×'\n",
        "    elif first_inputter == 2:\n",
        "        play_area[play_area.index(ai_input)] = '○'\n",
        "    return play_area, ai_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFUj1rwUT0HO"
      },
      "source": [
        "def show_play(play_area, inputter=0, inputted=0):\n",
        "    \"\"\"TIC TAC TOEの画面を表示する関数\n",
        "    表示すべきリスト(1～9の数値、○、×から成る)と直前の入力者および入力を受け取り、表示する\n",
        "    \"\"\"\n",
        "    clear_output()\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.plot()\n",
        "    plt.xticks([0, 5, 10, 15])\n",
        "    plt.yticks([0, 5, 10, 15])\n",
        "    plt.tick_params(labelbottom='off', bottom='off')\n",
        "    plt.tick_params(labelleft='off', left='off')\n",
        "    plt.xlim(0, 15)\n",
        "    plt.ylim(0, 15)\n",
        "\n",
        "    x_pos = [2.5, 7.5, 12.5]\n",
        "    y_pos = [2.5, 7.5, 12.5]\n",
        "\n",
        "    markers = ['$' + str(marker) + '$' for marker in play_area]\n",
        "\n",
        "    marker_count = 0\n",
        "    for y in reversed(y_pos):\n",
        "        for x in x_pos:\n",
        "            if markers[marker_count] == '$○$':\n",
        "                color = 'r'\n",
        "            elif markers[marker_count] == '$×$':\n",
        "                color = 'k'\n",
        "            else:\n",
        "                color = 'b'\n",
        "            plt.plot(x, y, marker=markers[marker_count], \n",
        "                     markersize=30, color=color)\n",
        "            marker_count += 1\n",
        "    if inputter == 0:\n",
        "        title = 'Play the TIC TAC TOE!!'\n",
        "    else:\n",
        "        title = '{} chose {}!!'.format(inputter, inputted)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skSuzezQT0Em"
      },
      "source": [
        "def judge(play_area, inputter):\n",
        "    \"\"\"ゲーム終了及び勝者を判定する\n",
        "    ゲームの状況をあらわすリストと直前の入力者を受け取り、ゲームが終了していれば勝者と終了判定を返す\n",
        "    \"\"\"\n",
        "    end_flg = 0\n",
        "    winner = 'NOBODY'\n",
        "    first_list = [0, 3, 6, 0, 1, 2, 0, 2]\n",
        "    second_list = [1, 4, 7, 3, 4, 5, 4, 4]\n",
        "    third_list = [2, 5, 8, 6, 7, 8, 8, 6]\n",
        "    for first, second, third in zip(first_list, second_list, third_list):\n",
        "        if play_area[first] == play_area[second] \\\n",
        "        and play_area[first] == play_area[third]:\n",
        "            winner = inputter\n",
        "            end_flg = 1\n",
        "            break\n",
        "    choosable_area = [str(area) for area in play_area if type(area) is int]\n",
        "    if len(choosable_area) == 0:\n",
        "        end_flg = 1\n",
        "    return winner, end_flg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36XQVHjATqXL"
      },
      "source": [
        "def player_vs_randomAI(first_inputter):\n",
        "    \"\"\"プレイヤーとAI(ランダム)のゲームを実行する関数\n",
        "    先手(1:プレイヤー、2:AI)を受け取り、ゲームが終了するまで実行する\n",
        "    \"\"\"\n",
        "    inputter1 = 'YOU'\n",
        "    inputter2 = 'AI'\n",
        "\n",
        "    play_area = list(range(1, 10))\n",
        "    show_play(play_area)\n",
        "    inputter_count = first_inputter\n",
        "    end_flg = 0\n",
        "    while True:\n",
        "        if (inputter_count % 2) == 1:\n",
        "            print('Your turn!')\n",
        "            play_area, player_input = get_player_input(play_area, first_inputter)\n",
        "            show_play(play_area, inputter1, player_input)\n",
        "            winner, end_flg = judge(play_area, inputter1)\n",
        "            if end_flg:\n",
        "                break\n",
        "        elif (inputter_count % 2) == 0:\n",
        "            print('AI\\'s turn!\\n.\\n.\\n.')\n",
        "            play_area, ai_input = get_ai_input(play_area, first_inputter, mode=0)\n",
        "            sleep(3)\n",
        "            show_play(play_area, inputter2, ai_input)\n",
        "            winner, end_flg = judge(play_area, inputter2)\n",
        "            if end_flg:\n",
        "                break\n",
        "        inputter_count += 1\n",
        "    print('{} win!!!'.format(winner))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm4un0s0TkDB"
      },
      "source": [
        "# ゲームしてみる\n",
        "# 引数1:プレイヤー先手\n",
        "# 引数2:プレイヤー後手\n",
        "\n",
        "player_vs_randomAI(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCqjcVncT-N0"
      },
      "source": [
        "# Qテーブル作成\n",
        "def make_q_table():\n",
        "    \"\"\"Qテーブルを作成する関数\n",
        "    \"\"\"\n",
        "    n_columns = 9\n",
        "    n_rows = 3**9\n",
        "    return np.zeros((n_rows, n_columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9sq6yp5USS1"
      },
      "source": [
        "def q_learning(play_area, ai_input, reward, play_area_next, q_table, end_flg):\n",
        "    \"\"\"Qテーブルを更新する関数\n",
        "    ゲームの状況をあらわすリスト・AIの行動・報酬・１手番後のゲームの状況をあらわすリスト・\n",
        "    Qテーブル・勝利フラグを受け取り、更新したQテーブルを返す\n",
        "    \"\"\"\n",
        "    # 行番号取得\n",
        "    row_index = find_q_row(play_area)\n",
        "    row_index_next = find_q_row(play_area_next)\n",
        "    column_index = ai_input - 1\n",
        "    # 勝利した or 敗北した場合\n",
        "    if end_flg == 1:\n",
        "        q_table[row_index, column_index] = \\\n",
        "        q_table[row_index, column_index] + eta \\\n",
        "        * (reward - q_table[row_index, column_index])\n",
        "    # まだ続いている場合以外\n",
        "    else:\n",
        "        q_table[row_index, column_index] = \\\n",
        "        q_table[row_index, column_index] + eta \\\n",
        "        * (reward + gamma * np.nanmax(q_table[row_index_next,: ]) \\\n",
        "           - q_table[row_index, column_index])\n",
        "    return q_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpsMgCvQUSQB"
      },
      "source": [
        "def find_q_row(play_area):\n",
        "    \"\"\"参照時の状況(state)が参照すべき行番号を計算する関数\n",
        "    ゲームの状況をあらわすリストを受け取り、行番号を返す\n",
        "    \"\"\"\n",
        "    row_index = 0\n",
        "    for index in range(len(play_area)):\n",
        "        if play_area[index] == '○':\n",
        "            coef = 1\n",
        "        elif play_area[index] == '×':\n",
        "            coef = 2\n",
        "        else:\n",
        "            coef = 0\n",
        "        row_index += (3 ** index) * coef\n",
        "    return row_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En4utY73USNq"
      },
      "source": [
        "def get_ql_action(play_area, choosable_area, q_table, epsilon):\n",
        "    \"\"\"AIの行動を決定する関数\n",
        "    ゲームの状況をあらわすリスト・選択可能エリア・Qテーブル・イプシロンを受け取り、行動を返す\n",
        "    \"\"\"\n",
        "    # esilonの確率でランダムな選択をする\n",
        "    if np.random.rand() < epsilon:\n",
        "        ai_input = int(random.choice(choosable_area))\n",
        "    # Qテーブルに従い行動を選択する\n",
        "    else:\n",
        "        row_index = find_q_row(play_area)\n",
        "        first_choice_flg = 1\n",
        "        for choice in choosable_area:\n",
        "            if first_choice_flg == 1:\n",
        "                ai_input = int(choice)\n",
        "                first_choice_flg = 0\n",
        "            else:\n",
        "                if q_table[row_index, ai_input-1] \\\n",
        "                < q_table[row_index, int(choice)-1]:\n",
        "                    ai_input = int(choice)\n",
        "    return ai_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Z8l-B1UYk6"
      },
      "source": [
        "def randomAI_vs_QLAI(first_inputter, q_table, epsilon=0):\n",
        "    \"\"\"AI(ランダム)とAI(Q学習)のゲームを実行する関数\n",
        "    先手(1:AI(ランダム)、2:AI(Q学習))とQテーブルを受け取り、ゲームが終了するまで実行する\n",
        "    \"\"\"\n",
        "    inputter1 = 'Random AI'\n",
        "    inputter2 = 'QL AI'\n",
        "\n",
        "    # Q学習退避用\n",
        "    ql_input_list = []\n",
        "    play_area_list = []\n",
        "\n",
        "    play_area = list(range(1, 10))\n",
        "    #show_play(play_area)\n",
        "    inputter_count = first_inputter\n",
        "    end_flg = 0\n",
        "    ql_flg = 0\n",
        "    reward = 0\n",
        "    while True:\n",
        "        # Q学習退避用\n",
        "        play_area_tmp = play_area.copy()\n",
        "        play_area_list.append(play_area_tmp)\n",
        "        # Q学習実行フラグ\n",
        "        ql_flg = 0\n",
        "        # AI(Q学習)の手番\n",
        "        if (inputter_count % 2) == 0:\n",
        "            # QL AI入力\n",
        "            play_area, ql_ai_input = get_ai_input(play_area, first_inputter, mode=1, q_table=q_table, epsilon=epsilon)\n",
        "            winner, end_flg = judge(play_area, inputter2)\n",
        "            # Q学習退避用\n",
        "            ql_input_list.append(ql_ai_input)            \n",
        "            # 勝利した場合\n",
        "            if winner == inputter2:\n",
        "                reward = 1\n",
        "                ql_flg = 1\n",
        "            play_area_before = play_area_list[-1]\n",
        "            ql_ai_input_before = ql_input_list[-1]\n",
        "        # AI(ランダム)の手番\n",
        "        elif (inputter_count % 2) == 1:\n",
        "            play_area, random_ai_input = get_ai_input(play_area, first_inputter+1, mode=0)\n",
        "            winner, end_flg = judge(play_area, inputter1)\n",
        "            # AI(ランダム)が先手の場合の初手以外は学習\n",
        "            if inputter_count != 1:\n",
        "                ql_flg = 1\n",
        "        # Q学習実行\n",
        "        if ql_flg == 1:\n",
        "            ql_ai_input_before = ql_input_list[-1]\n",
        "            q_table = q_learning(play_area_before, ql_ai_input_before, reward, play_area, q_table, end_flg)\n",
        "        if end_flg:\n",
        "            break\n",
        "        inputter_count += 1\n",
        "    ## print('{} win!!!'.format(winner))\n",
        "    return winner, q_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uze4Z5foUYhs"
      },
      "source": [
        "q_table = make_q_table()\n",
        "eta = 0.1  # 学習率\n",
        "gamma = 0.9  # 時間割引率\n",
        "initial_epsilon = 0.5  # ε-greedy法の初期値"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kq1EwOLUYeS"
      },
      "source": [
        "# ランダム vs QL(学習)\n",
        "# 試行数設定\n",
        "episode = int(5e5)  ### この値で２分くらいかかる\n",
        "winner_list = []\n",
        "start = time.time()\n",
        "for i in range(episode):\n",
        "    epsilon = initial_epsilon * (episode-i) / episode\n",
        "    winner, _ = randomAI_vs_QLAI(1, q_table, epsilon)\n",
        "    winner_list.append(winner)\n",
        "elapsed_time = time.time() - start\n",
        "print ('elapsed_time:{0}'.format(elapsed_time) + '[sec]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfN0F5lyT-J9"
      },
      "source": [
        "print('勝ち回数')\n",
        "print('Random AI:{}'.format(winner_list.count('Random AI')))\n",
        "print('QL AI    :{}'.format(winner_list.count('QL AI')))\n",
        "print('NOBODY   :{}'.format(winner_list.count('NOBODY')))\n",
        "print('QLの勝率 :{}'.format(winner_list.count('QL AI') / len(winner_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HiaUtotUfvs"
      },
      "source": [
        "def player_vs_QLAI(first_inputter, q_table, epsilon=0):\n",
        "    \"\"\"プレイヤーとAI(Q学習)のゲームを実行する関数\n",
        "    先手(1:プレイヤー)、2:AI(Q学習))を受け取り、ゲームが終了するまで実行する\n",
        "    \"\"\"\n",
        "    inputter1 = 'YOU'\n",
        "    inputter2 = 'QL AI'\n",
        "\n",
        "    # Q学習退避用\n",
        "    ql_input_list = []\n",
        "    play_area_list = []\n",
        "\n",
        "    play_area = list(range(1, 10))\n",
        "    show_play(play_area)\n",
        "    inputter_count = first_inputter\n",
        "    end_flg = 0\n",
        "    ql_flg = 0\n",
        "    reward = 0\n",
        "    while True:\n",
        "        # Q学習退避用\n",
        "        play_area_tmp = play_area.copy()\n",
        "        play_area_list.append(play_area_tmp)\n",
        "        # Q学習実行フラグ\n",
        "        ql_flg = 0\n",
        "        # AI(Q学習)の手番\n",
        "        if (inputter_count % 2) == 0:\n",
        "            # QL AI入力\n",
        "            play_area, ql_ai_input = get_ai_input(play_area, first_inputter, mode=1, q_table=q_table, epsilon=epsilon)\n",
        "            show_play(play_area, inputter2, ql_ai_input)\n",
        "            winner, end_flg = judge(play_area, inputter2)\n",
        "            # Q学習退避用\n",
        "            ql_input_list.append(ql_ai_input)            \n",
        "            # 勝利した場合\n",
        "            if winner == inputter2:\n",
        "                reward = 1\n",
        "                ql_flg = 1\n",
        "            play_area_before = play_area_list[-1]\n",
        "            ql_ai_input_before = ql_input_list[-1]\n",
        "        # プレイヤーの手番\n",
        "        elif (inputter_count % 2) == 1:\n",
        "            print('Your turn!')\n",
        "            # プレイヤーの入力受付\n",
        "            play_area, player_input = get_player_input(play_area, first_inputter)\n",
        "            show_play(play_area, inputter1, player_input)\n",
        "            winner, end_flg = judge(play_area, inputter1)\n",
        "            # プレイヤーが勝利した場合\n",
        "            if winner == inputter1:\n",
        "                reward = -1\n",
        "            # プレイヤーが先手の場合の初手以外は学習\n",
        "            if inputter_count != 1:\n",
        "                ql_flg = 1\n",
        "        # Q学習実行\n",
        "        if ql_flg == 1:\n",
        "#            print('Q学習')\n",
        "            ql_ai_input_before = ql_input_list[-1]\n",
        "            q_table = q_learning(play_area_before, ql_ai_input_before, reward, play_area, q_table, end_flg)\n",
        "        if end_flg:\n",
        "            break\n",
        "        inputter_count += 1\n",
        "    show_play(play_area)\n",
        "    print('{} win!!!'.format(winner))\n",
        "    sleep(1)\n",
        "    return winner, q_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LhpwU-8Ufr1"
      },
      "source": [
        "# プレイヤー vs QL\n",
        "# 試行数設定\n",
        "episode = 10\n",
        "winner_list = []\n",
        "for i in range(episode):\n",
        "    epsilon = initial_epsilon * (episode-i) / episode\n",
        "    winner, q_table = player_vs_QLAI(2, q_table, epsilon=epsilon)\n",
        "    winner_list.append(winner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYAGLBjhqmEh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-erM88KQ8Zo4"
      },
      "source": [
        "#### <font color=green>**1.3.** </font> A Simple Python Example and a Step Closer to AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnnzJjbcUZ4B"
      },
      "source": [
        "# 出典 : https://amunategui.github.io/reinforcement-learning/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI7wnZ-3UZ1y"
      },
      "source": [
        "## A VERY Simple Python Q-learning Example\n",
        "\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "\n",
        "# map cell to cell, add circular cell to goal point\n",
        "points_list = [(0,1), (1,5), (5,6), (5,4), (1,2), (2,3), (2,7)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9fFToD0UZxO"
      },
      "source": [
        "goal = 7\n",
        "\n",
        "import networkx as nx\n",
        "G=nx.Graph()\n",
        "G.add_edges_from(points_list)\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw_networkx_nodes(G,pos)\n",
        "nx.draw_networkx_edges(G,pos)\n",
        "nx.draw_networkx_labels(G,pos)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo0tbexCUZsv"
      },
      "source": [
        "# how many points in graph? x points\n",
        "MATRIX_SIZE = 8\n",
        "\n",
        "# create matrix x*y\n",
        "R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))\n",
        "R *= -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rr_r112UWgu"
      },
      "source": [
        "# assign zeros to paths and 100 to goal-reaching point\n",
        "for point in points_list:\n",
        "  print(point)\n",
        "  if point[1] == goal:\n",
        "    R[point] = 100\n",
        "  else:\n",
        "    R[point] = 0\n",
        "\n",
        "  if point[0] == goal:\n",
        "    R[point[::-1]] = 100\n",
        "  else:\n",
        "    # reverse of point\n",
        "    R[point[::-1]]= 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ4b79AoVaPx"
      },
      "source": [
        "# add goal point round trip\n",
        "R[goal,goal]= 100\n",
        "R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRBuNfzIUq6G"
      },
      "source": [
        "Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n",
        "\n",
        "# learning parameter\n",
        "gamma = 0.8\n",
        "\n",
        "initial_state = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc6_m-JkUq4A"
      },
      "source": [
        "def available_actions(state):\n",
        "  current_state_row = R[state,]\n",
        "  av_act = np.where(current_state_row >= 0)[1]\n",
        "  return av_act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPC8-nI4VkHf"
      },
      "source": [
        "def sample_next_action(available_actions_range):\n",
        "  next_action = int(np.random.choice(available_act,1))\n",
        "  return next_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF3xRuojVkFb"
      },
      "source": [
        "def update(current_state, action, gamma):\n",
        "  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
        "  if max_index.shape[0] > 1:\n",
        "    max_index = int(np.random.choice(max_index, size = 1))\n",
        "  else:\n",
        "    max_index = int(max_index)\n",
        "  max_value = Q[action, max_index]\n",
        "  \n",
        "  Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
        "  #print('max_value', R[current_state, action] + gamma * max_value)\n",
        "  \n",
        "  if (np.max(Q) > 0):\n",
        "    return (np.sum(Q/np.max(Q)*100))\n",
        "  else:\n",
        "    return (0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCzNcQ4gUqyW"
      },
      "source": [
        "available_act = available_actions(initial_state) \n",
        "\n",
        "action = sample_next_action(available_act)\n",
        "    \n",
        "update(initial_state, action, gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_lOJ5GdUqu6"
      },
      "source": [
        "# Training\n",
        "scores = []\n",
        "for i in range(700):\n",
        "  current_state = np.random.randint(0, int(Q.shape[0]))\n",
        "  available_act = available_actions(current_state)\n",
        "  action = sample_next_action(available_act)\n",
        "  score = update(current_state,action,gamma)\n",
        "  scores.append(score)\n",
        "  #print('Score:', str(score))\n",
        "\n",
        "print(\"Trained Q matrix:\")\n",
        "print(Q/np.max(Q)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAFTRALvUqtE"
      },
      "source": [
        "# Testing\n",
        "current_state = 0\n",
        "steps = [current_state]\n",
        "\n",
        "while current_state != 7:\n",
        "  next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
        "  if next_step_index.shape[0] > 1:\n",
        "    next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
        "  else:\n",
        "    next_step_index = int(next_step_index)\n",
        "  steps.append(next_step_index)\n",
        "  current_state = next_step_index\n",
        "\n",
        "\n",
        "print(\"Most efficient path:\")\n",
        "print(steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2HHygMUUqrX"
      },
      "source": [
        "plt.plot(scores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQLvlEUoUqo3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh2FdAWCWX8B"
      },
      "source": [
        "## Version 2.0, with Environmental Details\n",
        "\n",
        "bees = [2]\n",
        "smoke = [4,5,6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnUbileaWX5G"
      },
      "source": [
        "G = nx.Graph()\n",
        "G.add_edges_from(points_list)\n",
        "\n",
        "mapping = {\n",
        "    0:'Start', \n",
        "    1:'1', \n",
        "    2:'2 - Bees', \n",
        "    3:'3', \n",
        "    4:'4 - Smoke', \n",
        "    5:'5 - Smoke', \n",
        "    6:'6 - Smoke', \n",
        "    7:'7 - Beehive'\n",
        "    }\n",
        "\n",
        "H = nx.relabel_nodes(G, mapping) \n",
        "pos = nx.spring_layout(H)\n",
        "nx.draw_networkx_nodes(H, pos, \n",
        "                       node_size = [200,200,200,200,\n",
        "                                    200,200,200,200]\n",
        "                       )\n",
        "\n",
        "nx.draw_networkx_edges(H, pos)\n",
        "nx.draw_networkx_labels(H, pos)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjEkzHfyWX20"
      },
      "source": [
        "# re-initialize the matrices for new run\n",
        "Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n",
        "\n",
        "enviro_bees = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n",
        "enviro_smoke = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))\n",
        " \n",
        "initial_state = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by3hIoCfWXzU"
      },
      "source": [
        "def available_actions(state):\n",
        "  current_state_row = R[state,]\n",
        "  av_act = np.where(current_state_row >= 0)[1]\n",
        "  return av_act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wAcJUjmWXxO"
      },
      "source": [
        "def sample_next_action(available_actions_range):\n",
        "  next_action = int(np.random.choice(available_act,1))\n",
        "  return next_action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0y1TCUdWXvM"
      },
      "source": [
        "def collect_environmental_data(action):\n",
        "  found = []\n",
        "  if action in bees:\n",
        "    found.append('b')\n",
        "  if action in smoke:\n",
        "    found.append('s')\n",
        "  return (found)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJg9chEqXZvU"
      },
      "source": [
        "def update(current_state, action, gamma):\n",
        "  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
        "  if max_index.shape[0] > 1:\n",
        "    max_index = int(np.random.choice(max_index, size = 1))\n",
        "  else:\n",
        "    max_index = int(max_index)\n",
        "  max_value = Q[action, max_index]\n",
        "  \n",
        "  Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
        "  #print('max_value', R[current_state, action] + gamma * max_value)\n",
        "  \n",
        "  environment = collect_environmental_data(action)\n",
        "  if 'b' in environment:\n",
        "    enviro_bees[current_state, action] += 1\n",
        "  if 's' in environment:\n",
        "    enviro_smoke[current_state, action] += 1\n",
        "\n",
        "  if (np.max(Q) > 0):\n",
        "    return (np.sum(Q/np.max(Q)*100))\n",
        "  else:\n",
        "    return (0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nZBE3nVWbjj"
      },
      "source": [
        "available_act = available_actions(initial_state) \n",
        "action = sample_next_action(available_act)\n",
        "update(initial_state, action, gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laW6LQkeWbhO"
      },
      "source": [
        "scores = []\n",
        "for i in range(700):\n",
        "  current_state = np.random.randint(0, int(Q.shape[0]))\n",
        "  available_act = available_actions(current_state)\n",
        "  action = sample_next_action(available_act)\n",
        "  score = update(current_state,action,gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fp4-7x8WbbB"
      },
      "source": [
        "# print environmental matrices\n",
        "print('Bees Found')\n",
        "print(enviro_bees)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMqwoiWHXz0e"
      },
      "source": [
        "print('Smoke Found')\n",
        "print(enviro_smoke)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSx4jHaTYL5m"
      },
      "source": [
        "Q = np.matrix(np.zeros([MATRIX_SIZE, MATRIX_SIZE]))\n",
        "\n",
        "# subtract bees with smoke, this gives smoke a negative effect\n",
        "enviro_matrix = enviro_bees - enviro_smoke\n",
        "\n",
        "# Get available actions in the current state\n",
        "available_act = available_actions(initial_state) \n",
        "\n",
        "# Sample next action to be performed\n",
        "action = sample_next_action(available_act)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sZ_qP6YL33"
      },
      "source": [
        "# This function updates the Q matrix according to the path selected and the Q \n",
        "# learning algorithm\n",
        "def update(current_state, action, gamma):\n",
        "  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
        "  if max_index.shape[0] > 1:\n",
        "    max_index = int(np.random.choice(max_index, size = 1))\n",
        "  else:\n",
        "    max_index = int(max_index)\n",
        "  max_value = Q[action, max_index]\n",
        "\n",
        "  Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
        "  #print('max_value', R[current_state, action] + gamma * max_value)\n",
        "\n",
        "  environment = collect_environmental_data(action)\n",
        "  if 'b' in environment:\n",
        "    enviro_matrix[current_state, action] += 1\n",
        "  if 's' in environment:\n",
        "    enviro_matrix[current_state, action] -= 1\n",
        "\n",
        "  return (np.sum(Q/np.max(Q)*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjpv1O6uYL1-"
      },
      "source": [
        "def available_actions_with_enviro_help(state):\n",
        "  current_state_row = R[state,]\n",
        "  av_act = np.where(current_state_row >= 0)[1]\n",
        "  # if there are multiple routes, dis-favor anything negative\n",
        "  env_pos_row = enviro_matrix_snap[state, av_act]\n",
        "  if (np.sum(env_pos_row < 0)):\n",
        "    # can we remove the negative directions from av_act?\n",
        "    temp_av_act = av_act[np.array(env_pos_row)[0]>=0]\n",
        "    if len(temp_av_act) > 0:\n",
        "      #print('going from : {}'.format(av_act))\n",
        "      #print('to : {}'.format(temp_av_act))\n",
        "      av_act = temp_av_act\n",
        "  return av_act"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0lqOKseYLyN"
      },
      "source": [
        "update(initial_state,action,gamma)\n",
        "enviro_matrix_snap = enviro_matrix.copy()\n",
        "\n",
        "# Training\n",
        "scores = []\n",
        "for i in range(700):\n",
        "  current_state = np.random.randint(0, int(Q.shape[0]))\n",
        "  available_act = available_actions_with_enviro_help(current_state)\n",
        "  action = sample_next_action(available_act)\n",
        "  score = update(current_state,action,gamma)\n",
        "  scores.append(score)\n",
        "  #print('Score:', str(score))\n",
        "\n",
        "plt.plot(scores)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW9GR-oSYLvi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVOcDp-XCy5v"
      },
      "source": [
        "### <font color=blue>**2.** </font> SARSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSxXv-Cxrs6g"
      },
      "source": [
        "#### <font color=green>**2.1.** </font> 迷路を解く　その２"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5aTWiCe_pS"
      },
      "source": [
        "## 出典 : https://book.mynavi.jp/manatee/detail/id=88534"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyk27HYafSn-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsfNAKGOfSlw"
      },
      "source": [
        "# 初期位置での迷路の様子\n",
        " \n",
        "# 図を描く大きさと、図の変数名を宣言\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        " \n",
        "# 赤い壁を描く\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        " \n",
        "# 状態を示す文字S0～S8を描く\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        " \n",
        "# 描画範囲の設定と目盛りを消す設定\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
        "                labelbottom='off', right='off', left='off', labelleft='off')\n",
        " \n",
        "# 現在値S0に緑丸を描画する\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LztfRoRfSju"
      },
      "source": [
        "# 初期の方策を決定するパラメータtheta_0を設定\n",
        " \n",
        "# 行は状態0～7、列は移動方向で↑、→、↓、←を表す\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7、※s8はゴールなので、方策はなし\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgRk3aimfShL"
      },
      "source": [
        "# 初期の行動価値関数Qを設定\n",
        " \n",
        "[a, b] = theta_0.shape  # 行と列の数をa, bに格納\n",
        "Q = np.random.rand(a, b) * theta_0\n",
        "# *theta0をすることで要素ごとに掛け算をし、壁方向がnanになる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCVlWEEAfSdE"
      },
      "source": [
        "# 方策パラメータtheta_0をランダム方策piに変換する関数の定義\n",
        " \n",
        "def simple_convert_into_pi_from_theta(theta):\n",
        "  '''単純に割合を計算する'''\n",
        "  [m, n] = theta.shape  # thetaの行列サイズを取得\n",
        "  pi = np.zeros((m, n))\n",
        "  for i in range(0, m):\n",
        "    pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # 割合の計算\n",
        "  pi = np.nan_to_num(pi)  # nanを0に変換 \n",
        "  return pi\n",
        " \n",
        "# ランダム行動方策pi_0を求める\n",
        "pi_0 = simple_convert_into_pi_from_theta(theta_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy44vIb9fSZh"
      },
      "source": [
        "# ε-greedy法を実装\n",
        " \n",
        "def get_action_and_s_next(s, Q, epsilon, pi_0):\n",
        "  direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        " \n",
        "  # 行動を決める\n",
        "  if np.random.rand() < epsilon:\n",
        "    # εの確率でランダムに動く\n",
        "    next_direction = np.random.choice(direction, p=pi_0[s, :])\n",
        "  else:\n",
        "    # Qの最大値の行動を採用する\n",
        "    next_direction = direction[np.nanargmax(Q[s, :])]\n",
        " \n",
        "  # 決めた行動で次の状態を決める\n",
        "  if next_direction == \"up\":\n",
        "    action = 0\n",
        "    s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n",
        "  elif next_direction == \"right\":\n",
        "    action = 1\n",
        "    s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
        "  elif next_direction == \"down\":\n",
        "    action = 2\n",
        "    s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n",
        "  elif next_direction == \"left\":\n",
        "    action = 3\n",
        "    s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
        " \n",
        "  return [action, s_next]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMpdqpbe_l4"
      },
      "source": [
        "# Sarsaによる行動価値関数Qの更新\n",
        " \n",
        "def Sarsa(s, a, r, s_next, a_next, Q, eta, gamma):\n",
        "  if s_next == 8:  # ゴールした場合\n",
        "    Q[s, a] = Q[s, a] + eta * (r - Q[s, a])\n",
        "  else:\n",
        "    Q[s, a] = Q[s, a] + eta * (r + gamma * Q[s_next, a_next] - Q[s, a])\n",
        " \n",
        "  return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3NJL0oqe_jr"
      },
      "source": [
        "# Sarsaで迷路を解く関数の定義、状態と行動の履歴および更新したQを出力\n",
        " \n",
        "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0):\n",
        "  s = 0  # スタート地点\n",
        "  s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト \n",
        "  while (1):  # ゴールするまでループ\n",
        "    [a, s_next] = get_action_and_s_next(s, Q, epsilon, pi_0)\n",
        "    s_a_history[-1][1] = a\n",
        "    # 現在の状態（つまり一番最後なのでindex=-1）に行動を代入\n",
        " \n",
        "    s_a_history.append([s_next, np.nan])\n",
        "    # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        " \n",
        "    # 報酬を与え,　次の行動を求めます\n",
        "    if s_next == 8:\n",
        "      r = 1  # ゴールにたどり着いたなら報酬を与える\n",
        "      a_next = np.nan\n",
        "    else:\n",
        "      r = 0\n",
        "      [a_next, _] = get_action_and_s_next(s_next, Q, epsilon, pi_0)\n",
        "      # 実際行動しないですが、次の行動a_nextを求めます。\n",
        "      # 返り値の_は、その変数は無視するという意味です\n",
        " \n",
        "    # 価値関数を更新\n",
        "    Q = Sarsa(s, a, r, s_next, a_next, Q, eta, gamma)\n",
        " \n",
        "    # 終了判定\n",
        "    if s_next == 8:  # ゴール地点なら終了\n",
        "      break\n",
        "    else:\n",
        "      s = s_next\n",
        " \n",
        "  return [s_a_history, Q]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1rSxvh2gOr4"
      },
      "source": [
        "# Sarsaで迷路を解く\n",
        "eta = 0.1  # 学習率\n",
        "gamma = 0.9  # 時間割引率\n",
        "epsilon = 0.5  # ε-greedy法の初期値\n",
        "v = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\n",
        "is_continue = True\n",
        "episode = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJDRsvB_e_hI"
      },
      "source": [
        "while is_continue:  # is_continueがFalseになるまで繰り返す\n",
        "  print(\"エピソード:\" + str(episode))\n",
        "  # ε-greedyの値を少しずつ小さくする\n",
        "  epsilon = epsilon / 2\n",
        " \n",
        "  # Sarsaで迷路を解き、移動した履歴と更新したQを求める\n",
        "  [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
        " \n",
        "  # 状態価値の変化\n",
        "  new_v = np.nanmax(Q, axis=1)  # 状態ごとに価値の最大値を求める\n",
        "  print(np.sum(np.abs(new_v - v)))  # 状態価値の変化を出力\n",
        "  v = new_v\n",
        " \n",
        "  print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
        " \n",
        "  # 10エピソード繰り返す  # 100 -> 10\n",
        "  episode = episode + 1\n",
        "  if episode > 10:  # 100 -> 10\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nGRQdZsgqZI"
      },
      "source": [
        "# エージェントの移動の様子を可視化\n",
        "# 参考URL http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWKgXnswgse1"
      },
      "source": [
        "def init():\n",
        "  # 背景画像の初期化\n",
        "  line.set_data([], [])\n",
        "  return (line,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWydw2xWgsW4"
      },
      "source": [
        "def animate(i):\n",
        "  # フレームごとの描画内容\n",
        "  state = s_a_history[i][0]  # 現在の場所を描く\n",
        "  x = (state % 3) + 0.5  # 状態のx座標は、3で割った余り+0.5\n",
        "  y = 2.5 - int(state / 3)  # y座標は3で割った商を2.5から引く\n",
        "  line.set_data(x, y)\n",
        "  return (line,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS79XDFKe_cV"
      },
      "source": [
        "#　初期化関数とフレームごとの描画関数を用いて動画を作成\n",
        "anim = animation.FuncAnimation(\n",
        "    fig, \n",
        "    animate, \n",
        "    init_func=init, \n",
        "    frames=len(s_a_history), \n",
        "    interval=200, \n",
        "    repeat=False\n",
        "    )\n",
        " \n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsSJTFKNtd7L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK64THRg8qdO"
      },
      "source": [
        "#### <font color=green>**2.2.** </font> Gambler’s Problem from Sutton's book."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TptLq_lMWbY3"
      },
      "source": [
        "'''A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. \n",
        "If the coin comes up heads, he wins as many dollars as he has staked on that flip; \n",
        "if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of $100, or loses by running out of money. \n",
        "\n",
        "On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. \n",
        "This problem can be formulated as an undiscounted, episodic, finite MDP. \n",
        "\n",
        "The state is the gambler’s capital, s ∈ {1, 2, . . . , 99}.\n",
        "The actions are stakes, a ∈ {0, 1, . . . , min(s, 100 − s)}. \n",
        "The reward is zero on all transitions except those on which the gambler reaches his goal, when it is +1.\n",
        "\n",
        "The state-value function then gives the probability of winning from each state. \n",
        "A policy is a mapping from levels of capital to stakes. \n",
        "The optimal policy maximizes the probability of reaching the goal. Let p_h denote the probability of the coin coming up heads. \n",
        "If p_h is known, then the entire problem is known and it can be solved, for instance, by value iteration.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DgEHSgg2W6Mm"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "if \"../\" not in sys.path:\n",
        "  sys.path.append(\"../\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGiotW2AMvB"
      },
      "source": [
        "# Exercise 4.9 (programming)\n",
        "# Implement value iteration for the gambler’s problem and solve it for p_h = 0.25 and p_h = 0.55."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GT3aBsh5W6Mo"
      },
      "source": [
        "def value_iteration_for_gamblers(p_h, theta=0.0001, discount_factor=1.0):\n",
        "    \"\"\"Args:\n",
        "    p_h: Probability of the coin coming up heads\n",
        "    \"\"\"\n",
        "    # The reward is zero on all transitions except those on which the gambler reaches his goal,\n",
        "    # when it is +1.\n",
        "    rewards = np.zeros(101)\n",
        "    rewards[100] = 1 \n",
        "    \n",
        "    # We introduce two dummy states corresponding to termination with capital of 0 and 100\n",
        "    V = np.zeros(101)\n",
        "    \n",
        "    def one_step_lookahead(s, V, rewards):\n",
        "        \"\"\"Helper function to calculate the value for all action in a given state.\n",
        "        Args:\n",
        "            s: The gambler’s capital. Integer.\n",
        "            V: The vector that contains values at each state. \n",
        "            rewards: The reward vector.\n",
        "                        \n",
        "        Returns:\n",
        "            A vector containing the expected value of each action. \n",
        "            Its length equals to the number of actions.\n",
        "        \"\"\"\n",
        "        A = np.zeros(101)\n",
        "        stakes = range(1, min(s, 100-s)+1) # Your minimum bet is 1, maximum bet is min(s, 100-s).\n",
        "        for a in stakes:\n",
        "            # rewards[s+a], rewards[s-a] are immediate rewards.\n",
        "            # V[s+a], V[s-a] are values of the next states.\n",
        "            # This is the core of the Bellman equation: The expected value of your action is \n",
        "            # the sum of immediate rewards and the value of the next state.\n",
        "            A[a] = p_h * (rewards[s+a] + V[s+a]*discount_factor) + (1-p_h) * (rewards[s-a] + V[s-a]*discount_factor)\n",
        "        return A\n",
        "    \n",
        "    while True:\n",
        "        # Stopping condition\n",
        "        delta = 0\n",
        "        # Update each state...\n",
        "        for s in range(1, 100):\n",
        "            # Do a one-step lookahead to find the best action\n",
        "            A = one_step_lookahead(s, V, rewards)\n",
        "            # print(s,A,V) # if you want to debug.\n",
        "            best_action_value = np.max(A)\n",
        "            # Calculate delta across all states seen so far\n",
        "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
        "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
        "            V[s] = best_action_value        \n",
        "        # Check if we can stop \n",
        "        if delta < theta:\n",
        "            break\n",
        "    \n",
        "    # Create a deterministic policy using the optimal value function\n",
        "    policy = np.zeros(100)\n",
        "    for s in range(1, 100):\n",
        "        # One step lookahead to find the best action for this state\n",
        "        A = one_step_lookahead(s, V, rewards)\n",
        "        best_action = np.argmax(A)\n",
        "        # Always take the best action\n",
        "        policy[s] = best_action\n",
        "    \n",
        "    return policy, V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUMO4xqFW6Mp"
      },
      "source": [
        "policy, v = value_iteration_for_gamblers(0.25)\n",
        "\n",
        "print(\"Optimized Policy:\")\n",
        "print(policy)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Optimized Value Function:\")\n",
        "print(v)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfSwos63W6Ms"
      },
      "source": [
        "### Show your results graphically\n",
        "# Plotting Final Policy (action stake) vs State (Capital)\n",
        "\n",
        "# x axis values\n",
        "x = range(100)\n",
        "# corresponding y axis values\n",
        "y = v[:100]\n",
        " \n",
        "# plotting the points \n",
        "plt.plot(x, y)\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('Capital')\n",
        "# naming the y axis\n",
        "plt.ylabel('Value Estimates')\n",
        " \n",
        "# giving a title to the graph\n",
        "plt.title('Final Policy (action stake) vs State (Capital)')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wu3qz29W6Mt"
      },
      "source": [
        "# Plotting Capital vs Final Policy\n",
        "\n",
        "# x axis values\n",
        "x = range(100)\n",
        "# corresponding y axis values\n",
        "y = policy\n",
        " \n",
        "# plotting the bars\n",
        "plt.figure(figsize=(16,8))  ##\n",
        "plt.bar(x, y, align='center', alpha=0.5)\n",
        "plt.grid(axis='y')  ###\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('Capital')\n",
        "# naming the y axis\n",
        "plt.ylabel('Final policy (stake)')\n",
        " \n",
        "# giving a title to the graph\n",
        "plt.title('Capital vs Final Policy')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuU8KeuBAoHY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZKAN0zwAoWw"
      },
      "source": [
        "policy_2, v_2 = value_iteration_for_gamblers(0.55)\n",
        "\n",
        "print(\"Optimized Policy:\")\n",
        "print(policy_2)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Optimized Value Function:\")\n",
        "print(v_2)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iKf9CjeAoWz"
      },
      "source": [
        "### Show your results graphically\n",
        "# Plotting Final Policy (action stake) vs State (Capital)\n",
        "\n",
        "# x axis values\n",
        "x = range(100)\n",
        "# corresponding y axis values\n",
        "y = v_2[:100]\n",
        " \n",
        "# plotting the points \n",
        "plt.plot(x, y)\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('Capital')\n",
        "# naming the y axis\n",
        "plt.ylabel('Value Estimates')\n",
        " \n",
        "# giving a title to the graph\n",
        "plt.title('Final Policy (action stake) vs State (Capital)')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOu0gJtEAoWz"
      },
      "source": [
        "# Plotting Capital vs Final Policy\n",
        "\n",
        "# x axis values\n",
        "x = range(100)\n",
        "# corresponding y axis values\n",
        "y = policy_2\n",
        " \n",
        "# plotting the bars\n",
        "plt.figure(figsize=(16,8))  ##\n",
        "plt.bar(x, y, align='center', alpha=0.5)\n",
        "plt.grid(axis='y')  ###\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('Capital')\n",
        "# naming the y axis\n",
        "plt.ylabel('Final policy (stake)')\n",
        " \n",
        "# giving a title to the graph\n",
        "plt.title('Capital vs Final Policy')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7744op0AyiA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BAPIlwIAzMk"
      },
      "source": [
        "policy_3, v_3 = value_iteration_for_gamblers(0.75)\n",
        "\n",
        "print(\"Optimized Policy:\")\n",
        "print(policy_3)\n",
        "print(\"\")\n",
        "\n",
        "print(\"Optimized Value Function:\")\n",
        "print(v_3)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8FyMkKvAzMk"
      },
      "source": [
        "### Show your results graphically\n",
        "# Plotting Final Policy (action stake) vs State (Capital)\n",
        "\n",
        "# x axis values\n",
        "x = range(100)\n",
        "# corresponding y axis values\n",
        "y = v_3[:100]\n",
        " \n",
        "# plotting the points \n",
        "plt.plot(x, y)\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('Capital')\n",
        "# naming the y axis\n",
        "plt.ylabel('Value Estimates')\n",
        " \n",
        "# giving a title to the graph\n",
        "plt.title('Final Policy (action stake) vs State (Capital)')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgDU4NViAzMl"
      },
      "source": [
        "# Plotting Capital vs Final Policy\n",
        "\n",
        "# x axis values\n",
        "x = range(100)\n",
        "# corresponding y axis values\n",
        "y = policy_3\n",
        " \n",
        "# plotting the bars\n",
        "plt.figure(figsize=(16,8))  ##\n",
        "plt.bar(x, y, align='center', alpha=0.5)\n",
        "plt.grid(axis='y')  ###\n",
        " \n",
        "# naming the x axis\n",
        "plt.xlabel('Capital')\n",
        "# naming the y axis\n",
        "plt.ylabel('Final policy (stake)')\n",
        " \n",
        "# giving a title to the graph\n",
        "plt.title('Capital vs Final Policy')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2EvpOXBBBfZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUHcR21urs_7"
      },
      "source": [
        "#### <font color=green>**2.3.** </font> CartPole NumPyで実装 その１"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DSqEDJL6yhS"
      },
      "source": [
        "## 出典 : https://deepage.net/machine_learning/2017/08/10/reinforcement-learning.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BPDaoh9625f"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "goal_average_steps = 195\n",
        "max_number_of_steps = 200\n",
        "num_consecutive_iterations = 100\n",
        "num_episodes = 5000\n",
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "\n",
        "    episode_reward = 0\n",
        "    for t in range(max_number_of_steps):\n",
        "        # CartPoleの描画\n",
        "        #env.render()\n",
        "\n",
        "        # ランダムで行動の選択\n",
        "        action = np.random.choice([0, 1])\n",
        "\n",
        "        # 行動の実行とフィードバックの取得\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print('%d Episode finished after %d time steps / mean %f' % (episode, t + 1,\n",
        "                last_time_steps.mean()))\n",
        "            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))\n",
        "            break\n",
        "\n",
        "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードが195以上であれば成功\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTLxRo-E622a"
      },
      "source": [
        "q_table = np.random.uniform(low=-1, high=1, size=(4 ** 4, env.action_space.n))\n",
        "\n",
        "def bins(clip_min, clip_max, num):\n",
        "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
        "\n",
        "def digitize_state(observation):\n",
        "    # 各値を4個の離散値に変換\n",
        "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "    digitized = [np.digitize(cart_pos, bins=bins(-2.4, 2.4, 4)),\n",
        "                 np.digitize(cart_v, bins=bins(-3.0, 3.0, 4)),\n",
        "                 np.digitize(pole_angle, bins=bins(-0.5, 0.5, 4)),\n",
        "                 np.digitize(pole_v, bins=bins(-2.0, 2.0, 4))]\n",
        "    # 0~255に変換\n",
        "    return sum([x * (4 ** i) for i, x in enumerate(digitized)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yav2igyg62x9"
      },
      "source": [
        "def get_action(state, action, observation, reward):\n",
        "    next_state = digitize_state(observation)\n",
        "    next_action = np.argmax(q_table[next_state])\n",
        "\n",
        "    # Qテーブルの更新\n",
        "    alpha = 0.2\n",
        "    gamma = 0.99\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
        "            alpha * (reward + gamma * q_table[next_state, next_action])\n",
        "\n",
        "    return next_action, next_state\n",
        "\n",
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "for episode in range(num_episodes):\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "\n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "\n",
        "    episode_reward = 0\n",
        "    for t in range(max_number_of_steps):\n",
        "        # CartPoleの描画\n",
        "        #env.render()\n",
        "\n",
        "        # 行動の実行とフィードバックの取得\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # 行動の選択\n",
        "        action, state = get_action(state, action, observation, reward)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print('%d Episode finished after %d time steps / mean %f' % (episode, t + 1,\n",
        "                last_time_steps.mean()))\n",
        "            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))\n",
        "            break\n",
        "\n",
        "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードが195以上であれば成功\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04PxPTCM6yee"
      },
      "source": [
        "def get_action_2(state, action, observation, reward):\n",
        "    next_state = digitize_state(observation)\n",
        "\n",
        "    epsilon = 0.2\n",
        "    if  epsilon <= np.random.uniform(0, 1):\n",
        "        next_action = np.argmax(q_table[next_state])\n",
        "    else:\n",
        "        next_action = np.random.choice([0, 1])\n",
        "\n",
        "    # Qテーブルの更新\n",
        "    alpha = 0.2\n",
        "    gamma = 0.99\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
        "            alpha * (reward + gamma * q_table[next_state, next_action])\n",
        "\n",
        "    return next_action, next_state\n",
        "\n",
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "for episode in range(num_episodes):\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "\n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "\n",
        "    episode_reward = 0\n",
        "    for t in range(max_number_of_steps):\n",
        "        # CartPoleの描画\n",
        "        #env.render()\n",
        "\n",
        "        # 行動の実行とフィードバックの取得\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # 行動の選択\n",
        "        action, state = get_action_2(state, action, observation, reward)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print('%d Episode finished after %d time steps / mean %f' % (episode, t + 1,\n",
        "                last_time_steps.mean()))\n",
        "            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))\n",
        "            break\n",
        "\n",
        "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードが195以上であれば成功\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWbdDp9q6yZs"
      },
      "source": [
        "def get_action_3(state, action, observation, reward, episode):\n",
        "    next_state = digitize_state(observation)\n",
        "\n",
        "    epsilon = 0.5 * (0.99 ** episode)\n",
        "    if  epsilon <= np.random.uniform(0, 1):\n",
        "        next_action = np.argmax(q_table[next_state])\n",
        "    else:\n",
        "        next_action = np.random.choice([0, 1])\n",
        "\n",
        "    # Qテーブルの更新\n",
        "    alpha = 0.2\n",
        "    gamma = 0.99\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
        "            alpha * (reward + gamma * q_table[next_state, next_action])\n",
        "\n",
        "    return next_action, next_state\n",
        "\n",
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "for episode in range(num_episodes):\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "\n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "\n",
        "    episode_reward = 0\n",
        "    for t in range(max_number_of_steps):\n",
        "        # CartPoleの描画\n",
        "        #env.render()\n",
        "\n",
        "        # 行動の実行とフィードバックの取得\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # 行動の選択\n",
        "        action, state = get_action_3(state, action, observation, reward, episode)\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print('%d Episode finished after %d time steps / mean %f' % (episode, t + 1,\n",
        "                last_time_steps.mean()))\n",
        "            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))\n",
        "            break\n",
        "\n",
        "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードが195以上であれば成功\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ycxxBH6xp8"
      },
      "source": [
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "for episode in range(num_episodes):\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "\n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "\n",
        "    episode_reward = 0\n",
        "    for t in range(max_number_of_steps):\n",
        "        # CartPoleの描画\n",
        "        #env.render()\n",
        "\n",
        "        # 行動の実行とフィードバックの取得\n",
        "        observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # 罰則の追加\n",
        "        if done:\n",
        "            reward = -200\n",
        "\n",
        "        # 行動の選択\n",
        "        action, state = get_action_3(state, action, observation, reward, episode)\n",
        "\n",
        "        if done:\n",
        "            print('%d Episode finished after %f time steps / mean %f' % (episode, t + 1,\n",
        "                last_time_steps.mean()))\n",
        "            last_time_steps = np.hstack((last_time_steps[1:], [t + 1]))\n",
        "            break\n",
        "\n",
        "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードが195以上であれば成功\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kCPcAYEtf4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ERMWOCbL5n"
      },
      "source": [
        "### <font color=blue>**3.** </font> 方策勾配法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBeMK1kprstg"
      },
      "source": [
        "#### <font color=green>**3.1.** </font> 迷路を解く　その３"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG5GLTkIE1-0"
      },
      "source": [
        "## 出典 : https://book.mynavi.jp/manatee/detail/id=88297"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAgm6V_BE8R4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvOFW7S1E8Pe"
      },
      "source": [
        "# 初期位置での迷路の様子\n",
        " \n",
        "# 図を描く大きさと、図の変数名を宣言\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = plt.gca()\n",
        " \n",
        "# 赤い壁を描く\n",
        "plt.plot([1, 1], [0, 1], color='red', linewidth=2)\n",
        "plt.plot([1, 2], [2, 2], color='red', linewidth=2)\n",
        "plt.plot([2, 2], [2, 1], color='red', linewidth=2)\n",
        "plt.plot([2, 3], [1, 1], color='red', linewidth=2)\n",
        " \n",
        "# 状態を示す文字S0～S8を描く\n",
        "plt.text(0.5, 2.5, 'S0', size=14, ha='center')\n",
        "plt.text(1.5, 2.5, 'S1', size=14, ha='center')\n",
        "plt.text(2.5, 2.5, 'S2', size=14, ha='center')\n",
        "plt.text(0.5, 1.5, 'S3', size=14, ha='center')\n",
        "plt.text(1.5, 1.5, 'S4', size=14, ha='center')\n",
        "plt.text(2.5, 1.5, 'S5', size=14, ha='center')\n",
        "plt.text(0.5, 0.5, 'S6', size=14, ha='center')\n",
        "plt.text(1.5, 0.5, 'S7', size=14, ha='center')\n",
        "plt.text(2.5, 0.5, 'S8', size=14, ha='center')\n",
        "plt.text(0.5, 2.3, 'START', ha='center')\n",
        "plt.text(2.5, 0.3, 'GOAL', ha='center')\n",
        " \n",
        "# 描画範囲の設定と目盛りを消す設定\n",
        "ax.set_xlim(0, 3)\n",
        "ax.set_ylim(0, 3)\n",
        "plt.tick_params(axis='both', which='both', bottom='off', top='off',\n",
        "                labelbottom='off', right='off', left='off', labelleft='off')\n",
        " \n",
        "# 現在値S0に緑丸を描画する\n",
        "line, = ax.plot([0.5], [2.5], marker=\"o\", color='g', markersize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEFgISAoFDSg"
      },
      "source": [
        "# 初期の方策を決定するパラメータtheta_0を設定\n",
        " \n",
        "# 行は状態0～7、列は移動方向で↑、→、↓、←を表す\n",
        "theta_0 = np.array([[np.nan, 1, 1, np.nan],  # s0\n",
        "                    [np.nan, 1, np.nan, 1],  # s1\n",
        "                    [np.nan, np.nan, 1, 1],  # s2\n",
        "                    [1, 1, 1, np.nan],  # s3\n",
        "                    [np.nan, np.nan, 1, 1],  # s4\n",
        "                    [1, np.nan, np.nan, np.nan],  # s5\n",
        "                    [1, np.nan, np.nan, np.nan],  # s6\n",
        "                    [1, 1, np.nan, np.nan],  # s7、※s8はゴールなので、方策はなし\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxaMz3alFDPa"
      },
      "source": [
        "# 方策パラメータthetaを行動方策piにソフトマックス関数で変換する手法の定義\n",
        " \n",
        "def softmax_convert_into_pi_from_theta(theta):\n",
        "    '''ソフトマックス関数で割合を計算する'''\n",
        " \n",
        "    beta =1.0\n",
        "    [m, n] = theta.shape  # thetaの行列サイズを取得\n",
        "    pi = np.zeros((m, n))\n",
        " \n",
        "    exp_theta = np.exp(beta*theta) # thetaをexp(theta)へと変換\n",
        " \n",
        "    for i in range(0, m):\n",
        "        # pi[i, :] = theta[i, :] / np.nansum(theta[i, :])  # simpleに割合の計算の場合\n",
        "        pi[i, :] = exp_theta[i, :] / np.nansum(exp_theta[i, :])  # simpleに割合の計算の場合\n",
        " \n",
        "    pi = np.nan_to_num(pi)  # nanを0に変換\n",
        " \n",
        "    return pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-33b6MfiFDMQ"
      },
      "source": [
        "# 初期の方策pi_0を求める\n",
        " \n",
        "pi_0 = softmax_convert_into_pi_from_theta(theta_0)\n",
        "print(pi_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jWP2eMMFDJq"
      },
      "source": [
        "# 行動と1step移動後の状態sとを求める関数を定義\n",
        " \n",
        "def get_action_and_next_s(pi, s):\n",
        "    direction = [\"up\", \"right\", \"down\", \"left\"]\n",
        "    # pi[s,:]の確率に従って、directionが選択される\n",
        "    next_direction = np.random.choice(direction, p=pi[s, :])\n",
        " \n",
        "    if next_direction == \"up\":\n",
        "        action = 0\n",
        "        s_next = s - 3  # 上に移動するときは状態の数字が3小さくなる\n",
        "    elif next_direction == \"right\":\n",
        "        action = 1\n",
        "        s_next = s + 1  # 右に移動するときは状態の数字が1大きくなる\n",
        "    elif next_direction == \"down\":\n",
        "        action = 2\n",
        "        s_next = s + 3  # 下に移動するときは状態の数字が3大きくなる\n",
        "    elif next_direction == \"left\":\n",
        "        action = 3\n",
        "        s_next = s - 1  # 左に移動するときは状態の数字が1小さくなる\n",
        " \n",
        "    return [action, s_next]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBLKZTSkE8NT"
      },
      "source": [
        "# 迷路を解く関数の定義、状態と行動の履歴を出力\n",
        " \n",
        "def goal_maze_ret_s_a(pi):\n",
        "    s = 0  # スタート地点\n",
        "    s_a_history = [[0, np.nan]]  # エージェントの移動を記録するリスト\n",
        " \n",
        "    while (1):  # ゴールするまでループ\n",
        "        [action, next_s] = get_action_and_next_s(pi, s)\n",
        "        s_a_history[-1][1] = action\n",
        "        # 現在の状態（つまり一番最後なのでindex=-1）の行動を代入\n",
        " \n",
        "        s_a_history.append([next_s, np.nan])\n",
        "        # 次の状態を代入。行動はまだ分からないのでnanにしておく\n",
        " \n",
        "        if next_s == 8:  # ゴール地点なら終了\n",
        "            break\n",
        "        else:\n",
        "            s = next_s\n",
        " \n",
        "    return s_a_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35SQaU0E8Kt"
      },
      "source": [
        "s_a_history = goal_maze_ret_s_a(pi_0)\n",
        "print(s_a_history)\n",
        "print(\"迷路を解くのにかかったステップ数は\"+str(len(s_a_history)-1)+\"です\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qJmjgUE8H4"
      },
      "source": [
        "# thetaの更新関数を定義します\n",
        " \n",
        "def update_theta(theta, pi, s_a_history):\n",
        "    eta = 0.1  # 学習率\n",
        "    T = len(s_a_history) - 1  # ゴールまでの総ステップ数\n",
        " \n",
        "    [m, n] = theta.shape  # thetaの行列サイズを取得\n",
        "    delta_theta = theta.copy()  # Δthetaの元を作成、ポインタ参照なので、delta_theta = thetaはダメ\n",
        " \n",
        "    # delta_thetaを要素ごとに求めます\n",
        "    for i in range(0, m):\n",
        "        for j in range(0, n):\n",
        "            if not(np.isnan(theta[i, j])):  # thetaがnanでない場合\n",
        "                # 履歴から状態iのものを取り出すリスト内包表記です\n",
        "                SA_i = [SA for SA in s_a_history if SA[0] == i]\n",
        "                SA_ij = [SA for SA in s_a_history if SA ==\n",
        "                         [i, j]]  # 状態iで行動jをしたものを取り出す\n",
        "                N_i = len(SA_i)  # 状態iで行動した総回数\n",
        "                N_ij = len(SA_ij)  # 状態iで行動jをとった回数\n",
        "                delta_theta[i, j] = (N_ij + pi[i, j] * N_i) / T\n",
        " \n",
        "    new_theta = theta + eta * delta_theta\n",
        " \n",
        "    return new_theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkD0ifDkFSkP"
      },
      "source": [
        "new_theta = update_theta(theta_0, pi_0, s_a_history)\n",
        "pi = softmax_convert_into_pi_from_theta(new_theta)\n",
        "print(pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eQol_qoFSh5"
      },
      "source": [
        "# 方策勾配法で迷路を解く\n",
        " \n",
        "stop_epsilon = 10**-8  # 10^-8よりも方策に変化が少なくなったら学習終了とする\n",
        " \n",
        "theta = theta_0\n",
        "pi = pi_0\n",
        " \n",
        "is_continue = True\n",
        "count = 1\n",
        "while is_continue:  # is_continueがFalseになるまで繰り返す\n",
        "    s_a_history = goal_maze_ret_s_a(pi)  # 方策πで迷路内を探索した履歴を求める\n",
        "    new_theta = update_theta(theta, pi, s_a_history)  # パラメータΘを更新\n",
        "    new_pi = softmax_convert_into_pi_from_theta(new_theta)  # 方策πの更新\n",
        " \n",
        "    print(np.sum(np.abs(new_pi - pi)))  # 方策の変化を出力\n",
        "    print(\"迷路を解くのにかかったステップ数は\" + str(len(s_a_history) - 1) + \"です\")\n",
        " \n",
        "    if np.sum(np.abs(new_pi - pi)) < stop_epsilon:\n",
        "        is_continue = False\n",
        "    else:\n",
        "        theta = new_theta\n",
        "        pi = new_pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es6jyftwFSfg"
      },
      "source": [
        "np.set_printoptions(precision=3, suppress=True) #有効桁数3、指数表示しないという設定\n",
        "print(pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COsJQIatFSc7"
      },
      "source": [
        "# エージェントの移動の様子を可視化\n",
        "# 参考URL http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        " \n",
        " \n",
        "def init():\n",
        "    # 背景画像の初期化\n",
        "    line.set_data([], [])\n",
        "    return (line,)\n",
        " \n",
        " \n",
        "def animate(i):\n",
        "    # フレームごとの描画内容\n",
        "    state = s_a_history[i][0]  # 現在の場所を描く\n",
        "    x = (state % 3) + 0.5  # 状態のx座標は、3で割った余り+0.5\n",
        "    y = 2.5 - int(state / 3)  # y座標は3で割った商を2.5から引く\n",
        "    line.set_data(x, y)\n",
        "    return (line,)\n",
        " \n",
        " \n",
        "#　初期化関数とフレームごとの描画関数を用いて動画を作成\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(\n",
        "    s_a_history), interval=200, repeat=False)\n",
        " \n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXSHiO_XtdAE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-OFs7I7rtCZ"
      },
      "source": [
        "#### <font color=green>**3.2.** </font> CartPole NumPyで実装 その２"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRBIuXq4_vvQ"
      },
      "source": [
        "## 出典 : https://deepage.net/features/numpy-rl.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WmByAWS_vsC"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "observation = env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRtzYZTC_vmg"
      },
      "source": [
        "# とりあえず右に押して見る\n",
        "action = 1\n",
        "\n",
        "# stepを実行すると行動を起こした直後の状態、報酬、ゲームが終了したかどうか、情報の４つの変数が返される\n",
        "observation, reward, done, info = env.step(action)\n",
        "\n",
        "#env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEdm6fcn_vka"
      },
      "source": [
        "import numpy as np\n",
        "observation = env.reset()\n",
        "\n",
        "for k in range(100):\n",
        "    #env.render()\n",
        "    observation, reward, done, info = env.step(np.random.randint(1)) # 0か1の乱数で実行\n",
        "    env.reset()\n",
        "\n",
        "env.close() # 終了するときはenv.close()を実行する必要がある。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LMFFuUiCOtJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUyqEZnMB2q4"
      },
      "source": [
        "##########\n",
        "# Q-learning\n",
        "##########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsbtrTD2B2WZ"
      },
      "source": [
        "# gymとNumPyのインポート。\n",
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19qD2JHpB8SU"
      },
      "source": [
        "env = gym.make('CartPole-v0') # 環境に相当するオブジェクトをenvとおく。\n",
        "\n",
        "goal_average_steps = 195 # 195ステップ連続でポールが倒れないことを目指す\n",
        "max_number_of_steps = 200 # 最大ステップ数\n",
        "num_consecutive_iterations = 100 # 評価の範囲のエピソード数\n",
        "num_episodes = 5000\n",
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "\n",
        "# 価値関数の値を保存するテーブルを作成する。\n",
        "# np.random.uniformは指定された範囲での一様乱数を返す。\n",
        "q_table = np.random.uniform(low=-1, high=1, size=(4**4, env.action_space.n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwHp76IB8P3"
      },
      "source": [
        "def bins(clip_min, clip_max, num):\n",
        "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n",
        "    # np.linspaceは指定された範囲における等間隔数列を返す。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM8rjRISB8Kl"
      },
      "source": [
        "def digitize_state(observation):\n",
        "    # 各値を４個の離散値に変換\n",
        "    # np.digitizeは与えられた値をbinsで指定した基数に当てはめる関数。インデックスを返す。\n",
        "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
        "    digitized = [np.digitize(cart_pos, bins=bins(-2.4, 2.4, 4)),\n",
        "                 np.digitize(cart_v, bins = bins(-3.0, 3.0, 4)),\n",
        "                 np.digitize(pole_angle, bins=bins(-0.5, 0.5, 4)),\n",
        "                 np.digitize(pole_v, bins=bins(-2.0, 2.0, 4))]\n",
        "\n",
        "    # 0~255に変換\n",
        "    return sum([x* (4**i) for i, x in enumerate(digitized)]) # インデックス付きループをすることができる。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2qNsWigB8Il"
      },
      "source": [
        "def get_action(state, action, observation, reward, episode):\n",
        "    next_state = digitize_state(observation)\n",
        "    epsilon = 0.5 * (0.99** episode)\n",
        "    if epsilon <= np.random.uniform(0, 1): # もし一様乱数のほうが大きければ\n",
        "        next_action = np.argmax(q_table[next_state])# q_tableの中で次に取りうる行動の中で最も価値の高いものを\n",
        "                                                    # next_actionに格納する\n",
        "    else:# そうでなければ20%の確率でランダムな行動を取る\n",
        "        next_action = np.random.choice([0, 1])\n",
        "\n",
        "\n",
        "    # Qテーブルの更新\n",
        "    alpha = 0.2\n",
        "    gamma = 0.99\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] + \\\n",
        "            alpha * (reward + gamma * q_table[next_state, next_action])\n",
        "    return next_action, next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdShriLN_vhs"
      },
      "source": [
        "step_list = []\n",
        "for episode in range(num_episodes):\n",
        "    # 環境の初期化\n",
        "    observation = env.reset()\n",
        "\n",
        "    state = digitize_state(observation)\n",
        "    action = np.argmax(q_table[state])\n",
        "\n",
        "    episode_reward = 0\n",
        "    for t in range(max_number_of_steps):\n",
        "        #env.render() # CartPoleの描画\n",
        "\n",
        "        observation, reward, done, info = env.step(action) # actionを取ったときの環境、報酬、状態が終わったかどうか、デバッグに有益な情報\n",
        "\n",
        "        if done: # 倒れた時罰則を追加する\n",
        "            reward -= 200\n",
        "        # 行動の選択\n",
        "        action, state = get_action(state, action, observation, reward, episode)\n",
        "        episode_reward += reward\n",
        "\n",
        "\n",
        "        if done:\n",
        "            print('%d Episode finished after %f time steps / mean %f' %\n",
        "                (episode, t + 1, last_time_steps.mean()))\n",
        "            last_time_steps = np.hstack((last_time_steps[1:], [t+1]))\n",
        "            # 継続したステップ数をステップのリストの最後に加える。np.hstack関数は配列をつなげる関数。\n",
        "            step_list.append(t+1)\n",
        "            break\n",
        "\n",
        "    if (last_time_steps.mean() >= goal_average_steps): # 直近の100エピソードの平均が195以上であれば成功\n",
        "        print('Episode %d train agent successfully!' % episode)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc_nWjHlCJnP"
      },
      "source": [
        "# 以下のコードでグラフを表示\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(len(step_list)), step_list)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('max_step')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBkKMoj8BTTc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjK3LfqJBTuR"
      },
      "source": [
        "##########\n",
        "# 方策勾配法\n",
        "##########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On2tnOuSBXik"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-RYLTrrBXgf"
      },
      "source": [
        "def do_episode(w, env):\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    num_steps = 0\n",
        "\n",
        "    while not done and num_steps <= max_number_of_steps:\n",
        "        action = take_action(observation, w)\n",
        "        observation, _, done, _ = env.step(action)\n",
        "        num_steps += 1\n",
        "    # ここで報酬を与える。基本的に(連続したステップ数)-(最大ステップ数)で与えられる。\n",
        "    step_val = -1 if num_steps >= max_number_of_steps else num_steps - max_number_of_steps\n",
        "    return step_val, num_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z44yLXxmBXCY"
      },
      "source": [
        "def take_action(X, w): # 値が0を超えたら1を返すようにする\n",
        "    action = 1 if calculate(X, w) > 0.0 else 0\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZwmPVj0BcNg"
      },
      "source": [
        "def calculate(X, w):\n",
        "    result = np.dot(X, w) # 返り値は配列ではなく、１つの値になる。\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDuvg2eUBcI0"
      },
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# env.render()\n",
        "# ゲームの様子を見たいときは env.render()を実行すれば良い\n",
        "\n",
        "eta = 0.2\n",
        "sigma = 0.05 # パラメーターを変動させる値の標準偏差\n",
        "\n",
        "max_episodes = 5000 # 学習を行う最大エピソード数\n",
        "max_number_of_steps = 200\n",
        "n_states = 4 # 入力のパラメーター数\n",
        "num_batch = 10\n",
        "num_consecutive_iterations = 100 # 評価の範囲のエピソード数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9vepI0gBcFd"
      },
      "source": [
        "w = np.random.randn(n_states)\n",
        "reward_list = np.zeros(num_batch)\n",
        "last_time_steps = np.zeros(num_consecutive_iterations)\n",
        "mean_list = [] # 学習の進行具合を過去100エピソードのステップ数の平均で記録する"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91yLbOjUBmUe"
      },
      "source": [
        "for episode in range(max_episodes//num_batch):\n",
        "    N = np.random.normal(scale=sigma,size=(num_batch, w.shape[0]))\n",
        "    # パラメーターの値を変動させるための値。これが偏差になる。\n",
        "\n",
        "    for i in range(num_batch):\n",
        "        w_try = w + N[i]\n",
        "        reward, steps = do_episode(w_try, env)\n",
        "        if i == num_batch-1:\n",
        "            print('%d Episode finished after %d steps / mean %f' %(episode*num_batch, steps, last_time_steps.mean()))\n",
        "        last_time_steps = np.hstack((last_time_steps[1:], [steps]))\n",
        "        reward_list[i] = reward\n",
        "        mean_list.append(last_time_steps.mean())\n",
        "    if last_time_steps.mean() >= 195: break # 平均が195超えたら学習終了\n",
        "\n",
        "    std = np.std(reward_list)\n",
        "    if std == 0: std = 1\n",
        "    # 報酬の値を正規化する\n",
        "    A = (reward_list - np.mean(reward_list))/std\n",
        "    # ここでパラメーターの更新を行う\n",
        "    w_delta = eta /(num_batch*sigma) * np.dot(N.T, A)\n",
        "    # 振れ幅を調整するためにsigmaをかけている。\n",
        "    w += w_delta\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3ikNRV3_zHW"
      },
      "source": [
        "# グラフの表示\n",
        "plt.plot(mean_list)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbe8Wq_rtfQf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNTFnWgmrEW5"
      },
      "source": [
        "### <font color=blue>**4.** </font> DQN（Deep Q-Network）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA82k7ZqrtEn"
      },
      "source": [
        "#### <font color=green>**4.1.** </font> CartPole kerasで実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kNe8apxEE4E"
      },
      "source": [
        "## 出典 : https://qiita.com/sugulu_Ogawa_ISID/items/bc7c70e6658f204f85f9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kArUvJq6EE0P"
      },
      "source": [
        "# coding:utf-8\n",
        "# [0]必要なライブラリのインポート\n",
        "import gym  # 倒立振子(cartpole)の実行環境\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from collections import deque\n",
        "from gym import wrappers  # gymの画像保存\n",
        "from keras import backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61ds6AhSEEvW"
      },
      "source": [
        "# [1]損失関数の定義\n",
        "# 損失関数にhuber関数を使用します 参考https://github.com/jaara/AI-blog/blob/master/CartPole-DQN.py\n",
        "def huberloss(y_true, y_pred):\n",
        "    err = y_true - y_pred\n",
        "    cond = K.abs(err) < 1.0\n",
        "    L2 = 0.5 * K.square(err)\n",
        "    L1 = (K.abs(err) - 0.5)\n",
        "    loss = tf.where(cond, L2, L1)  # Keras does not cover where function in tensorflow :-(\n",
        "    return K.mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCTpGqMqEWaM"
      },
      "source": [
        "# [2]Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "class QNetwork:\n",
        "    def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size))\n",
        "        self.model.add(Dense(hidden_size, activation='relu'))\n",
        "        self.model.add(Dense(action_size, activation='linear'))\n",
        "        self.optimizer = Adam(lr=learning_rate)  # 誤差を減らす学習方法はAdam\n",
        "        # self.model.compile(loss='mse', optimizer=self.optimizer)\n",
        "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\n",
        "\n",
        "    # 重みの学習\n",
        "    def replay(self, memory, batch_size, gamma, targetQN):\n",
        "        inputs = np.zeros((batch_size, 4))\n",
        "        targets = np.zeros((batch_size, 2))\n",
        "        mini_batch = memory.sample(batch_size)\n",
        "\n",
        "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
        "            inputs[i:i + 1] = state_b\n",
        "            target = reward_b\n",
        "\n",
        "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
        "                # 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）\n",
        "                retmainQs = self.model.predict(next_state_b)[0]\n",
        "                next_action = np.argmax(retmainQs)  # 最大の報酬を返す行動を選択する\n",
        "                target = reward_b + gamma * targetQN.model.predict(next_state_b)[0][next_action]\n",
        "\n",
        "            targets[i] = self.model.predict(state_b)    # Qネットワークの出力\n",
        "            targets[i][action_b] = target               # 教師信号\n",
        "\n",
        "        # shiglayさんよりアドバイスいただき、for文の外へ修正しました\n",
        "        self.model.fit(inputs, targets, epochs=1, verbose=0)  # epochsは訓練データの反復回数、verbose=0は表示なしの設定"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgUHKHw6EWXh"
      },
      "source": [
        "# [3]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス\n",
        "class Memory:\n",
        "    def __init__(self, max_size=1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
        "        return [self.buffer[ii] for ii in idx]\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m8TV880EWU1"
      },
      "source": [
        "# [4]カートの状態に応じて、行動を決定するクラス\n",
        "# アドバイスいただき、引数にtargetQNを使用していたのをmainQNに修正しました\n",
        "class Actor:\n",
        "    def get_action(self, state, episode, mainQN):   # [C]ｔ＋１での行動を返す\n",
        "        # 徐々に最適行動のみをとる、ε-greedy法\n",
        "        epsilon = 0.001 + 0.9 / (1.0+episode)\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            retTargetQs = mainQN.model.predict(state)[0]\n",
        "            action = np.argmax(retTargetQs)  # 最大の報酬を返す行動を選択する\n",
        "\n",
        "        else:\n",
        "            action = np.random.choice([0, 1])  # ランダムに行動する\n",
        "\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRtn6rHDEf7o"
      },
      "source": [
        "# [5] メイン関数開始----------------------------------------------------\n",
        "# [5.1] 初期設定--------------------------------------------------------\n",
        "DQN_MODE = 1    # 1がDQN、0がDDQNです\n",
        "LENDER_MODE = 0 # 0は学習後も描画なし、1は学習終了後に描画する\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "num_episodes = 5  # 総試行回数   時間かかりすぎるので、299 -> 5\n",
        "max_number_of_steps = 200  # 1試行のstep数\n",
        "goal_average_reward = 195  # この報酬を超えると学習終了\n",
        "num_consecutive_iterations = 10  # 学習完了評価の平均計算を行う試行回数\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\n",
        "gamma = 0.99    # 割引係数\n",
        "islearned = 0  # 学習が終わったフラグ\n",
        "isrender = 0  # 描画フラグ\n",
        "# ---\n",
        "hidden_size = 16               # Q-networkの隠れ層のニューロンの数\n",
        "learning_rate = 0.00001         # Q-networkの学習係数\n",
        "memory_size = 10000            # バッファーメモリの大きさ\n",
        "batch_size = 32                # Q-networkを更新するバッチの大記載"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lR6q1UHEkO_"
      },
      "source": [
        "# [5.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------\n",
        "mainQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)     # メインのQネットワーク\n",
        "targetQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)   # 価値を計算するQネットワーク\n",
        "# plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化\n",
        "memory = Memory(max_size=memory_size)\n",
        "actor = Actor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe4T9gb5EEsT"
      },
      "source": [
        "# [5.3]メインルーチン--------------------------------------------------------\n",
        "for episode in range(num_episodes):  # 試行数分繰り返す\n",
        "    env.reset()  # cartPoleの環境初期化\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())  # 1step目は適当な行動をとる\n",
        "    state = np.reshape(state, [1, 4])   # list型のstateを、1行4列の行列に変換\n",
        "    episode_reward = 0\n",
        "\n",
        "    # 2018.05.16\n",
        "    # skanmeraさんより間違いを修正いただきました\n",
        "    # targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\n",
        "    # ↓\n",
        "    targetQN.model.set_weights(mainQN.model.get_weights())\n",
        "\n",
        "    for t in range(max_number_of_steps + 1):  # 1試行のループ\n",
        "        if (islearned == 1) and LENDER_MODE:  # 学習終了したらcartPoleを描画する\n",
        "            env.render()\n",
        "            time.sleep(0.1)\n",
        "            print(state[0, 0])  # カートのx位置を出力するならコメントはずす\n",
        "\n",
        "        action = actor.get_action(state, episode, mainQN)   # 時刻tでの行動を決定する\n",
        "        next_state, reward, done, info = env.step(action)   # 行動a_tの実行による、s_{t+1}, _R{t}を計算する\n",
        "        next_state = np.reshape(next_state, [1, 4])     # list型のstateを、1行4列の行列に変換\n",
        "\n",
        "        # 報酬を設定し、与える\n",
        "        if done:\n",
        "            next_state = np.zeros(state.shape)  # 次の状態s_{t+1}はない\n",
        "            if t < 195:\n",
        "                reward = -1  # 報酬クリッピング、報酬は1, 0, -1に固定\n",
        "            else:\n",
        "                reward = 1  # 立ったまま195step超えて終了時は報酬\n",
        "        else:\n",
        "            reward = 0  # 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）\n",
        "\n",
        "        episode_reward += 1 # reward  # 合計報酬を更新\n",
        "\n",
        "        memory.add((state, action, reward, next_state))     # メモリの更新する\n",
        "        state = next_state  # 状態更新\n",
        "\n",
        "\n",
        "        # Qネットワークの重みを学習・更新する replay\n",
        "        if (memory.len() > batch_size) and not islearned:\n",
        "            mainQN.replay(memory, batch_size, gamma, targetQN)\n",
        "\n",
        "        if DQN_MODE:\n",
        "        # 2018.06.12\n",
        "        # shiglayさんさんより間違いを修正いただきました\n",
        "        # targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\n",
        "        # ↓\n",
        "            targetQN.model.set_weights(mainQN.model.get_weights())\n",
        "\n",
        "        # 1施行終了時の処理\n",
        "        if done:\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n",
        "            print('%d Episode finished after %f time steps / mean %f' % (episode, t + 1, total_reward_vec.mean()))\n",
        "            break\n",
        "\n",
        "    # 複数施行の平均報酬で終了を判断\n",
        "    if total_reward_vec.mean() >= goal_average_reward:\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        islearned = 1\n",
        "        if isrender == 0:   # 学習済みフラグを更新\n",
        "            isrender = 1\n",
        "\n",
        "            # env = wrappers.Monitor(env, './movie/cartpoleDDQN')  # 動画保存する場合\n",
        "            # 10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
        "            # if episode>10:\n",
        "            #    if isrender == 0:\n",
        "            #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
        "            #        isrender = 1\n",
        "            #    islearned=1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS4w_cVctfBL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwTCWNUSrtHN"
      },
      "source": [
        "#### <font color=green>**4.2.** </font> CartPole DQN keras-rl2で実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8UDg6z2kfXX"
      },
      "source": [
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 >/dev/null\n",
        "!ln -snf /usr/lib/x86 64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86 64-linux-gnu/libnvrtc-builtins.so \n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg >/dev/null\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay\n",
        "!pip install keras-rl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccJukU0KkbdK"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ll5Gy_pmeeL"
      },
      "source": [
        "#####\n",
        "# DQN\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO_PRK74bDuW"
      },
      "source": [
        "import numpy\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHNzQ6inOC5E"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # 強化学習タスクの環境を構築する\n",
        "  ENV_NAME = 'CartPole-v0'\n",
        "  env = gym.make(ENV_NAME)\n",
        "  numpy.random.seed(123)\n",
        "  env.seed(123)\n",
        "  nb_actions = env.action_space.n\n",
        "\n",
        "  # DQNモデルを準備する\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "  model.add(Dense(16))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(16))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(16))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(nb_actions))\n",
        "  model.add(Activation('linear'))\n",
        "  print(model.summary())\n",
        "\n",
        "  # DQNモデルを最適化する上での目的関数の設定\n",
        "  memory = SequentialMemory(limit=10000, ## 50000 -> 10000\n",
        "                            window_length=1)\n",
        "  policy = BoltzmannQPolicy()\n",
        "  dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,\n",
        "                 nb_steps_warmup=10, target_model_update=1e-2, policy=policy)\n",
        "  dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "  # トレーニング\n",
        "  history = dqn.fit(env, nb_steps=10000, ## 50000 -> 10000 \n",
        "                    visualize=True, verbose=2)\n",
        "\n",
        "  # トレーニングした重みの保存\n",
        "  dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
        "\n",
        "  # テスト\n",
        "  dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE31rWFCOCx6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(history.history['episode_reward'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eCk2VfzT3Wi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qwNqG89rEZK"
      },
      "source": [
        "### <font color=blue>**5.** </font> A2C（Advantage Actor Critic）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe-sXO4yrtJm"
      },
      "source": [
        "#### <font color=green>**5.1.** </font> ライブラリ使用例 : Stable-Baselines3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXEKCGahqmC5"
      },
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-URjFQ8sql_F"
      },
      "source": [
        "!apt install swig\n",
        "!pip install box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDJrerNbPb0W"
      },
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI8zLZYSPi1h"
      },
      "source": [
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Parallel environments\n",
        "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
        "\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "mean_reward_1, std_reward_1 = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "model.learn(total_timesteps=25000)  ###\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward_1:.2f} +/- {std_reward_1}\")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv9YuGAEX5lW"
      },
      "source": [
        "from stable_baselines3 import DQN\n",
        "\n",
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "model = DQN(\"MlpPolicy\", \"CartPole-v1\", verbose=1, exploration_final_eps=0.1, target_update_interval=250)\n",
        "\n",
        "mean_reward_1, std_reward_1 = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "model.learn(total_timesteps=25000)  ###\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward_1:.2f} +/- {std_reward_1}\")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cRkmN1QmeV"
      },
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "eval_env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "model = PPO(\"MlpPolicy\", \"CartPole-v1\", verbose=1)\n",
        "\n",
        "mean_reward_1, std_reward_1 = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "model.learn(total_timesteps=25000)  ###\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward_1:.2f} +/- {std_reward_1}\")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osvTnbzdQmau"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaTbBWdpRZ3L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D73txMntYkfn"
      },
      "source": [
        "## 別のゲームで比較"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9_8uYgZYk-B"
      },
      "source": [
        "eval_env = gym.make('LunarLander-v2')\n",
        "\n",
        "# Parallel environments\n",
        "env = make_vec_env('LunarLander-v2', n_envs=4)\n",
        "\n",
        "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "mean_reward_1, std_reward_1 = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "model.learn(total_timesteps=25000)  ###\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward_1:.2f} +/- {std_reward_1}\")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL5itWnmYk-D"
      },
      "source": [
        "eval_env = gym.make('LunarLander-v2')\n",
        "\n",
        "model = DQN(\"MlpPolicy\", 'LunarLander-v2', verbose=1, exploration_final_eps=0.1, target_update_interval=250)\n",
        "\n",
        "mean_reward_1, std_reward_1 = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "model.learn(total_timesteps=25000)  ###\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward_1:.2f} +/- {std_reward_1}\")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNG1Ehg8Yk-E"
      },
      "source": [
        "eval_env = gym.make('LunarLander-v2')\n",
        "\n",
        "model = PPO(\"MlpPolicy\", 'LunarLander-v2', verbose=1)\n",
        "\n",
        "mean_reward_1, std_reward_1 = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "model.learn(total_timesteps=25000)  ###\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward={mean_reward_1:.2f} +/- {std_reward_1}\")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6x9hKqlYkZb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCtQrR7iYkT6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swk-E9_gYkOa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRcaG20SY1x2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkGZQ0NhY1lR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F907anZRZw0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}