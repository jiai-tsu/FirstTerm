{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_20210413.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Lx1WIP8ts9_S",
        "ky9CyUDnfHSz",
        "ltPJCG6pAUoc",
        "GBypXe5VXdeg",
        "Lcq0o4iwyFmX",
        "zm4jRwc40i3e",
        "SVhdkifcDNk_",
        "jCL-6ATNDSNF",
        "lZ9eHgRRGCFW",
        "I6rlXmZux-U8",
        "vzr9AztdB1Jf",
        "8WntFzUxB_eK",
        "IUEHWgTGEFwE",
        "otkKnpzQT4nM",
        "PZCO1rrXyvOL",
        "fWeIA_pH3FF4",
        "LES5tGOUB59C"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx1WIP8ts9_S"
      },
      "source": [
        "## 42. 変分ベイズ（Variational Bayes）　続き"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky9CyUDnfHSz"
      },
      "source": [
        "### <font color = blue>**4.** </font> ベイズ・ロジスティック回帰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZRh7Rcwtmc5"
      },
      "source": [
        "## 出典： https://github.com/msamunetogetoge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uLlG6n9J4LQ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RP44Ei_Lk_t"
      },
      "source": [
        "def likelifood(w, w_0, S, t, x):\n",
        "  w_1 = w - w_0\n",
        "  y = sigmoid(np.dot(w.T, x)).reshape(-1,)\n",
        "  lf = -1/2 * np.dot(np.dot(w_1.T, np.linalg.inv(S)), w_1) + np.sum(t*np.log(y) + (1-t)*np.log(1-y))\n",
        "  return lf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cI4W7gcgY9w"
      },
      "source": [
        "class SGD():\n",
        "\n",
        "  def __init__(self, w_0, S_0, x, t, rate, th):\n",
        "    self.r = rate\n",
        "    self.w_0 = w_0\n",
        "    self.S_0 = S_0\n",
        "    self.x = x\n",
        "    self.t = t\n",
        "    self.th = th\n",
        "  \n",
        "  def diff(self, w):\n",
        "    Nd = -np.dot((w-self.w_0).T, np.linalg.inv(self.S_0.T))\n",
        "    Br = np.dot(t.T - sigmoid(np.dot(w.T, self.x)), self.x.T)\n",
        "    d = Nd+Br\n",
        "    d = d.reshape(-1,1)\n",
        "    return d\n",
        "  \n",
        "  def learn(self, N):\n",
        "    self.w = self.w_0\n",
        "    for i in range(N):\n",
        "      self.d = self.diff(self.w)\n",
        "      self.w_new = self.w - self.r * self.d\n",
        "      lf_new = likelifood(self.w_new, self.w_0, self.S_0, self.t, self.x)\n",
        "      lf_old = likelifood(self.w, self.w_0, self.S_0, self.t, self.x)\n",
        "      c = (lf_new - lf_old)/lf_old\n",
        "      self.w = self.w_new\n",
        "      self.lf = lf_new\n",
        "      if i % int(N/10) == 0:\n",
        "        print(\"End iteration of {}\".format(i+1))\n",
        "      if np.abs(c)<self.th:\n",
        "        print(\"Convergence!! itr={}\".format(i+1))\n",
        "        break\n",
        "    print(\"End the learning\")\n",
        "    return self.w_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be7DaWSpt0_L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPg_tA5jDjUz"
      },
      "source": [
        "## データの準備\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target_names[iris.target]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-36VWaa-lqU0"
      },
      "source": [
        "df.target.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrA4w1eOlude"
      },
      "source": [
        "data = df.target==\"setosa\"\n",
        "df_reg_index = df[data].index\n",
        "df_reg = df.drop(index=df_reg_index)\n",
        "df_reg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGzOz7w5u3j1"
      },
      "source": [
        "df_reg.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1KsSbTVZEeo"
      },
      "source": [
        "y = df_reg.target\n",
        "x = df_reg.drop(columns=\"target\")\n",
        "\n",
        "stats = x.describe().T\n",
        "def norm(x):\n",
        "  return (x - stats['mean']) /stats['std']\n",
        "\n",
        "m=len(x.columns)\n",
        "w_0 = np.zeros((m,1))\n",
        "S=np.eye(m)\n",
        "\n",
        "x=norm(x).T\n",
        "#x=x.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsO5lEaLm7pD"
      },
      "source": [
        "oe = OrdinalEncoder()\n",
        "encoded = oe.fit_transform(np.array(y).reshape(-1,1))\n",
        "t = encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mj_pxGbvO8Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_823K3xJp1nP"
      },
      "source": [
        "SG = SGD(w_0=w_0, S_0=S, x=x, t=t, rate=0.0001,th=0.00001)\n",
        "w_new = SG.learn(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0A9Hk4gidbY"
      },
      "source": [
        "SG.diff(w_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64TBe0yItwz4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0HQgoYJqh8k"
      },
      "source": [
        "y = sigmoid(np.dot(w_new.T, x))\n",
        "S = -np.linalg.inv(S) + np.sum(y*(1-y)*np.dot(x.T, x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzqklLfGTAAf"
      },
      "source": [
        "w_s = np.random.multivariate_normal(w_new.reshape(-1,), S, 5)\n",
        "for i in w_s :\n",
        "  y = sigmoid(np.dot(i, x))\n",
        "  pred = []\n",
        "  for i in range(max(y.shape)):\n",
        "    pred.append(np.argmax([y[i], 1-y[i]]))\n",
        "  cm = confusion_matrix(t, pred)\n",
        "  print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ7SjRq9lB-7"
      },
      "source": [
        "y = sigmoid(np.dot(w_new.T, x)).reshape(-1,)\n",
        "t_pred = []\n",
        "for i in range(len(y)):\n",
        "  t_pred.append(np.argmax([y[i], 1-y[i]]))\n",
        "cm = confusion_matrix(t, t_pred)\n",
        "print(cm)\n",
        "## 80%くらいの正解率"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLe6zy1YijGa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzzdNX4PduxA"
      },
      "source": [
        "x = df_reg.drop(columns=\"target\")\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x, t.ravel())\n",
        "t_pred = lr.predict(x)\n",
        "cm = confusion_matrix(t, t_pred)\n",
        "print(cm)\n",
        "\n",
        "## sklearn のロジスティック回帰では95%くらい"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgNzmypfQYxi"
      },
      "source": [
        "print(w_new, \"\\n\")\n",
        "print(lr.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imy60CQ6QcOu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltPJCG6pAUoc"
      },
      "source": [
        "### <font color = blue>**5.** </font> ライブラリのサンプルコード\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJDuGA1zMSKw"
      },
      "source": [
        "# TFP Probabilistic Layers: Regression\n",
        "# https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression\n",
        "\n",
        "\n",
        "## Copyright 2019 The TensorFlow Probability Authors.\n",
        "## Licensed under the Apache License, Version 2.0 (the \"License\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dqkZ-33MhPU"
      },
      "source": [
        "# In this example we show how to fit regression models using TFP's \"probabilistic layers.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUwXDXCuwQZz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ0MdF1j8WJf"
      },
      "source": [
        "## Dependencies & Prerequisites\n",
        "\n",
        "# Import\n",
        "\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "sns.reset_defaults()\n",
        "#sns.set_style('whitegrid')\n",
        "#sns.set_context('talk')\n",
        "sns.set_context(context='talk',font_scale=0.7)\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS6nmcOYwPaj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PFfNeJzifo7"
      },
      "source": [
        "## Motivation\n",
        "# Wouldn't it be great if we could use TFP to specify a probabilistic model then simply minimize the negative log-likelihood, i.e.,\n",
        "\n",
        "negloglik = lambda y, rv_y: -rv_y.log_prob(y)\n",
        "\n",
        "# Well not only is it possible, but this colab shows how! (In context of linear regression problems.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zCEYpzu7bDX"
      },
      "source": [
        "# Synthesize dataset.\n",
        "\n",
        "w0 = 0.125\n",
        "b0 = 5.\n",
        "x_range = [-20, 60]\n",
        "\n",
        "def load_dataset(n=150, n_tst=150):\n",
        "  np.random.seed(43)\n",
        "  def s(x):\n",
        "    g = (x - x_range[0]) / (x_range[1] - x_range[0])\n",
        "    return 3 * (0.25 + g**2.)\n",
        "  x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0]\n",
        "  eps = np.random.randn(n) * s(x)\n",
        "  y = (w0 * x * (1. + np.sin(x)) + b0) + eps\n",
        "  x = x[..., np.newaxis]\n",
        "  x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32)\n",
        "  x_tst = x_tst[..., np.newaxis]\n",
        "  return y, x, x_tst\n",
        "\n",
        "y, x, x_tst = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1b7gTZPwUC4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqpE3V1VlD3O"
      },
      "source": [
        "## Case 1: No Uncertainty\n",
        "\n",
        "# Build model.\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(1),\n",
        "  #tf.keras.layers.Dense(1),\n",
        "  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)),\n",
        "])\n",
        "\n",
        "# Do inference.\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)\n",
        "model.fit(x, y, epochs=1000, verbose=False);\n",
        "\n",
        "# Profit.\n",
        "[print(np.squeeze(w.numpy())) for w in model.weights];\n",
        "yhat = model(x_tst)\n",
        "assert isinstance(yhat, tfd.Distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AE9ElaKI6Er"
      },
      "source": [
        "## Figure 1: No uncertainty.\n",
        "\n",
        "w = np.squeeze(model.layers[-2].kernel.numpy())\n",
        "b = np.squeeze(model.layers[-2].bias.numpy())\n",
        "\n",
        "#plt.figure(figsize=[6, 1.5])  # inches\n",
        "plt.figure(figsize=[8, 5])  # inches\n",
        "plt.plot(x, y, 'b.', label='observed');\n",
        "plt.plot(x_tst, yhat.mean(),'r', label='mean', linewidth=4);\n",
        "plt.ylim(-0.,17);\n",
        "plt.yticks(np.linspace(0, 15, 4)[1:]);\n",
        "plt.xticks(np.linspace(*x_range, num=9));\n",
        "\n",
        "ax=plt.gca();\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.spines['left'].set_position(('data', 0))\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "#ax.spines['left'].set_smart_bounds(True)\n",
        "#ax.spines['bottom'].set_smart_bounds(True)\n",
        "plt.legend(loc='center left', fancybox=True, framealpha=0., bbox_to_anchor=(1.05, 0.5))\n",
        "\n",
        "#plt.savefig('/tmp/fig1.png', bbox_inches='tight', dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqkkTI1jwdYZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM2NjnEZlJQt"
      },
      "source": [
        "## Case 2: Aleatoric Uncertainty\n",
        "\n",
        "# Build model.\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(1 + 1),\n",
        "                             tfp.layers.DistributionLambda(\n",
        "                                 lambda t: tfd.Normal(loc=t[..., :1],\n",
        "                                                      scale=1e-3 + tf.math.softplus(0.05 * t[...,1:]))),\n",
        "                             ])\n",
        "\n",
        "# Do inference.\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)\n",
        "model.fit(x, y, epochs=1000, verbose=False);\n",
        "\n",
        "# Profit.\n",
        "[print(np.squeeze(w.numpy())) for w in model.weights];\n",
        "yhat = model(x_tst)\n",
        "assert isinstance(yhat, tfd.Distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZ97_V4PP-f"
      },
      "source": [
        "# Build model.\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(1 + 1),\n",
        "                             tfp.layers.DistributionLambda(\n",
        "                                 lambda t: tfd.Normal(loc=t[..., :1],\n",
        "                                                      scale=1e-3 + tf.math.softplus(0.05 * t[...,1:]))),\n",
        "                             ])\n",
        "\n",
        "# Do inference.\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)\n",
        "model.fit(x, y, epochs=1000, verbose=False);\n",
        "\n",
        "# Profit.\n",
        "[print(np.squeeze(w.numpy())) for w in model.weights];\n",
        "yhat = model(x_tst)\n",
        "assert isinstance(yhat, tfd.Distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSSWw2-FPCiG"
      },
      "source": [
        "#Figure 2: Aleatoric Uncertainty\n",
        "\n",
        "#plt.figure(figsize=[6, 1.5])  # inches\n",
        "plt.figure(figsize=[8, 5])  # inches\n",
        "\n",
        "plt.plot(x, y, 'b.', label='observed');\n",
        "\n",
        "m = yhat.mean()\n",
        "s = yhat.stddev()\n",
        "\n",
        "plt.plot(x_tst, m, 'r', linewidth=4, label='mean');\n",
        "plt.plot(x_tst, m + 2 * s, 'g', linewidth=2, label=r'mean + 2 stddev');\n",
        "plt.plot(x_tst, m - 2 * s, 'g', linewidth=2, label=r'mean - 2 stddev');\n",
        "\n",
        "plt.ylim(-0.,17);\n",
        "plt.yticks(np.linspace(0, 15, 4)[1:]);\n",
        "plt.xticks(np.linspace(*x_range, num=9));\n",
        "\n",
        "ax=plt.gca();\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.spines['left'].set_position(('data', 0))\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "#ax.spines['left'].set_smart_bounds(True)\n",
        "#ax.spines['bottom'].set_smart_bounds(True)\n",
        "plt.legend(loc='center left', fancybox=True, framealpha=0., bbox_to_anchor=(1.05, 0.5))\n",
        "\n",
        "#plt.savefig('/tmp/fig2.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zyfAIq6w0fY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "VwzbWw3_CQ2z"
      },
      "source": [
        "## Case 3: Epistemic Uncertainty\n",
        "\n",
        "# Specify the surrogate posterior over `keras.layers.Dense` `kernel` and `bias`.\n",
        "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
        "  n = kernel_size + bias_size\n",
        "  c = np.log(np.expm1(1.))\n",
        "  return tf.keras.Sequential([\n",
        "                              tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
        "                              tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "                                  tfd.Normal(loc=t[..., :n],\n",
        "                                             scale=1e-5 + tf.nn.softplus(c + t[..., n:])),\n",
        "                                             reinterpreted_batch_ndims=1)),\n",
        "                              ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "aAQhyK9Y_lm1"
      },
      "source": [
        "# Specify the prior over `keras.layers.Dense` `kernel` and `bias`.\n",
        "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
        "  n = kernel_size + bias_size\n",
        "  return tf.keras.Sequential([\n",
        "                              tfp.layers.VariableLayer(n, dtype=dtype),\n",
        "                              tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
        "                                  tfd.Normal(loc=t, scale=1),\n",
        "                                  reinterpreted_batch_ndims=1)),\n",
        "                              ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI7ZCFzSnrWN"
      },
      "source": [
        "# Build model.\n",
        "model = tf.keras.Sequential([\n",
        "  tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/x.shape[0]),\n",
        "  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)),\n",
        "])\n",
        "\n",
        "# Do inference.\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)\n",
        "model.fit(x, y, epochs=1000, verbose=False);\n",
        "\n",
        "# Profit.\n",
        "[print(np.squeeze(w.numpy())) for w in model.weights];\n",
        "yhat = model(x_tst)\n",
        "assert isinstance(yhat, tfd.Distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Bypix9UvTO"
      },
      "source": [
        "# Figure 3: Epistemic Uncertainty\n",
        "\n",
        "#plt.figure(figsize=[6, 1.5])  # inches\n",
        "plt.figure(figsize=[8, 5])  # inches\n",
        "\n",
        "plt.clf();\n",
        "plt.plot(x, y, 'b.', label='observed');\n",
        "\n",
        "yhats = [model(x_tst) for _ in range(100)]\n",
        "avgm = np.zeros_like(x_tst[..., 0])\n",
        "for i, yhat in enumerate(yhats):\n",
        "  m = np.squeeze(yhat.mean())\n",
        "  s = np.squeeze(yhat.stddev())\n",
        "  if i < 25:\n",
        "    plt.plot(x_tst, m, 'g', label='ensemble means' if i == 0 else None, linewidth=0.5)\n",
        "  avgm += m\n",
        "plt.plot(x_tst, avgm/len(yhats), 'r', label='overall mean', linewidth=4)\n",
        "\n",
        "plt.ylim(-0.,17);\n",
        "plt.yticks(np.linspace(0, 15, 4)[1:]);\n",
        "plt.xticks(np.linspace(*x_range, num=9));\n",
        "\n",
        "ax=plt.gca();\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.spines['left'].set_position(('data', 0))\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "#ax.spines['left'].set_smart_bounds(True)\n",
        "#ax.spines['bottom'].set_smart_bounds(True)\n",
        "plt.legend(loc='center left', fancybox=True, framealpha=0., bbox_to_anchor=(1.05, 0.5))\n",
        "\n",
        "#plt.savefig('/tmp/fig3.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liHY2wH6xFnl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcRC3uwcft6l"
      },
      "source": [
        "## Case 4: Aleatoric & Epistemic Uncertainty\n",
        "\n",
        "# Build model.\n",
        "model = tf.keras.Sequential([\n",
        "  tfp.layers.DenseVariational(1 + 1, posterior_mean_field, prior_trainable, kl_weight=1/x.shape[0]),\n",
        "  tfp.layers.DistributionLambda(\n",
        "      lambda t: tfd.Normal(loc=t[..., :1],\n",
        "                           scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]))),\n",
        "])\n",
        "\n",
        "# Do inference.\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)\n",
        "model.fit(x, y, epochs=1000, verbose=False);\n",
        "\n",
        "# Profit.\n",
        "[print(np.squeeze(w.numpy())) for w in model.weights];\n",
        "yhat = model(x_tst)\n",
        "assert isinstance(yhat, tfd.Distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWhfYYzcgFak"
      },
      "source": [
        "# Figure 4: Both Aleatoric & Epistemic Uncertainty\n",
        "\n",
        "#plt.figure(figsize=[6, 1.5])  # inches\n",
        "plt.figure(figsize=[8, 5])  # inches\n",
        "\n",
        "plt.plot(x, y, 'b.', label='observed');\n",
        "\n",
        "yhats = [model(x_tst) for _ in range(100)]\n",
        "avgm = np.zeros_like(x_tst[..., 0])\n",
        "for i, yhat in enumerate(yhats):\n",
        "  m = np.squeeze(yhat.mean())\n",
        "  s = np.squeeze(yhat.stddev())\n",
        "  if i < 15:\n",
        "    plt.plot(x_tst, m, 'r', label='ensemble means' if i == 0 else None, linewidth=1.)\n",
        "    plt.plot(x_tst, m + 2 * s, 'g', linewidth=0.5, label='ensemble means + 2 ensemble stdev' if i == 0 else None);\n",
        "    plt.plot(x_tst, m - 2 * s, 'g', linewidth=0.5, label='ensemble means - 2 ensemble stdev' if i == 0 else None);\n",
        "  avgm += m\n",
        "plt.plot(x_tst, avgm/len(yhats), 'r', label='overall mean', linewidth=4)\n",
        "\n",
        "plt.ylim(-0.,17);\n",
        "plt.yticks(np.linspace(0, 15, 4)[1:]);\n",
        "plt.xticks(np.linspace(*x_range, num=9));\n",
        "\n",
        "ax=plt.gca();\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "ax.yaxis.set_ticks_position('left')\n",
        "ax.spines['left'].set_position(('data', 0))\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "#ax.spines['left'].set_smart_bounds(True)\n",
        "#ax.spines['bottom'].set_smart_bounds(True)\n",
        "plt.legend(loc='center left', fancybox=True, framealpha=0., bbox_to_anchor=(1.05, 0.5))\n",
        "\n",
        "#plt.savefig('/tmp/fig4.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8vwoEMyB5QH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBypXe5VXdeg"
      },
      "source": [
        "### <font color = blue>**6.** </font> ADVI（Automatic Differentiation Variational Inference）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmuPVeQf2O9y"
      },
      "source": [
        "## Variational Inference: Bayesian Neural Networks in PyMC3\n",
        "## https://docs.pymc.io/notebooks/bayesian_neural_network_advi.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFJhslDv26yM"
      },
      "source": [
        "!pip install Theano==1.0.5\n",
        "!pip install arviz\n",
        "!pip install --upgrade pymc3==3.11.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1f2B63uXiUU"
      },
      "source": [
        "## Generating data\n",
        "\n",
        "from warnings import filterwarnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pymc3 as pm\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "print(f\"Running on PyMC3 v{pm.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5yVERTVXicc"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "floatX = theano.config.floatX\n",
        "filterwarnings(\"ignore\")\n",
        "#sns.set_style(\"white\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVra8nPeXiZR"
      },
      "source": [
        "X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\n",
        "X = scale(X)\n",
        "X = X.astype(floatX)\n",
        "Y = Y.astype(floatX)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9ZJVOzhR9_w"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(X[Y == 0, 0], X[Y == 0, 1], label=\"Class 0\")\n",
        "ax.scatter(X[Y == 1, 0], X[Y == 1, 1], color=\"r\", label=\"Class 1\")\n",
        "sns.despine()\n",
        "ax.legend()\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Toy binary classification data set\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtrVQ6XmR979"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAjqYeEnR95M"
      },
      "source": [
        "## Model specification\n",
        "\n",
        "def construct_nn(ann_input, ann_output):\n",
        "    n_hidden = 5\n",
        "\n",
        "    # Initialize random weights between each layer\n",
        "    init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)\n",
        "    init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
        "    init_out = np.random.randn(n_hidden).astype(floatX)\n",
        "\n",
        "    with pm.Model() as neural_network:\n",
        "        # Trick: Turn inputs and outputs into shared variables using the data container pm.Data\n",
        "        # It's still the same thing, but we can later change the values of the shared variable\n",
        "        # (to switch in the test-data later) and pymc3 will just use the new data.\n",
        "        # Kind-of like a pointer we can redirect.\n",
        "        # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
        "        ann_input = pm.Data(\"ann_input\", X_train)\n",
        "        ann_output = pm.Data(\"ann_output\", Y_train)\n",
        "\n",
        "        # Weights from input to hidden layer\n",
        "        weights_in_1 = pm.Normal(\"w_in_1\", 0, sigma=1, shape=(X.shape[1], n_hidden), testval=init_1)\n",
        "\n",
        "        # Weights from 1st to 2nd layer\n",
        "        weights_1_2 = pm.Normal(\"w_1_2\", 0, sigma=1, shape=(n_hidden, n_hidden), testval=init_2)\n",
        "\n",
        "        # Weights from hidden layer to output\n",
        "        weights_2_out = pm.Normal(\"w_2_out\", 0, sigma=1, shape=(n_hidden,), testval=init_out)\n",
        "\n",
        "        # Build neural-network using tanh activation function\n",
        "        act_1 = pm.math.tanh(pm.math.dot(ann_input, weights_in_1))\n",
        "        act_2 = pm.math.tanh(pm.math.dot(act_1, weights_1_2))\n",
        "        act_out = pm.math.sigmoid(pm.math.dot(act_2, weights_2_out))\n",
        "\n",
        "        # Binary classification -> Bernoulli likelihood\n",
        "        out = pm.Bernoulli(\n",
        "            \"out\",\n",
        "            act_out,\n",
        "            observed=ann_output,\n",
        "            total_size=Y_train.shape[0],  # IMPORTANT for minibatches\n",
        "        )\n",
        "    return neural_network\n",
        "\n",
        "\n",
        "neural_network = construct_nn(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuPiDn8rR92R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5K58WjyR9zd"
      },
      "source": [
        "## Variational Inference: Scaling model complexity\n",
        "\n",
        "#from pymc3.theanof import MRG_RandomStreams, set_tt_rng\n",
        "from theano.sandbox.rng_mrg import MRG_RandomStream\n",
        "\n",
        "#set_tt_rng(MRG_RandomStreams(42))\n",
        "pm.set_tt_rng(MRG_RandomStream(42))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lioz5WG0X48x"
      },
      "source": [
        "%%time\n",
        "\n",
        "with neural_network:\n",
        "    inference = pm.ADVI()\n",
        "    approx = pm.fit(n=30000, method=inference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odQxIdqUX46O"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(-inference.hist, label=\"new ADVI\", alpha=0.3)\n",
        "plt.plot(approx.hist, label=\"old ADVI\", alpha=0.3)\n",
        "plt.legend()\n",
        "plt.ylabel(\"ELBO\")\n",
        "plt.xlabel(\"iteration\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Eyh1sbX43i"
      },
      "source": [
        "trace = approx.sample(draws=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq5ckC5BX40q"
      },
      "source": [
        "# We can get predicted probability from model\n",
        "neural_network.out.distribution.p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh15T_q2X4yG"
      },
      "source": [
        "# create symbolic input\n",
        "x = T.matrix(\"X\")\n",
        "# symbolic number of samples is supported, we build vectorized posterior on the fly\n",
        "n = T.iscalar(\"n\")\n",
        "# Do not forget test_values or set theano.config.compute_test_value = 'off'\n",
        "x.tag.test_value = np.empty_like(X_train[:10])\n",
        "n.tag.test_value = 100\n",
        "_sample_proba = approx.sample_node(\n",
        "    neural_network.out.distribution.p, size=n, more_replacements={neural_network[\"ann_input\"]: x}\n",
        ")\n",
        "# It is time to compile the function\n",
        "# No updates are needed for Approximation random generator\n",
        "# Efficient vectorized form of sampling is used\n",
        "sample_proba = theano.function([x, n], _sample_proba)\n",
        "\n",
        "# Create bechmark functions\n",
        "def production_step1():\n",
        "    pm.set_data(new_data={\"ann_input\": X_test, \"ann_output\": Y_test}, model=neural_network)\n",
        "    ppc = pm.sample_posterior_predictive(\n",
        "        trace, samples=500, progressbar=False, model=neural_network\n",
        "    )\n",
        "\n",
        "    # Use probability of > 0.5 to assume prediction of class 1\n",
        "    pred = ppc[\"out\"].mean(axis=0) > 0.5\n",
        "\n",
        "\n",
        "def production_step2():\n",
        "    sample_proba(X_test, 500).mean(0) > 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLks-6tSiDvl"
      },
      "source": [
        "%timeit production_step1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC-U0OL8iDs3"
      },
      "source": [
        "%timeit production_step2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfnbN0cKiDqT"
      },
      "source": [
        "pred = sample_proba(X_test, 500).mean(0) > 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6ZUWcauiDnN"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.scatter(X_test[pred == 0, 0], X_test[pred == 0, 1])\n",
        "ax.scatter(X_test[pred == 1, 0], X_test[pred == 1, 1], color=\"r\")\n",
        "sns.despine()\n",
        "ax.set(title=\"Predicted labels in testing set\", xlabel=\"X\", ylabel=\"Y\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDgaf3soiDkK"
      },
      "source": [
        "print(\"Accuracy = {}%\".format((Y_test == pred).mean() * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CgimO5TiDgn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3cgYIdgiOvk"
      },
      "source": [
        "## Lets look at what the classifier has learned\n",
        "\n",
        "grid = pm.floatX(np.mgrid[-3:3:100j, -3:3:100j])\n",
        "grid_2d = grid.reshape(2, -1).T\n",
        "dummy_out = np.ones(grid.shape[1], dtype=np.int8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOWp6F4EiOrZ"
      },
      "source": [
        "ppc = sample_proba(grid_2d, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddcnoKSGiOoP"
      },
      "source": [
        "## Probability surface\n",
        "\n",
        "cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
        "fig, ax = plt.subplots(figsize=(16, 9))\n",
        "contour = ax.contourf(grid[0], grid[1], ppc.mean(axis=0).reshape(100, 100), cmap=cmap)\n",
        "ax.scatter(X_test[pred == 0, 0], X_test[pred == 0, 1])\n",
        "ax.scatter(X_test[pred == 1, 0], X_test[pred == 1, 1], color=\"r\")\n",
        "cbar = plt.colorbar(contour, ax=ax)\n",
        "_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel=\"X\", ylabel=\"Y\")\n",
        "cbar.ax.set_ylabel(\"Posterior predictive mean probability of class label = 0\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYMvRpkeiOkv"
      },
      "source": [
        "## Uncertainty in predicted value\n",
        "\n",
        "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
        "fig, ax = plt.subplots(figsize=(16, 9))\n",
        "contour = ax.contourf(grid[0], grid[1], ppc.std(axis=0).reshape(100, 100), cmap=cmap)\n",
        "ax.scatter(X_test[pred == 0, 0], X_test[pred == 0, 1])\n",
        "ax.scatter(X_test[pred == 1, 0], X_test[pred == 1, 1], color=\"r\")\n",
        "cbar = plt.colorbar(contour, ax=ax)\n",
        "_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel=\"X\", ylabel=\"Y\")\n",
        "cbar.ax.set_ylabel(\"Uncertainty (posterior predictive standard deviation)\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOeGEJcSig4k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMjl3TCviOgW"
      },
      "source": [
        "## Mini-batch ADVI\n",
        "\n",
        "minibatch_x = pm.Minibatch(X_train, batch_size=50)\n",
        "minibatch_y = pm.Minibatch(Y_train, batch_size=50)\n",
        "neural_network_minibatch = construct_nn(minibatch_x, minibatch_y)\n",
        "with neural_network_minibatch:\n",
        "    approx = pm.fit(40000, method=pm.ADVI())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApT7swowiObb"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(inference.hist)\n",
        "plt.ylabel(\"ELBO\")\n",
        "plt.xlabel(\"iteration\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83xP8xoVik-X"
      },
      "source": [
        "pm.traceplot(trace);\n",
        "#arviz.plot_trace(trace)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bJYMspmik6_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTLH-Vosik3Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcq0o4iwyFmX"
      },
      "source": [
        "## 43. VAE (Variational Autoencoder)\n",
        "\n",
        "<font color=red size=7>GPUの使用を推奨</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm4jRwc40i3e"
      },
      "source": [
        "### <font color=blue>**1.** </font> 実装例その１"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsGln8e9zjEs"
      },
      "source": [
        "## 出典：https://qiita.com/jun40vn/items/374763f478ee094c5041"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVhdkifcDNk_"
      },
      "source": [
        "#### <font color=green>**1.1.** </font> Autoencoderの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eozcn6Rfy5sg"
      },
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUx85plYy5qL"
      },
      "source": [
        "# データセット読み込み\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv2gRoWGy25x"
      },
      "source": [
        "# モデル構築\n",
        "encoding_dim = 32\n",
        "input_img = Input(shape=(784,))\n",
        "\n",
        "x1 = Dense(256, activation='relu')(input_img)  \n",
        "x2 = Dense(64, activation='relu')(x1)  \n",
        "\n",
        "encoded = Dense(encoding_dim, activation='relu')(x2) \n",
        "\n",
        "x3 = Dense(64, activation='relu')(encoded)\n",
        "x4 = Dense(256, activation='relu')(x3)  \n",
        "\n",
        "decoded = Dense(784, activation='sigmoid')(x4) \n",
        "\n",
        "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBE8uZaSy23v"
      },
      "source": [
        "# 学習\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-JH1Y9Wy21Z"
      },
      "source": [
        "# 学習モデルでテスト画像を変換\n",
        "decoded_imgs = autoencoder.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV0mFSidy2zc"
      },
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(10, 2))\n",
        "\n",
        "for i in range(n):\n",
        "  # テスト画像を表示\n",
        "  ax = plt.subplot(2, n, i+1)\n",
        "  plt.imshow(x_test[i].reshape(28, 28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # 変換画像を表示\n",
        "  ax = plt.subplot(2, n, i+1+n)\n",
        "  plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCL-6ATNDSNF"
      },
      "source": [
        "#### <font color=green>**1.2.** </font> VAE全体の実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ1QxJXvDGj4"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse \n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtseEFtpDGhh"
      },
      "source": [
        "# データセット読み込み\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "image_size = x_train.shape[1] # = 784\n",
        "original_dim = image_size * image_size\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "input_shape = (original_dim, )\n",
        "latent_dim = 2 ## 潜在空間の次元"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTmwzdTFDGcW"
      },
      "source": [
        "# Reparametrization Trick \n",
        "def sampling(args):\n",
        "  z_mean, z_logvar = args\n",
        "  batch = K.shape(z_mean)[0]\n",
        "  dim = K.int_shape(z_mean)[1]\n",
        "  epsilon = K.random_normal(shape=(batch, dim), seed = 5) # ε\n",
        "  return z_mean + K.exp(0.5 * z_logvar) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBQyBKMFDGZo"
      },
      "source": [
        "# VAEモデル構築\n",
        "inputs = Input(shape=input_shape)\n",
        "x1 = Dense(256, activation='relu')(inputs)  \n",
        "x2 = Dense(64, activation='relu')(x1) \n",
        "\n",
        "z_mean = Dense(latent_dim)(x2)\n",
        "z_logvar = Dense(latent_dim)(x2)\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_logvar])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_logvar, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyBmUjroy2tb"
      },
      "source": [
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "x3 = Dense(64, activation='relu')(latent_inputs)  \n",
        "x4 = Dense(256, activation='relu')(x3)  \n",
        "\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x4)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXuUnedTy2n-"
      },
      "source": [
        "z_output = encoder(inputs)[2]\n",
        "outputs = decoder(z_output)\n",
        "\n",
        "vae = Model(inputs, outputs, name='variational_autoencoder')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiUtVMz_y2le"
      },
      "source": [
        "## 損失関数\n",
        "\n",
        "# Kullback-Leibler Loss\n",
        "kl_loss = 1 + z_logvar - K.square(z_mean) - K.exp(z_logvar)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "\n",
        "# Reconstruction Loss\n",
        "reconstruction_loss = mse(inputs, outputs)\n",
        "reconstruction_loss *= original_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szm-A-3SD7LO"
      },
      "source": [
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.fit(x_train,\n",
        "        epochs=50,\n",
        "        batch_size=256,\n",
        "        validation_data=(x_test, None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SBUWX7qD7I9"
      },
      "source": [
        "# テスト画像を変換\n",
        "decoded_imgs = vae.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkAiGHP8yFKo"
      },
      "source": [
        "# テスト画像と変換画像の表示\n",
        "n = 10\n",
        "plt.figure(figsize=(10, 2))\n",
        "\n",
        "for i in range(n):\n",
        "  # テスト画像を表示\n",
        "  ax = plt.subplot(2, n, i+1)\n",
        "  plt.imshow(x_test[i].reshape(28, 28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # 変換された画像を表示\n",
        "  ax = plt.subplot(2, n, i+1+n)\n",
        "  plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ9eHgRRGCFW"
      },
      "source": [
        "#### <font color=green>**1.3.** </font> 潜在空間zを平面で表してみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW-cK9f-GB19"
      },
      "source": [
        "import matplotlib.cm as cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9D7zmQnGByS"
      },
      "source": [
        "def plot_results(encoder,\n",
        "                 decoder,\n",
        "                 x_test,\n",
        "                 y_test,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "  z_mean, _, _ = encoder.predict(x_test,\n",
        "                                 batch_size=128)\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  cmap=cm.tab10\n",
        "  plt.scatter(z_mean[:, 0], z_mean[:, 1], c=cmap(y_test))\n",
        "  m = cm.ScalarMappable(cmap=cmap)\n",
        "  m.set_array(y_test)\n",
        "  plt.colorbar(m)\n",
        "  plt.xlabel(\"z[0]\")\n",
        "  plt.ylabel(\"z[1]\")\n",
        "  plt.show()\n",
        "\n",
        "  # (-4, -4) から (4, 4) までを30x30分割してプロットする\n",
        "  n = 30  # 50>30\n",
        "  digit_size = 28\n",
        "  figure = np.zeros((digit_size * n, digit_size * n))\n",
        "  grid_x = np.linspace(-4, 4, n)\n",
        "  grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "  for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "      z_sample = np.array([[xi, yi]])\n",
        "      x_decoded = decoder.predict(z_sample)\n",
        "      digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "      figure[i * digit_size: (i + 1) * digit_size,\n",
        "             j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "\n",
        "  start_range = digit_size // 2\n",
        "  end_range = n * digit_size + start_range + 1\n",
        "  pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "\n",
        "  sample_range_x = np.round(grid_x, 1)\n",
        "  sample_range_y = np.round(grid_y, 1)\n",
        "\n",
        "  plt.xticks(pixel_range, sample_range_x)\n",
        "  plt.yticks(pixel_range, sample_range_y)\n",
        "\n",
        "  plt.xlabel(\"z[0]\")\n",
        "  plt.ylabel(\"z[1]\")\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.imshow(figure, cmap='Greys_r')\n",
        "  #plt.savefig(filename)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWT-E_RmGBt7"
      },
      "source": [
        "plot_results(encoder,\n",
        "             decoder,\n",
        "             x_test,\n",
        "             y_test,\n",
        "             batch_size=128,\n",
        "             model_name=\"vae_mlp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE9bSFkFGBpD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6rlXmZux-U8"
      },
      "source": [
        "### <font color=blue>**2.** </font> 実装例その２"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHolxoD81NH-"
      },
      "source": [
        "## 出典：　https://qiita.com/MuAuan/items/cdb8ae656da60b6d89ca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzr9AztdB1Jf"
      },
      "source": [
        "#### <font color=green>**2.1.** </font> MNISTのAutoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECYHod5yyAiz"
      },
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Reshape, Embedding,InputLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKmW0TNV94jf"
      },
      "source": [
        "def plot_fig(x_test, decoded_imgs, encoded_imgs,k):\n",
        "  n = 10\n",
        "  plt.figure(figsize=(10, 16))\n",
        "  for j in range(0,n):\n",
        "    for i in range(1,n+1):\n",
        "      # display original\n",
        "      ax1 = plt.subplot(20, n*1, i+10*2*j)\n",
        "      ax1.imshow(x_test[i+10*j].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax1.get_xaxis().set_visible(False)\n",
        "      ax1.get_yaxis().set_visible(False)\n",
        "\n",
        "      # display reconstruction\n",
        "      ax2 = plt.subplot(20, n*1, i + (2*j+1)*10)\n",
        "      ax2.imshow(decoded_imgs[i+10*j].reshape(28, 28))\n",
        "      plt.gray()\n",
        "      ax2.get_xaxis().set_visible(False)\n",
        "      ax2.get_yaxis().set_visible(False)\n",
        "\n",
        "  plt.savefig(\"./mnist1000/mnist_training_by_100_10_{}\".format(k))    \n",
        "  plt.pause(0.01)\n",
        "  plt.close()\n",
        "\n",
        "  n = 100\n",
        "  plt.figure(figsize=(10, 16))\n",
        "  for i in range(1,n+1):\n",
        "    ax = plt.subplot(10, n*0.1, i)\n",
        "    plt.imshow(encoded_imgs[i].reshape(8, 2 * 8).T)\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  plt.savefig(\"/content/mnist1000/mnist_intermid_training_by_100_10_{}\".format(k))  ##\n",
        "  plt.pause(0.01)\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeZ_CyPDAT2C"
      },
      "source": [
        "!mkdir /content/mnist1000/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZCnVBsl94lv"
      },
      "source": [
        "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
        "\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same',name='encoded')(x)\n",
        "encoder=Model(input_img, encoded)\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocG7B9Oc94p4"
      },
      "source": [
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqaeHr-l94uJ"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train[:1000].astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train[:1000], (len(x_train[:1000]), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "y_train=y_train[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pd6f27G94yl"
      },
      "source": [
        "for j in range(10):\n",
        "  x_train1 = x_train\n",
        "  x_test1 = x_test\n",
        "\n",
        "  autoencoder.fit(x_train1, x_train1,\n",
        "                  epochs=10,\n",
        "                  batch_size=128,\n",
        "                  shuffle=True,\n",
        "                  validation_data=(x_test1, x_test1)\n",
        "                  )\n",
        "\n",
        "  decoded_imgs = autoencoder.predict(x_test)\n",
        "  encoded_imgs = encoder.predict(x_test)\n",
        "  \n",
        "  plot_fig(x_test,decoded_imgs,encoded_imgs,j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QIYi5Mh941H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WntFzUxB_eK"
      },
      "source": [
        "#### <font color=green>**2.2.** </font> MNISTのVAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXOg0N9gCNAU"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Activation, Flatten\n",
        "from keras.layers import Reshape, Embedding,InputLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAu1NGxkCM-A"
      },
      "source": [
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
        "# z = z_mean + sqrt(var)*eps\n",
        "def sampling(args):\n",
        "  \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
        "  # Arguments\n",
        "      args (tensor): mean and log of variance of Q(z|X)\n",
        "  # Returns\n",
        "      z (tensor): sampled latent vector\n",
        "  \"\"\"\n",
        "  z_mean, z_log_var = args\n",
        "  batch = K.shape(z_mean)[0]\n",
        "  dim = K.int_shape(z_mean)[1]\n",
        "  # by default, random_normal has mean=0 and std=1.0\n",
        "  epsilon = K.random_normal(shape=(batch, dim))\n",
        "  return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWnEgBCpCkcT"
      },
      "source": [
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "  \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
        "  # Arguments\n",
        "      models (tuple): encoder and decoder models\n",
        "      data (tuple): test data and label\n",
        "      batch_size (int): prediction batch size\n",
        "      model_name (string): which model is using this function\n",
        "  \"\"\"\n",
        "  encoder, decoder = models\n",
        "  x_test, y_test = data\n",
        "  os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "  filename1 = \"/content/mnist1000/vae_mean_all.png\"  ##\n",
        "  # display a 2D plot of the digit classes in the latent space\n",
        "  z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size)\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel(\"z[0]\")\n",
        "  plt.ylabel(\"z[1]\")\n",
        "  plt.savefig(filename1)\n",
        "  plt.show()\n",
        "\n",
        "  filename2 = \"/content/mnist1000/digits_over_latent_all.png\"  ##\n",
        "  # display a 30x30 2D manifold of digits\n",
        "  n = 30\n",
        "  digit_size = 28\n",
        "  figure = np.zeros((digit_size * n, digit_size * n))\n",
        "  # linearly spaced coordinates corresponding to the 2D plot\n",
        "  # of digit classes in the latent space\n",
        "  grid_x = np.linspace(-4, 4, n)\n",
        "  grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "  for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "      z_sample = np.array([[xi, yi]])\n",
        "      x_decoded = decoder.predict(z_sample)\n",
        "      digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "      figure[i * digit_size: (i + 1) * digit_size,\n",
        "             j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  start_range = digit_size // 2\n",
        "  end_range = n * digit_size + start_range + 1\n",
        "  pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "  sample_range_x = np.round(grid_x, 1)\n",
        "  sample_range_y = np.round(grid_y, 1)\n",
        "  plt.xticks(pixel_range, sample_range_x)\n",
        "  plt.yticks(pixel_range, sample_range_y)\n",
        "  plt.xlabel(\"z[0]\")\n",
        "  plt.ylabel(\"z[1]\")\n",
        "  plt.imshow(figure, cmap='Greys_r')\n",
        "  plt.savefig(filename2)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzZBHwXiCkZQ"
      },
      "source": [
        "def plot_results2(models,\n",
        "                  data,\n",
        "                  batch_size=128,\n",
        "                  model_name=\"vae_mnist\"):\n",
        "  z0=[-0.7,-3]\n",
        "  z7=[-0.7,2]\n",
        "  for t in range(50):\n",
        "    s=t/50\n",
        "    z_sample=np.array([[s*(-0.7)+(1-s)*(-0.7),s*(-3)+(1-s)*2]])\n",
        "    x_decoded = decoder.predict(z_sample)\n",
        "    plt.imshow(x_decoded.reshape(28, 28))\n",
        "    plt.title(\"z_sample=\"+str(z_sample))\n",
        "    plt.savefig('/content/mnist1000/z_sample_t{}'.format(t)) ##\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBkutGq6CM6s"
      },
      "source": [
        "# MNIST dataset\n",
        "#(x_train, _), (x_test, _) = mnist.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "x_train = x_train[:60000].astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train[:60000], (len(x_train[:60000]), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) \n",
        "y_train=y_train[:60000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBRwXOjtDTos"
      },
      "source": [
        "# network parameters\n",
        "#input_shape = (original_dim, )\n",
        "input_shape = (image_size, image_size, 1)\n",
        "intermediate_dim = 512\n",
        "batch_size = 64 ## 128 -> 64\n",
        "latent_dim = 2\n",
        "epochs = 20  ### 100 -> 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccvV6tqsDTlm"
      },
      "source": [
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same',name='encoded')(x)\n",
        "shape = K.int_shape(x)\n",
        "print(\"shape[1], shape[2], shape[3]\",shape[1], shape[2], shape[3])\n",
        "x = Flatten()(x)\n",
        "\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0zotnviDThg"
      },
      "source": [
        "# build decoder model\n",
        "# decoder\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n",
        "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "outputs = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvR-adO4Dkas"
      },
      "source": [
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "# loss関数\n",
        "# Compute VAE loss\n",
        "reconstruction_loss = binary_crossentropy(K.flatten(inputs),\n",
        "                                          K.flatten(outputs))\n",
        "reconstruction_loss *= image_size * image_size\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooeJqvEwDkYL"
      },
      "source": [
        "x_train1 = x_train\n",
        "x_test1 = x_test\n",
        "\n",
        "#vae.load_weights('vae_mnist_weights_100.h5')\n",
        "#encoder.load_weights('encoder_mnist_weights_100.h5')\n",
        "#decoder.load_weights('decoder_mnist_weights_100.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXXAcgErDkVA"
      },
      "source": [
        "# autoencoderの実行\n",
        "vae.fit(x_train1,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test1, None))\n",
        "\n",
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)\n",
        "\n",
        "plot_results(models,\n",
        "             data,\n",
        "             batch_size=batch_size,\n",
        "             model_name=\"vae_mlp\")\n",
        "\n",
        "plot_results2(models,\n",
        "              data,\n",
        "              batch_size=batch_size,\n",
        "              model_name=\"vae_mlp\")\n",
        "\n",
        "vae.save_weights('vae_mnist_weights_100.h5')\n",
        "encoder.save_weights('encoder_mnist_weights_100.h5')\n",
        "decoder.save_weights('decoder_mnist_weights_100.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzrUipxB943k"
      },
      "source": [
        "# 実行結果の表示\n",
        "n = 10\n",
        "decoded_imgs = vae.predict(x_test[:n])\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(n):\n",
        "  # original_image\n",
        "  orig_img = x_test[i].reshape(image_size, image_size)\n",
        "\n",
        "  # reconstructed_image\n",
        "  reconst_img = decoded_imgs[i].reshape(image_size, image_size)\n",
        "\n",
        "  # diff image\n",
        "  diff_img = ((orig_img - reconst_img)+2)/4\n",
        "  diff_img = (diff_img*255).astype(np.uint8)\n",
        "  orig_img = (orig_img*255).astype(np.uint8)\n",
        "  reconst_img = (reconst_img*255).astype(np.uint8)\n",
        "  diff_img_color = cv2.applyColorMap(diff_img, cv2.COLORMAP_JET)\n",
        "\n",
        "  # display original\n",
        "  ax = plt.subplot(3, n,  i + 1)\n",
        "  plt.imshow(orig_img, cmap=plt.cm.gray)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(3, n, i + n + 1)\n",
        "  plt.imshow(reconst_img, cmap=plt.cm.gray)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display diff\n",
        "  ax = plt.subplot(3, n, i + n*2 + 1)\n",
        "  plt.imshow(diff_img, cmap=plt.cm.jet)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.savefig(\"autodetect_all.jpg\")\n",
        "plt.pause(1)\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY0qWbVX948a"
      },
      "source": [
        "# 学習結果の保存\n",
        "vae.save('./ae_mnist.h5')\n",
        "\n",
        "# json and weights\n",
        "model_json = vae.to_json()\n",
        "with open('ae_mnist.json', 'w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "vae.save_weights('ae_mnist_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0La98f3946E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUEHWgTGEFwE"
      },
      "source": [
        "#### <font color=green>**2.3.** </font> 異常検知について"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VW2E7iI94-y"
      },
      "source": [
        "## 以下のコードのように学習を７のみに限定して学習すると、\n",
        "## そのz空間での様子を見るとほぼ全領域で７のような形状になっています\n",
        "\n",
        "# 学習に使うデータを限定する\n",
        "x_train1 = x_train[y_train==7]\n",
        "x_test1 = x_test[y_test==7]\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "vae.fit(x_train1,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test1, None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEomdQmnr01S"
      },
      "source": [
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)\n",
        "\n",
        "plot_results(models,\n",
        "             data,\n",
        "             batch_size=batch_size,\n",
        "             model_name=\"vae_mlp\")\n",
        "\n",
        "plot_results2(models,\n",
        "              data,\n",
        "              batch_size=batch_size,\n",
        "              model_name=\"vae_mlp\")\n",
        "\n",
        "vae.save_weights('vae_mnist_weights_100.h5')\n",
        "encoder.save_weights('encoder_mnist_weights_100.h5')\n",
        "decoder.save_weights('decoder_mnist_weights_100.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEPIK7nwr0y4"
      },
      "source": [
        "# 実行結果の表示\n",
        "n = 10\n",
        "decoded_imgs = vae.predict(x_test[:n])\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(n):\n",
        "  # original_image\n",
        "  orig_img = x_test[i].reshape(image_size, image_size)\n",
        "\n",
        "  # reconstructed_image\n",
        "  reconst_img = decoded_imgs[i].reshape(image_size, image_size)\n",
        "\n",
        "  # diff image\n",
        "  diff_img = ((orig_img - reconst_img)+2)/4\n",
        "  diff_img = (diff_img*255).astype(np.uint8)\n",
        "  orig_img = (orig_img*255).astype(np.uint8)\n",
        "  reconst_img = (reconst_img*255).astype(np.uint8)\n",
        "  diff_img_color = cv2.applyColorMap(diff_img, cv2.COLORMAP_JET)\n",
        "\n",
        "  # display original\n",
        "  ax = plt.subplot(3, n,  i + 1)\n",
        "  plt.imshow(orig_img, cmap=plt.cm.gray)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(3, n, i + n + 1)\n",
        "  plt.imshow(reconst_img, cmap=plt.cm.gray)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display diff\n",
        "  ax = plt.subplot(3, n, i + n*2 + 1)\n",
        "  plt.imshow(diff_img, cmap=plt.cm.jet)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.savefig(\"autodetect_all.jpg\")\n",
        "plt.pause(1)\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCUfD5DGr0wA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFY9I6-T4XH4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE56zYEz4WYC"
      },
      "source": [
        "## 44. サンプリング法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otkKnpzQT4nM"
      },
      "source": [
        "### <font color=blue>**1.** </font> NUTS（No-U-Turn Sampler）の簡単な例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgIhOLHh4lhq"
      },
      "source": [
        "## Variational API quickstart\n",
        "## https://docs.pymc.io/notebooks/variational_api_quickstart.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qus8zCl2SSch"
      },
      "source": [
        "!pip install Theano==1.0.5\n",
        "!pip install arviz\n",
        "!pip install --upgrade pymc3==3.11.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tJgYmnWPMEX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pymc3 as pm\n",
        "import theano\n",
        "\n",
        "np.random.seed(42)\n",
        "pm.set_tt_rng(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCJl3I6hUp5T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnS-S2olPMBe"
      },
      "source": [
        "## Basic setup\n",
        "\n",
        "w = pm.floatX([0.2, 0.8])\n",
        "mu = pm.floatX([-0.3, 0.5])\n",
        "sd = pm.floatX([0.1, 0.1])\n",
        "\n",
        "with pm.Model() as model:\n",
        "  x = pm.NormalMixture(\"x\", w=w, mu=mu, sigma=sd, dtype=theano.config.floatX)\n",
        "  x2 = x ** 2\n",
        "  sin_x = pm.math.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsIt81O0MlRg"
      },
      "source": [
        "with model:\n",
        "  pm.Deterministic(\"x2\", x2)\n",
        "  pm.Deterministic(\"sin_x\", sin_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx3qiW1wReuq"
      },
      "source": [
        "with model:\n",
        "  trace = pm.sample(50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ibb42OFResc"
      },
      "source": [
        "#pm.traceplot(trace)\n",
        "\n",
        "import arviz\n",
        "arviz.plot_trace(trace);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-QgByn4RBrd"
      },
      "source": [
        "with pm.Model() as model:\n",
        "  x = pm.NormalMixture(\"x\", w=w, mu=mu, sigma=sd, dtype=theano.config.floatX)\n",
        "  x2 = x ** 2\n",
        "  sin_x = pm.math.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRussa_oRBpR"
      },
      "source": [
        "with model:\n",
        "  mean_field = pm.fit(method=\"advi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rqTt7VDRBns"
      },
      "source": [
        "pm.plot_posterior(mean_field.sample(1000), color=\"LightSeaGreen\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQCuVTn-RBkj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_pr_vYnRBi5"
      },
      "source": [
        "## Checking convergence\n",
        "\n",
        "help(pm.callbacks.CheckParametersConvergence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PovkUAkbUlMH"
      },
      "source": [
        "from pymc3.variational.callbacks import CheckParametersConvergence\n",
        "\n",
        "with model:\n",
        "  mean_field = pm.fit(method=\"advi\", callbacks=[CheckParametersConvergence()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwTx47yLUlKF"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(mean_field.hist);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvaF9eNcU2yj"
      },
      "source": [
        "with model:\n",
        "  mean_field = pm.fit(\n",
        "      method=\"advi\", callbacks=[pm.callbacks.CheckParametersConvergence(diff=\"absolute\")]\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpiu-lsIU2wD"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(mean_field.hist);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uvo75fzU2t6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYDj0uuwU8ku"
      },
      "source": [
        "## Tracking parameters\n",
        "\n",
        "with model:\n",
        "  advi = pm.ADVI()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBFIcaQOU8jO"
      },
      "source": [
        "advi.approx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsIFs6szU8hN"
      },
      "source": [
        "advi.approx.shared_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH1Itrs-U8fP"
      },
      "source": [
        "advi.approx.mean.eval(), advi.approx.std.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pa_IVCFVPfw"
      },
      "source": [
        "tracker = pm.callbacks.Tracker(\n",
        "    mean=advi.approx.mean.eval,  # callable that returns mean\n",
        "    std=advi.approx.std.eval,  # callable that returns std\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWSrsqQqU8dN"
      },
      "source": [
        "approx = advi.fit(20000, callbacks=[tracker])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOF8lsZpUlH_"
      },
      "source": [
        "fig = plt.figure(figsize=(16, 9))\n",
        "mu_ax = fig.add_subplot(221)\n",
        "std_ax = fig.add_subplot(222)\n",
        "hist_ax = fig.add_subplot(212)\n",
        "mu_ax.plot(tracker[\"mean\"])\n",
        "mu_ax.set_title(\"Mean track\")\n",
        "std_ax.plot(tracker[\"std\"])\n",
        "std_ax.set_title(\"Std track\")\n",
        "hist_ax.plot(advi.hist)\n",
        "hist_ax.set_title(\"Negative ELBO track\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMiGqVB9Vm9h"
      },
      "source": [
        "advi.refine(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_bqSu40Vm61"
      },
      "source": [
        "fig = plt.figure(figsize=(16, 9))\n",
        "mu_ax = fig.add_subplot(221)\n",
        "std_ax = fig.add_subplot(222)\n",
        "hist_ax = fig.add_subplot(212)\n",
        "mu_ax.plot(tracker[\"mean\"])\n",
        "mu_ax.set_title(\"Mean track\")\n",
        "std_ax.plot(tracker[\"std\"])\n",
        "std_ax.set_title(\"Std track\")\n",
        "hist_ax.plot(advi.hist)\n",
        "hist_ax.set_title(\"Negative ELBO track\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Y5NyFyVm3e"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "ax = sns.kdeplot(trace[\"x\"], label=\"NUTS\")\n",
        "sns.kdeplot(approx.sample(10000)[\"x\"], label=\"ADVI\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtwFyfu1Vm1a"
      },
      "source": [
        "with model:\n",
        "  svgd_approx = pm.fit(\n",
        "      300,\n",
        "      method=\"svgd\",\n",
        "      inf_kwargs=dict(n_particles=1000),\n",
        "      obj_optimizer=pm.sgd(learning_rate=0.01)\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_UYGY6nVmzY"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "ax = sns.kdeplot(trace[\"x\"], label=\"NUTS\")\n",
        "sns.kdeplot(approx.sample(10000)[\"x\"], label=\"ADVI\")\n",
        "sns.kdeplot(svgd_approx.sample(2000)[\"x\"], label=\"SVGD\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpTd4QRVLoK"
      },
      "source": [
        "# recall x ~ NormalMixture\n",
        "a = x ** 2\n",
        "b = pm.math.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-GI7akVLli"
      },
      "source": [
        "help(svgd_approx.sample_node)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN0un1eaUlGL"
      },
      "source": [
        "a_sample = svgd_approx.sample_node(a)\n",
        "a_sample.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syt1KDWpWi5q"
      },
      "source": [
        "a_sample.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8JUHShmWkie"
      },
      "source": [
        "a_sample.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aIhlqKvWlHY"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.kdeplot(np.array([a_sample.eval() for _ in range(2000)]))\n",
        "plt.title(\"$x^2$ distribution\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXFjupQrWobN"
      },
      "source": [
        "a_samples = svgd_approx.sample_node(a, size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9b4-VvQWrGw"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.kdeplot(a_samples.eval())\n",
        "plt.title(\"$x^2$ distribution\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUkEvC2NWsrI"
      },
      "source": [
        "a_samples.var(0).eval()  # variance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6kYnJXYWubG"
      },
      "source": [
        "a_samples.mean(0).eval()  # mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41eC0fgWvx8"
      },
      "source": [
        "i = theano.tensor.iscalar(\"i\")\n",
        "i.tag.test_value = 1\n",
        "a_samples_i = svgd_approx.sample_node(a, size=i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfwL1RkAW1d7"
      },
      "source": [
        "a_samples_i.eval({i: 100}).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4S4k2rzW1bu"
      },
      "source": [
        "a_samples_i.eval({i: 10000}).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVua_FGlW1XT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiH95koFW4d0"
      },
      "source": [
        "## Converting a Trace to an Approximation\n",
        "\n",
        "trace_approx = pm.Empirical(trace, model=model)\n",
        "trace_approx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHDxT4xHW4b4"
      },
      "source": [
        "pm.plot_posterior(trace_approx.sample(10000));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zjKTLOXW4Zo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ekms0IXW1VX"
      },
      "source": [
        "## Multilabel logistic regression\n",
        "\n",
        "import pandas as pd\n",
        "import theano.tensor as tt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_iris(True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwn3b3HEXeXA"
      },
      "source": [
        "Xt = theano.shared(X_train)\n",
        "yt = theano.shared(y_train)\n",
        "\n",
        "with pm.Model() as iris_model:\n",
        "  # Coefficients for features\n",
        "  β = pm.Normal(\"β\", 0, sigma=1e2, shape=(4, 3))\n",
        "  # Transoform to unit interval\n",
        "  a = pm.Flat(\"a\", shape=(3,))\n",
        "  p = tt.nnet.softmax(Xt.dot(β) + a)\n",
        "\n",
        "  observed = pm.Categorical(\"obs\", p=p, observed=yt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TqMfLG0XePT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlzPIZQMXeNU"
      },
      "source": [
        "## Applying replacements in practice\n",
        "\n",
        "with iris_model:\n",
        "  # We'll use SVGD\n",
        "  inference = pm.SVGD(n_particles=500, jitter=1)\n",
        "\n",
        "  # Local reference to approximation\n",
        "  approx = inference.approx\n",
        "\n",
        "  # Here we need `more_replacements` to change train_set to test_set\n",
        "  test_probs = approx.sample_node(p, more_replacements={Xt: X_test}, size=100)\n",
        "\n",
        "  # For train set no more replacements needed\n",
        "  train_probs = approx.sample_node(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXHRH5zzXeLU"
      },
      "source": [
        "test_ok = tt.eq(test_probs.argmax(-1), y_test)\n",
        "train_ok = tt.eq(train_probs.argmax(-1), y_train)\n",
        "test_accuracy = test_ok.mean(-1)\n",
        "train_accuracy = train_ok.mean(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFNAqDJeXeIU"
      },
      "source": [
        "eval_tracker = pm.callbacks.Tracker(\n",
        "    test_accuracy=test_accuracy.eval, train_accuracy=train_accuracy.eval\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDm4kjwbXeGA"
      },
      "source": [
        "inference.fit(100, callbacks=[eval_tracker]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVKdqpksW1S0"
      },
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(12,8))\n",
        "df = pd.DataFrame(eval_tracker[\"test_accuracy\"]).T.melt()\n",
        "sns.lineplot(x=\"variable\", y=\"value\", data=df, color=\"red\", ax=ax)\n",
        "ax.plot(eval_tracker[\"train_accuracy\"], color=\"blue\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "plt.legend([\"test_accuracy\", \"train_accuracy\"])\n",
        "plt.title(\"Training Progress\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jNAIkAFX0In"
      },
      "source": [
        "inference.fit(400, obj_optimizer=pm.adamax(learning_rate=0.1), callbacks=[eval_tracker]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7UBMOOoX0Fy"
      },
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(12,8))\n",
        "df = pd.DataFrame(np.asarray(eval_tracker[\"test_accuracy\"])).T.melt()\n",
        "sns.lineplot(x=\"variable\", y=\"value\", data=df, color=\"red\", ax=ax)\n",
        "ax.plot(eval_tracker[\"train_accuracy\"], color=\"blue\")\n",
        "ax.set_xlabel(\"epoch\")\n",
        "plt.legend([\"test_accuracy\", \"train_accuracy\"])\n",
        "plt.title(\"Training Progress\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op2KVl3uX0Dx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DtTunxWX0BY"
      },
      "source": [
        "## Minibatches\n",
        "\n",
        "issubclass(pm.Minibatch, theano.tensor.TensorVariable)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD4yn1SbXz_R"
      },
      "source": [
        "# Raw values\n",
        "data = np.random.rand(40000, 100)\n",
        "# Scaled values\n",
        "data *= np.random.randint(1, 10, size=(100,))\n",
        "# Shifted values\n",
        "data += np.random.rand(100) * 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWJoIL0WXz5Q"
      },
      "source": [
        "with pm.Model() as model:\n",
        "  mu = pm.Flat(\"mu\", shape=(100,))\n",
        "  sd = pm.HalfNormal(\"sd\", shape=(100,))\n",
        "  lik = pm.Normal(\"lik\", mu, sd, observed=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5piU9lBX9nM"
      },
      "source": [
        "def stop_after_10(approx, loss_history, i):\n",
        "  if (i > 0) and (i % 10) == 0:\n",
        "    raise StopIteration(\"I was slow, sorry\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCgFxll0X9lK"
      },
      "source": [
        "with model:\n",
        "  advifit = pm.fit(callbacks=[stop_after_10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0P6vZ7WX9i-"
      },
      "source": [
        "X = pm.Minibatch(data, batch_size=500)\n",
        "\n",
        "with pm.Model() as model:\n",
        "  mu = pm.Flat(\"mu\", shape=(100,))\n",
        "  sd = pm.HalfNormal(\"sd\", shape=(100,))\n",
        "  likelihood = pm.Normal(\"likelihood\", mu, sd, observed=X, total_size=data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJTzxnevYNs0"
      },
      "source": [
        "with model:\n",
        "  advifit = pm.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0qKyLQ3YNqF"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(advifit.hist);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWYG-NDiYNnb"
      },
      "source": [
        "print(pm.Minibatch.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hMqle72YNhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib5XJQvYREB9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZCO1rrXyvOL"
      },
      "source": [
        "### <font color=blue>**2.** </font> PyMC3 でベイズ統計モデリング\n",
        "\n",
        "https://qiita.com/0NE_shoT_/items/2b41ae3e8e8f2d8809c4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH55JUlTyvAj"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUma-ZLdy7-e"
      },
      "source": [
        "## データの生成\n",
        "\n",
        "def generate_sample_data(num, seed=0):\n",
        "  target_list = [] # 目的変数のリスト\n",
        "  feature_vector_list = [] # 説明変数（特徴量）のリスト\n",
        "\n",
        "  feature_num = 8 # 特徴量の数\n",
        "  intercept = 0.2 # 切片\n",
        "  weight = [0.2, 0.3, 0.5, -0.4, 0.1, 0.2, 0.5, -0.3] # 各特徴量の重み\n",
        "\n",
        "  np.random.seed(seed=seed)\n",
        "  for i in range(num):\n",
        "    feature_vector = [np.random.rand() for n in range(feature_num)] # 特徴量をランダムに生成\n",
        "    noise = [np.random.normal(0, 0.1) for n in range(feature_num)] # ノイズをランダムに生成\n",
        "    target = sum([intercept+feature_vector[n]*weight[n]+noise[n] for n in range(feature_num)]) # 目的変数を生成\n",
        "\n",
        "    target_list.append(target)\n",
        "    feature_vector_list.append(feature_vector)\n",
        "\n",
        "  df = pd.DataFrame(np.c_[target_list, feature_vector_list],\n",
        "                    columns=['target', 'feature0', 'feature1', 'feature2',\n",
        "                             'feature3', 'feature4', 'feature5', 'feature6', 'feature7']\n",
        "                    )\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBG-icbKyzXo"
      },
      "source": [
        "data = generate_sample_data(num=1000, seed=0)\n",
        "\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "## パラメータの事後分布と検証用データ X_test に対する目的変数の予測分布を算出することを目標とする"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnouwQEUikup"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojvINp1zzym"
      },
      "source": [
        "# PyMC3 は、Python の文法の枠内で統計モデリングができるライブラリです\n",
        "# 行列操作や微分などの数式処理ができる Theano を内部で利用することで、確率分布の計算の高速化を図っています\n",
        "# Stan と同様に、 NUTS アルゴリズムによるサンプリングや ADVI による変分推論が可能です\n",
        "\n",
        "# Theano -> https://www.sejuku.net/blog/64336"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSrFwXMbygkg"
      },
      "source": [
        "!pip install Theano==1.0.5\n",
        "!pip install arviz\n",
        "!pip install --upgrade pymc3==3.11.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLwLI7_8-_My"
      },
      "source": [
        "import theano\n",
        "import arviz\n",
        "import pymc3 as pm\n",
        "\n",
        "#print(theano.__version__)\n",
        "#print(arviz.__version__)\n",
        "#print(pm.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f54RBXh70Egx"
      },
      "source": [
        "## モデル構築\n",
        "\n",
        "## 学習データを Theano の共有変数としている理由は、検証用データに対する予測分布を計算する際に、モデル（＝パラメータの事後分布）を再利用するため\n",
        "# 学習用データの説明変数\n",
        "X_shared = theano.shared(X_train.values)\n",
        "\n",
        "# 学習用データの目的変数\n",
        "y_shared = theano.shared(y_train.values)\n",
        "\n",
        "with pm.Model() as linear_model:\n",
        "  ## w0,w1,…,wD を(−∞,∞)の一様分布に設定\n",
        "  w0 = pm.Flat('w0')\n",
        "  w = pm.Flat('w', shape=X_shared.get_value().shape[1])\n",
        "  \n",
        "  ## σ は (0,∞)の一様分布に設定\n",
        "  sigma = pm.HalfFlat('sigma')\n",
        "\n",
        "  ## 目的変数の確率モデル式を記述\n",
        "  y_obs = pm.Normal('y_obs',\n",
        "                    mu=w0+pm.math.dot(X_shared,w), \n",
        "                    sigma=sigma, \n",
        "                    observed=y_shared, \n",
        "                    shape=y_shared.get_value().shape[0])\n",
        "  \n",
        "  ## サンプリングを実行\n",
        "  trace = pm.sample(\n",
        "      500,        ## iteration ステップ数を 1000\n",
        "      tune=500,   ## warm up ステップ数\n",
        "      cores=4     ## chain 数\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_wnWQJJzzr3"
      },
      "source": [
        "## 各パラメータについて得られたサンプル列の取得\n",
        "trace.get_values('w0', chains=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHdlSWR-zzqF"
      },
      "source": [
        "## 各パラメータの事後分布と trace plot を描画\n",
        "\n",
        "#pm.traceplot(trace)\n",
        "arviz.plot_trace(trace)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnu4DpjdzznW"
      },
      "source": [
        "## サンプリングアルゴリズムの収束の度合いを表す指標を確認\n",
        "\n",
        "#pm.gelman_rubin(trace)\n",
        "arviz.summary(trace)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdgUuoO0N8Q"
      },
      "source": [
        "## 各パラメータについて、各chain のベイズ信頼区間を表示\n",
        "\n",
        "#pm.forestplot(trace)\n",
        "arviz.plot_forest(trace)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eql80cU60RSw"
      },
      "source": [
        "X_shared.set_value(X_test)\n",
        "y_shared.set_value(np.zeros(X_test.shape[0],)) # 目的変数を初期化\n",
        "\n",
        "with linear_model:\n",
        "  ## 予測分布（=検証用データに対する目的変数のサンプル）\n",
        "  post_pred = pm.sample_posterior_predictive(trace, samples=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOfpNTCG0RP1"
      },
      "source": [
        "## サンプリング結果から求めた事後平均を予測値とすれば、機械学習による予測タスクと同様に精度検証することも可能\n",
        "y_pred = post_pred['y_obs'].mean(axis=0)\n",
        "print('MSE(test) = {:.2f}'.format(mean_squared_error(y_test, y_pred)))\n",
        "print('R^2(test) = {:.2f}'.format(r2_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4LIRqmrX4rF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI5uGkBEX4nA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWeIA_pH3FF4"
      },
      "source": [
        "### <font color=blue>**3.** </font> ギブスサンプリング（MCMC : Markov chain Monte Carlo methods）による画像のノイズ除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZDyRwGe5Ne6"
      },
      "source": [
        "## 出典: https://ichi.pro/gibusu-sanpuringu-mcmc-niyoru-gazo-no-noizu-jokyo-19602944876883"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g-RmNgbY2WJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPUY2v1zZYfU"
      },
      "source": [
        "img = cv2.imread(\"/content/img.png\", 0)\n",
        "img_noisy = cv2.imread(\"/content/img_noisy.png\", 0)\n",
        "\n",
        "plt.figure(figsize=(16,7))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img, cmap = 'gray')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_noisy, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG8o03Hu3Er5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t4AksKw-K4a"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qUL2iuD-K12"
      },
      "source": [
        "def load_image(filename):\n",
        "  ## PNG画像をnumpy配列に読み取り\n",
        "  my_img = plt.imread(filename)\n",
        "  \n",
        "  ## グレースケールに変換\n",
        "  img_gray = np.dot(my_img[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "  \n",
        "  ## ピクセルを{-1、1}に再スケーリング\n",
        "  img_gray = np.where(img_gray > 0.5, 1, -1)\n",
        "  \n",
        "  ## 各ピクセルの隣接ピクセルを検索するときにコーナーケースを処理できるように、エッジに0個のパディングを追加\n",
        "  img_padded = np.zeros([img_gray.shape[0] + 2, img_gray.shape[1] + 2])\n",
        "  img_padded[1:-1, 1:-1] = img_gray\n",
        "  return img_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNhYne9zoU12"
      },
      "source": [
        "$\\log{P(Y|X)} = \\log{P(X|Y)} + \\log{P(Y)} - \\log{P(X)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPjUpOQ3oUyz"
      },
      "source": [
        "$\\displaystyle P(Y, X) = \\dfrac{1}{Z} \\exp \\left( \\eta \\sum_{i=1}^{N} \\sum_{j=1}^{M}{x_{ij}y_{ij}} + \\beta \\sum_{i'j' \\in N(ij)}^{} {y_{ij}y_{i'j'}} \\right)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLKMBmNS-Ky5"
      },
      "source": [
        "def sample_y(i, j, Y, X):\n",
        "  ## 行と列のインデックス i と j\n",
        "  ## 復元された画像配列Y\n",
        "  ## ノイズの多い画像配列X\n",
        "\n",
        "  ## yij の近傍 yij_neighbors を検索し、条件付き確率P（yij = 1 | yij_neighbors）を計算\n",
        "  markov_blanket = [Y[i - 1, j], Y[i, j - 1], Y[i, j + 1], Y[i + 1, j], X[i, j]]\n",
        "  w = ETA * markov_blanket[-1] + BETA * sum(markov_blanket[:4])\n",
        "\n",
        "  ## 条件付き確率でサンプリングされた yij の値（1または-1）を返す\n",
        "  prob = 1 / (1 + math.exp(-2*w))\n",
        "  return (np.random.rand() < prob) * 2 - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLS6bxm6oUu6"
      },
      "source": [
        "$\\displaystyle P(y_{ij} = 1 | y_{N(ij)}, x_{i, j}) = \\cdots = \\dfrac{1}{1 + \\exp (-2w_{ij})}$\n",
        "\n",
        "$\\displaystyle w_{ij} = \\eta x_{ij} + \\beta \\sum_{N(ij)} y_{N(ij)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbiNTWKqHVCv"
      },
      "source": [
        "def get_posterior(filename, burn_in_steps, total_samples, logfile):\n",
        "  ## ノイズの多い画像Xをロード\n",
        "  X = load_image(filename)\n",
        "  \n",
        "  posterior = np.zeros(X.shape)\n",
        "  print(\"img shape: {}\".format(X.shape))\n",
        "  \n",
        "  ## 復元された画像Yをランダムに初期化\n",
        "  Y = np.random.choice([1, -1], size=X.shape)\n",
        "  energy_list = list()\n",
        "  \n",
        "  ## Yをサンプリングし、事後確率P（Y | Y_neighbor）を計算\n",
        "  for step in range(burn_in_steps + total_samples):\n",
        "    if step % 10 == 0:\n",
        "      print(\"{}th step start\".format(step+1))\n",
        "    for i in range(1, Y.shape[0]-1):\n",
        "      for j in range(1, Y.shape[1]-1):\n",
        "        ## Yの各ピクセルをサンプリング\n",
        "        y = sample_y(i, j, Y, X)\n",
        "\n",
        "        ## サンプリングされた値でYを更新\n",
        "        Y[i, j] = y\n",
        "\n",
        "        ## バーンイン期間が終了すると、Yのyijについて、yij = 1というイベントの発生総数を合計\n",
        "        if y == 1 and step >= burn_in_steps:\n",
        "          posterior[i, j] += 1\n",
        "    ## 収束を視覚化できるように、エネルギーを追跡\n",
        "    energy = -np.sum(np.multiply(Y, X))*ITA-(np.sum(np.multiply(Y[:-1], Y[1:]))+np.sum(np.multiply(Y[:, :-1], Y[:, 1:])))*BETA\n",
        "    if step < burn_in_steps:\n",
        "      energy_list.append(str(step) + \"\\t\" + str(energy) + \"\\tB\")\n",
        "    else:\n",
        "      energy_list.append(str(step) + \"\\t\" + str(energy) + \"\\tS\")\n",
        "  ## サンプリングが完了したら、モンテカルロ法を使用して事後確率を取得\n",
        "  ## 事後確率は、基本的にYの集計値を合計サンプル数で除算\n",
        "  posterior = posterior / total_samples\n",
        "\n",
        "  file = open(logfile, 'w')\n",
        "  for element in energy_list:\n",
        "    file.writelines(element)\n",
        "    file.write('\\n')\n",
        "  file.close()\n",
        "  return posterior"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcipQqmRHU_Z"
      },
      "source": [
        "## 入力関数\n",
        "\n",
        "def denoise_image(filename, burn_in_steps, total_samples, logfile):\n",
        "  ## 推定事後確率p（Y = 1 | Y_neighbor）を取得\n",
        "  posterior = get_posterior(filename, burn_in_steps, total_samples, logfile=logfile)\n",
        "  \n",
        "  denoised = np.zeros(posterior.shape, dtype=np.float64)\n",
        "  \n",
        "  ## しきい値を0.5に設定すると、復元された画像配列Yを後方から取得\n",
        "  denoised[posterior > 0.5] = 1\n",
        "  \n",
        "  ## 画像配列のエッジを取り除いて返す\n",
        "  return denoised[1:-1, 1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ov88rhHoha"
      },
      "source": [
        "def plot_energy(filename):\n",
        "  x = np.genfromtxt(filename, dtype=None, encoding='utf8')\n",
        "  its, energies, phases = zip(*x)\n",
        "  its = np.asarray(its)\n",
        "  energies = np.asarray(energies)\n",
        "  phases = np.asarray(phases)\n",
        "  burn_mask = (phases == 'B')\n",
        "  samp_mask = (phases == 'S')\n",
        "  assert np.sum(burn_mask) + np.sum(samp_mask) == len(x), 'Found bad phase'\n",
        "  its_burn, energies_burn = its[burn_mask], energies[burn_mask]\n",
        "  its_samp, energies_samp = its[samp_mask], energies[samp_mask]\n",
        "  p1, = plt.plot(its_burn, energies_burn, 'r')\n",
        "  p2, = plt.plot(its_samp, energies_samp, 'b')\n",
        "  plt.title(\"energy\")\n",
        "  plt.xlabel('iteration number')\n",
        "  plt.ylabel('energy')\n",
        "  plt.legend([p1, p2], ['burn in', 'sampling'])\n",
        "  plt.show()  ###\n",
        "\n",
        "  plt.savefig('%s.png' % filename[:-4])\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgXiy9BMHofK"
      },
      "source": [
        "def save_image(denoised_image):\n",
        "  plt.figure(figsize=(8,7))  ###\n",
        "  plt.imshow(denoised_image, cmap='gray')\n",
        "  plt.title(\"denoised image\")\n",
        "  plt.show()  ###\n",
        "\n",
        "  plt.savefig('/content/denoise_image.png') ###\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbWAcC4jHocn"
      },
      "source": [
        "## ハイパーパラメータ η と β\n",
        "ETA = 1\n",
        "BETA = 1\n",
        "\n",
        "## サンプリングステップ\n",
        "total_samples = 180  ###\n",
        "\n",
        "## 書き込みステップ\n",
        "burn_in_steps = 20 ###\n",
        "\n",
        "logfile = \"/content/log_energy.txt\" ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUGAFi127hN"
      },
      "source": [
        "time1 = time.time()\n",
        "denoised_img = denoise_image(\"/content/img_noisy.png\",  ###\n",
        "                             burn_in_steps = burn_in_steps,\n",
        "                             total_samples = total_samples, \n",
        "                             logfile = logfile\n",
        "                             )\n",
        "print(\"total time: {}\".format(time.time() - time1))\n",
        "save_image(denoised_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtPiDyaw27bt"
      },
      "source": [
        "# log = open(\"/content/log_energy.txt\")\n",
        "plot_energy(logfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYBM6J7W25qq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7M25m-x7R-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LES5tGOUB59C"
      },
      "source": [
        "### <font color=blue>**4.** </font> STS（structural time series） models with non-Gaussian observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DuRz17-66dh"
      },
      "source": [
        "## https://www.tensorflow.org/probability/examples/STS_approximate_inference_for_models_with_non_Gaussian_observations\n",
        "\n",
        "## Copyright 2019 The TensorFlow Probability Authors.\n",
        "## Licensed under the Apache License, Version 2.0 (the \"License\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wAp_7USzdPt"
      },
      "source": [
        "## Approximate inference for STS(structural time series) models with non-Gaussian observations\n",
        "\n",
        "# This notebook demonstrates the use of TFP approximate inference tools to\n",
        "# incorporate a (non-Gaussian) observation model when fitting and forecasting\n",
        "# with structural time series (STS) models.\n",
        "# In this example, we'll use a Poisson observation model to work with discrete count data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YJz-JDu0X9E"
      },
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow_probability import bijectors as tfb\n",
        "from tensorflow_probability import distributions as tfd\n",
        "\n",
        "tf.enable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iEVDFdrzyCJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKgRbodJ4EuU"
      },
      "source": [
        "## Synthetic Data\n",
        "# First we'll generate some synthetic count data:\n",
        "\n",
        "num_timesteps = 30\n",
        "observed_counts = np.round(3 + np.random.lognormal(np.log(np.linspace(\n",
        "    num_timesteps, 5, num=num_timesteps)), 0.20, size=num_timesteps)) \n",
        "observed_counts = observed_counts.astype(np.float32)\n",
        "plt.plot(observed_counts)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1KnOnfbzw2E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSsekKzIwsg6"
      },
      "source": [
        "## Model\n",
        "# We'll specify a simple model with a randomly walking linear trend:\n",
        "\n",
        "def build_model(approximate_unconstrained_rates):\n",
        "  trend = tfp.sts.LocalLinearTrend(\n",
        "      observed_time_series=approximate_unconstrained_rates)\n",
        "  return tfp.sts.Sum([trend],\n",
        "                     observed_time_series=approximate_unconstrained_rates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY-pH3hQz0Vp"
      },
      "source": [
        "Instead of operating on the observed time series, this model will operate on the series of Poisson rate parameters that govern the observations.\n",
        "\n",
        "Since Poisson rates must be positive, we'll use a bijector to transform the\n",
        "real-valued STS model into a distribution over positive values. The `Softplus`\n",
        "transformation $y = \\log(1 + \\exp(x))$ is a natural choice, since it is nearly linear for positive values, but other choices such as `Exp` (which transforms the normal random walk into a lognormal random walk) are also possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgdd9-Yo7Ly0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg_B4tofzxgc"
      },
      "source": [
        "positive_bijector = tfb.Softplus()  # Or tfb.Exp()\n",
        "\n",
        "# Approximate the unconstrained Poisson rate just to set heuristic priors.\n",
        "# We could avoid this by passing explicit priors on all model params.\n",
        "approximate_unconstrained_rates = positive_bijector.inverse(\n",
        "    tf.convert_to_tensor(observed_counts) + 0.01)\n",
        "sts_model = build_model(approximate_unconstrained_rates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxua5B2wxIMz"
      },
      "source": [
        "To use approximate inference for a non-Gaussian observation model,\n",
        "we'll encode the STS model as a TFP JointDistribution. The random variables in this joint distribution are the parameters of the STS model, the time series of latent Poisson rates, and the observed counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otw67KQq7MMK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vquh2LxgBjfy"
      },
      "source": [
        "Root = tfd.JointDistributionCoroutine.Root\n",
        "def sts_with_poisson_likelihood_model():\n",
        "  # Encode the parameters of the STS model as random variables.\n",
        "  param_vals = []\n",
        "  for param in sts_model.parameters:\n",
        "    param_val = yield Root(param.prior)\n",
        "    param_vals.append(param_val)\n",
        "\n",
        "  # Use the STS model to encode the log- (or inverse-softplus)\n",
        "  # rate of a Poisson.\n",
        "  unconstrained_rate = yield sts_model.make_state_space_model(\n",
        "      num_timesteps, param_vals)\n",
        "  rate = positive_bijector.forward(unconstrained_rate[..., 0])\n",
        "  observed_counts = yield tfd.Independent(tfd.Poisson(rate),\n",
        "      reinterpreted_batch_ndims=1)\n",
        "model = tfd.JointDistributionCoroutine(sts_with_poisson_likelihood_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhJmGgrrzzLc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSj7blvWh1w8"
      },
      "source": [
        "## Preparation for inference\n",
        "# We want to infer the unobserved quantities in the model, given the observed counts.\n",
        "# First, we condition the joint log density on the observed counts.\n",
        "\n",
        "# Condition a joint log-prob on the observed counts.\n",
        "target_log_prob_fn = lambda *args: model.log_prob(args + (observed_counts,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFeZ7NYt1qnw"
      },
      "source": [
        "HMC and VI inference also like to operate over unconstrained real-valued spaces, so we'll construct the list of bijectors that constrains each of the parameters to their respective supports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyhb06i41qIg"
      },
      "source": [
        "constraining_bijectors = ([param.bijector for param in sts_model.parameters] +\n",
        "                           # `unconstrained_rate` is already unconstrained, but\n",
        "                           # we can speed up inference by rescaling it.\n",
        "                           [tfb.Scale(positive_bijector.inverse(\n",
        "                               np.float32(np.max(observed_counts / 5.))))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3BnLMtUz5Uj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kXS15HQz91C"
      },
      "source": [
        "'''Inference with HMC\n",
        "\n",
        "We'll use HMC (specifically, NUTS) to sample from the joint posterior over model parameters and latent rates.\n",
        "This will be significantly slower than fitting a standard STS model with HMC, since in addition to the model's\n",
        " (relatively small number of) parameters we also have to infer the entire series of Poisson rates. \n",
        " So we'll run for a relatively small number of steps; for applications where inference quality is critical \n",
        " it might make sense to increase these values or to run multiple chains.\n",
        " '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMPlVBk6PcpT"
      },
      "source": [
        "# Sampler configuration\n",
        "\n",
        "# Allow external control of sampling to reduce test runtimes.\n",
        "num_results = 100\n",
        "num_results = int(num_results)\n",
        "\n",
        "num_burnin_steps = 50\n",
        "num_burnin_steps = int(num_burnin_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DLxik_27QJL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhSe-GFDPg9o"
      },
      "source": [
        "First we specify a sampler, and then use `sample_chain` to run that sampling\n",
        "kernel to produce samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ue-mBGdcmh"
      },
      "source": [
        "sampler = tfp.mcmc.TransformedTransitionKernel(\n",
        "    tfp.mcmc.NoUTurnSampler(\n",
        "        target_log_prob_fn=target_log_prob_fn,\n",
        "        step_size=0.1),\n",
        "    bijector=constraining_bijectors)\n",
        "\n",
        "adaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
        "    inner_kernel=sampler,\n",
        "    num_adaptation_steps=int(0.8 * num_burnin_steps),\n",
        "    target_accept_prob=0.75,\n",
        "    # NUTS inside of a TTK requires custom getter/setter functions.\n",
        "    step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n",
        "        inner_results=pkr.inner_results._replace(step_size=new_step_size)\n",
        "        ),\n",
        "    step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n",
        "    log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio,\n",
        ")\n",
        "\n",
        "initial_state = [b.forward(tf.random.normal(part_shape))\n",
        "                 for (b, part_shape) in zip(\n",
        "                     constraining_bijectors, model.event_shape[:-1])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvriVTPlih3B"
      },
      "source": [
        "# Speed up sampling by tracing with `tf.function`.\n",
        "@tf.function(autograph=False, experimental_compile=True)\n",
        "def do_sampling():\n",
        "  return tfp.mcmc.sample_chain(\n",
        "      kernel=adaptive_sampler,\n",
        "      current_state=initial_state,\n",
        "      num_results=num_results,\n",
        "      num_burnin_steps=num_burnin_steps)\n",
        "\n",
        "t0 = time.time()\n",
        "samples, kernel_results = do_sampling()\n",
        "t1 = time.time()\n",
        "print(\"Inference ran in {:.2f}s.\".format(t1-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG7o9JV_7RqC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwE0yWm_2_kE"
      },
      "source": [
        "We can sanity-check the inference by examining the parameter traces. In this case they appear to have explored multiple explanations for the data, which is good, although more samples would be helpful to judge how well the chain is mixing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPOVTbboAtGr"
      },
      "source": [
        "f = plt.figure(figsize=(15, 5))\n",
        "for i, param in enumerate(sts_model.parameters):\n",
        "  ax = f.add_subplot(1, len(sts_model.parameters), i + 1)\n",
        "  ax.plot(samples[i])\n",
        "  ax.set_title(\"{} samples\".format(param.name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqwrA_P27Smz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZOydxU53oE9"
      },
      "source": [
        "Now for the payoff: let's see the posterior over Poisson rates! We'll also plot the 80% predictive interval over observed counts, and can check that this interval appears to contain about 80% of the counts we actually observed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56rIH8MCeU9F"
      },
      "source": [
        "param_samples = samples[:-1]\n",
        "unconstrained_rate_samples = samples[-1][..., 0]\n",
        "rate_samples = positive_bijector.forward(unconstrained_rate_samples)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "mean_lower, mean_upper = np.percentile(rate_samples, [10, 90], axis=0)\n",
        "pred_lower, pred_upper = np.percentile(np.random.poisson(rate_samples), \n",
        "                                       [10, 90], axis=0)\n",
        "\n",
        "_ = plt.plot(observed_counts, color=\"blue\", ls='--', marker='o', label='observed', alpha=0.7)\n",
        "_ = plt.plot(np.mean(rate_samples, axis=0), label='rate', color=\"green\", ls='dashed', lw=2, alpha=0.7)\n",
        "_ = plt.fill_between(np.arange(0, 30), mean_lower, mean_upper, color='green', alpha=0.2)\n",
        "_ = plt.fill_between(np.arange(0, 30), pred_lower, pred_upper, color='grey', label='counts', alpha=0.2)\n",
        "plt.xlabel(\"Day\")\n",
        "plt.ylabel(\"Daily Sample Size\")\n",
        "plt.title(\"Posterior Mean\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrDWkw5u0VmC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1HuVuk6Qocm"
      },
      "source": [
        "## Forecasting\n",
        "# To forecast the observed counts, we'll use the standard STS tools to build a forecast distribution\n",
        "# over the latent rates (in unconstrained space, again since STS is designed to model real-valued data),\n",
        "# then pass the sampled forecasts through a Poisson observation model:\n",
        "\n",
        "def sample_forecasted_counts(sts_model, posterior_latent_rates,\n",
        "                             posterior_params, num_steps_forecast,\n",
        "                             num_sampled_forecasts):\n",
        "\n",
        "  # Forecast the future latent unconstrained rates, given the inferred latent\n",
        "  # unconstrained rates and parameters.\n",
        "  unconstrained_rates_forecast_dist = tfp.sts.forecast(sts_model,\n",
        "    observed_time_series=unconstrained_rate_samples,\n",
        "    parameter_samples=posterior_params,\n",
        "    num_steps_forecast=num_steps_forecast)\n",
        "\n",
        "  # Transform the forecast to positive-valued Poisson rates.\n",
        "  rates_forecast_dist = tfd.TransformedDistribution(\n",
        "      unconstrained_rates_forecast_dist,\n",
        "      positive_bijector)\n",
        "\n",
        "  # Sample from the forecast model following the chain rule:\n",
        "  # P(counts) = P(counts | latent_rates)P(latent_rates)\n",
        "  sampled_latent_rates = rates_forecast_dist.sample(num_sampled_forecasts)\n",
        "  sampled_forecast_counts = tfd.Poisson(rate=sampled_latent_rates).sample()\n",
        "\n",
        "  return sampled_forecast_counts, sampled_latent_rates\n",
        "\n",
        "forecast_samples, rate_samples = sample_forecasted_counts(\n",
        "   sts_model,\n",
        "   posterior_latent_rates=unconstrained_rate_samples,\n",
        "   posterior_params=param_samples,\n",
        "   # Days to forecast:\n",
        "   num_steps_forecast=30,\n",
        "   num_sampled_forecasts=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyPFQzV8SOSs"
      },
      "source": [
        "forecast_samples = np.squeeze(forecast_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD_kLwF1V3m-"
      },
      "source": [
        "def plot_forecast_helper(data, forecast_samples, CI=90):\n",
        "  \"\"\"Plot the observed time series alongside the forecast.\"\"\"\n",
        "  plt.figure(figsize=(10, 4))\n",
        "  forecast_median = np.median(forecast_samples, axis=0)\n",
        "\n",
        "  num_steps = len(data)\n",
        "  num_steps_forecast = forecast_median.shape[-1]\n",
        "\n",
        "  plt.plot(np.arange(num_steps), data, lw=2, color='blue', linestyle='--', marker='o',\n",
        "           label='Observed Data', alpha=0.7)\n",
        "\n",
        "  forecast_steps = np.arange(num_steps, num_steps+num_steps_forecast)\n",
        "\n",
        "  CI_interval = [(100 - CI)/2, 100 - (100 - CI)/2]\n",
        "  lower, upper = np.percentile(forecast_samples, CI_interval, axis=0)\n",
        "\n",
        "  plt.plot(forecast_steps, forecast_median, lw=2, ls='--', marker='o', color='orange',\n",
        "           label=str(CI) + '% Forecast Interval', alpha=0.7)\n",
        "  plt.fill_between(forecast_steps,\n",
        "                   lower,\n",
        "                   upper, color='orange', alpha=0.2)\n",
        "\n",
        "  plt.xlim([0, num_steps+num_steps_forecast])\n",
        "  ymin, ymax = min(np.min(forecast_samples), np.min(data)), max(np.max(forecast_samples), np.max(data))\n",
        "  yrange = ymax-ymin\n",
        "  plt.title(\"{}\".format('Observed time series with ' + str(num_steps_forecast) + ' Day Forecast'))\n",
        "  plt.xlabel('Day')\n",
        "  plt.ylabel('Daily Sample Size')\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyUp4NnzWOcs"
      },
      "source": [
        "plot_forecast_helper(observed_counts, forecast_samples, CI=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhOh0vqi0Xtk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHyHLaPb7VTw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTGRrDfO0Z2h"
      },
      "source": [
        "'''VI inference\n",
        "\n",
        "Variational inference can be problematic when inferring a full time series, like our approximate counts \n",
        "(as opposed to just the *parameters* of a time series, as in standard STS models). \n",
        "The standard assumption that variables have independent posteriors is quite wrong, \n",
        "since each timestep is correlated with its neighbors, which can lead to underestimating uncertainty. \n",
        "For this reason, HMC may be a better choice for approximate inference over full time series. \n",
        "However, VI can be quite a bit faster, and may be useful for model prototyping or in cases \n",
        "where its performance can be empirically shown to be 'good enough'.\n",
        "\n",
        "To fit our model with VI, we simply build and optimize a surrogate posterior:\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aZQEnTThgMT"
      },
      "source": [
        "surrogate_posterior = tfp.experimental.vi.build_factored_surrogate_posterior(\n",
        "    event_shape=model.event_shape[:-1],  # Infer everything but the observed counts.\n",
        "    constraining_bijectors=constraining_bijectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65cf0_EiimGq"
      },
      "source": [
        "# Allow external control of optimization to reduce test runtimes.\n",
        "num_variational_steps = 200\n",
        "num_variational_steps = int(num_variational_steps)\n",
        "\n",
        "t0 = time.time()\n",
        "losses = tfp.vi.fit_surrogate_posterior(target_log_prob_fn,\n",
        "                                        surrogate_posterior,\n",
        "                                        optimizer=tf.optimizers.Adam(0.1),\n",
        "                                        num_steps=num_variational_steps)\n",
        "t1 = time.time()\n",
        "print(\"Inference ran in {:.2f}s.\".format(t1-t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX8WtcLmk2mj"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.title(\"Variational loss\")\n",
        "_ = plt.xlabel(\"Steps\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQoUExeBkpC0"
      },
      "source": [
        "posterior_samples = surrogate_posterior.sample(50)\n",
        "param_samples = posterior_samples[:-1]\n",
        "unconstrained_rate_samples = posterior_samples[-1][..., 0]\n",
        "rate_samples = positive_bijector.forward(unconstrained_rate_samples)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "mean_lower, mean_upper = np.percentile(rate_samples, [10, 90], axis=0)\n",
        "pred_lower, pred_upper = np.percentile(\n",
        "    np.random.poisson(rate_samples), [10, 90], axis=0)\n",
        "\n",
        "_ = plt.plot(observed_counts, color='blue', ls='--', marker='o',\n",
        "             label='observed', alpha=0.7)\n",
        "_ = plt.plot(np.mean(rate_samples, axis=0), label='rate', color='green',\n",
        "             ls='dashed', lw=2, alpha=0.7)\n",
        "_ = plt.fill_between(\n",
        "    np.arange(0, 30), mean_lower, mean_upper, color='green', alpha=0.2)\n",
        "_ = plt.fill_between(np.arange(0, 30), pred_lower, pred_upper, color='grey',\n",
        "    label='counts', alpha=0.2)\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Daily Sample Size')\n",
        "plt.title('Posterior Mean')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aoMoQyf_fWC"
      },
      "source": [
        "forecast_samples, rate_samples = sample_forecasted_counts(\n",
        "   sts_model,\n",
        "   posterior_latent_rates=unconstrained_rate_samples,\n",
        "   posterior_params=param_samples,\n",
        "   # Days to forecast:\n",
        "   num_steps_forecast=30,\n",
        "   num_sampled_forecasts=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ7zJpEr_hHU"
      },
      "source": [
        "forecast_samples = np.squeeze(forecast_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcEpkAEi_jcn"
      },
      "source": [
        "plot_forecast_helper(observed_counts, forecast_samples, CI=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hoTLMXny8Pw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU34HQf7y8Lx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_c_hU-Ay8F6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}