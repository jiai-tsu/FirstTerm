{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_20210518.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6JJPOvHVP7yM",
        "uH4hgnXSP8Ri",
        "rurPKbrnftn8",
        "8HJTUdEYkGI5",
        "ZerXxoy4kgvS",
        "Sq30G5wJkwTD",
        "vEdSWt_0k_en",
        "ccsJjfeZlAkU",
        "LCS66hdSlCXx",
        "sGzKcfSOlDpJ",
        "f6yEcDeZlEdu",
        "tSqaYFovlFje",
        "QvB-YbugoRkZ",
        "wmYRiE3APK8K",
        "ZDNBgbWEPLQb",
        "xSGiUjG6PNZi",
        "Phh_A5OLPOWM",
        "UwN20kefPOv9",
        "8cA3MktRT7Vr",
        "jBa2R-_BT7tj",
        "mIPhJuyzXeQs"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnijj-lZg_s"
      },
      "source": [
        "## 47. 時系列分析（Time Series Analysis）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ERMWOCbL5n"
      },
      "source": [
        "### <font color=blue>**1.** </font> 古典的方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VrrjnRdhdn0"
      },
      "source": [
        "'''自己回帰モデル\n",
        "（AR : AutoRegressive integrated moving model）\n",
        "\n",
        "移動平均モデル\n",
        "（MA : Moving Average）\n",
        "\n",
        "自己回帰移動平均モデル\n",
        "（ARMA : Auto Regressive Moving Average）\n",
        "\n",
        "自己回帰和分移動平均モデル\n",
        "（ARIMA : Auto Regressive Integrated Moving Average）\n",
        "\n",
        "季節的自己回帰和分移動平均モデル\n",
        "（SARIMA : Seasonal AutoRegressive Integrated Moving Average）\n",
        "\n",
        "外部変数付き季節的自己回帰和分移動平均モデル\n",
        "（SARIMAX : Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors）\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjgzVIrrhddd"
      },
      "source": [
        "# 元データ : https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/AirPassengers.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sMXpKqkWgSq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JJPOvHVP7yM"
      },
      "source": [
        "#### <font color=green>**1.1.** </font> サンプルコード　その１"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6iPH4pHP7yM"
      },
      "source": [
        "## 出典 : https://logics-of-blue.com/python-time-series-analysis/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2JnE7U3hRSg"
      },
      "source": [
        "# 基本のライブラリを読み込む\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# グラフ描画\n",
        "from matplotlib import pylab as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# グラフを横長にする\n",
        "#from matplotlib.pylab import rcParams\n",
        "#rcParams['figure.figsize'] = 15, 6\n",
        "\n",
        "# 統計モデル\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ワーニングを表示させない\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZOJQMUPhRNl"
      },
      "source": [
        "## 時系列データの読み込み\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/jiai-tus/FirstTerm/main/20210518/datasets/AirPassengers.csv', \n",
        "                   index_col='Month', \n",
        "                   parse_dates=True, \n",
        "                   dtype='float')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb_CQ_NnhRCE"
      },
      "source": [
        "# 日付形式にする\n",
        "ts = data['#Passengers'] \n",
        "ts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgV0Ojnyi1sk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UHwWIp4hRAC"
      },
      "source": [
        "## 時系列データの取り扱い\n",
        "# プロット\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(ts)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsXjnsqKhQ9t"
      },
      "source": [
        "# データの取得方法その1\n",
        "ts['1949-01-01']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ciw2DwelxlW"
      },
      "source": [
        "# データの取得方法その2\n",
        "from datetime import datetime\n",
        "ts[datetime(1949,1, 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc44WiJMlxie"
      },
      "source": [
        "# 1949年のデータをすべて取ってくる\n",
        "ts['1949']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c3APQu8lxao"
      },
      "source": [
        "# 差分系列\n",
        "ts.diff().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64ttfegnlxgW"
      },
      "source": [
        "# シフト\n",
        "ts.shift().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmQuKWrdlxWc"
      },
      "source": [
        "# 対数差分系列\n",
        "logDiff = np.log(ts) - np.log(ts.shift())\n",
        "\n",
        "# NaNを取り除てから表示\n",
        "logDiff.dropna().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgPBJtDfi0cT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2moZkSqlxUT"
      },
      "source": [
        "## 自己相関係数の推定\n",
        "# 自己相関を求める\n",
        "# 過去の値とどれくらい似ているか\n",
        "ts_acf = sm.tsa.stattools.acf(ts, nlags=40)\n",
        "ts_acf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8CzOwnXlxRj"
      },
      "source": [
        "# 偏自己相関\n",
        "# 注目している時以外の要因を無視して計算された自己相関係数\n",
        "ts_pacf = sm.tsa.stattools.pacf(ts, nlags=40, method='ols')\n",
        "ts_pacf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0eGNIwUmiYV"
      },
      "source": [
        "# 自己相関のグラフ\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(ts, lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(ts, lags=40, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFcCDHXPi5cO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E6SNypZmiWE"
      },
      "source": [
        "## ARIMAモデルの推定\n",
        "# 和分過程なので、差分をとる\n",
        "diff = ts.diff()\n",
        "diff = diff.dropna()\n",
        "diff.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZGnj2ySmiTx"
      },
      "source": [
        "# 差分系列のグラフ\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(diff)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSAU4h_rmiRO"
      },
      "source": [
        "# 差分系列への自動ARMA推定関数の実行\n",
        "resDiff = sm.tsa.arma_order_select_ic(diff, ic='aic', trend='nc')\n",
        "resDiff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1DssIjZmiOz"
      },
      "source": [
        "# P=3, q=2 が最善となったので、それをモデル化\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "ARIMA_3_1_2 = ARIMA(ts, order=(3, 1, 2)).fit(dist=False)\n",
        "ARIMA_3_1_2.params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jdRVvAMhQ7w"
      },
      "source": [
        "# 残差のチェック\n",
        "# SARIMAじゃないので、周期性が残ってしまっている。。。\n",
        "resid = ARIMA_3_1_2.resid\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCDZCRxkjZpG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv6R8znXhQ5K"
      },
      "source": [
        "## SARIMAモデルの推定\n",
        "# SARIMAモデルを「決め打ち」で推定する\n",
        "\n",
        "SARIMA_3_1_2_111 = sm.tsa.SARIMAX(ts, order=(3,1,2), seasonal_order=(1,1,1,12)).fit(method='bfgs', maxiter=300)\n",
        "print(SARIMA_3_1_2_111.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7d58i13nKel"
      },
      "source": [
        "# 残差のチェック\n",
        "residSARIMA = SARIMA_3_1_2_111.resid\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(residSARIMA.values.squeeze(), lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(residSARIMA, lags=40, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ-HyeednKcg"
      },
      "source": [
        "# 予測\n",
        "pred = SARIMA_3_1_2_111.predict('1960-01-01', '1961-12-01')\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iKYAhGLnKad"
      },
      "source": [
        "# 実データと予測結果の図示\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(ts)\n",
        "plt.plot(pred, \"r\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nwiCqe8nKVc"
      },
      "source": [
        "## 総当たり法によるSARIMAモデル次数の決定\n",
        "'''総当たりで、AICが最小となるSARIMAの次数を探す\n",
        "ARIMA(p, d, q)、季節(sp, sd, sq)\n",
        "各々、pは自己回帰モデルの次数：AR(p)、\n",
        "     qは移動平均モデルの次数：MA(q)、\n",
        "     dは差分をとる回数：I(d)\n",
        "'''\n",
        "max_p = 3\n",
        "max_q = 3\n",
        "max_d = 1\n",
        "max_sp = 1\n",
        "max_sq = 1\n",
        "max_sd = 1\n",
        "\n",
        "## 上記設定で3分くらい。増やすとアホみたいに時間かかる\n",
        "\n",
        "pattern = max_p*(max_q + 1)*(max_d + 1)*(max_sp + 1)*(max_sq + 1)*(max_sd + 1)\n",
        "\n",
        "modelSelection = pd.DataFrame(index=range(pattern), columns=[\"model\", \"aic\"])\n",
        "pattern"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm5pvr5AhE5P"
      },
      "source": [
        "# 自動SARIMA選択\n",
        "\n",
        "num = 0\n",
        "\n",
        "for p in range(1, max_p + 1):\n",
        "    for d in range(0, max_d + 1):\n",
        "        for q in range(0, max_q + 1):\n",
        "            for sp in range(0, max_sp + 1):\n",
        "                for sd in range(0, max_sd + 1):\n",
        "                    for sq in range(0, max_sq + 1):\n",
        "                        sarima = sm.tsa.SARIMAX(\n",
        "                            ts, order=(p,d,q), \n",
        "                            seasonal_order=(sp,sd,sq,12), \n",
        "                            enforce_stationarity = False, \n",
        "                            enforce_invertibility = False\n",
        "                        ).fit(method='bfgs', maxiter=300, disp=False)\n",
        "                        modelSelection.iloc[num][\"model\"] = \"order=(\" + str(p) + \",\"+ str(d) + \",\"+ str(q) + \"), season=(\"+ str(sp) + \",\"+ str(sd) + \",\" + str(sq) + \")\"\n",
        "                        modelSelection.iloc[num][\"aic\"] = sarima.aic\n",
        "                        num = num + 1\n",
        "# .ix -> .iloc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZWlWvTPhE3E"
      },
      "source": [
        "modelSelection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY-qTBpxhCB8"
      },
      "source": [
        "# AIC最小モデル\n",
        "modelSelection[modelSelection.aic == min(modelSelection.aic)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovhvcqLtoGnr"
      },
      "source": [
        "# 参考：次数がすべて０だとエラーになる \n",
        "# sarima = sm.tsa.SARIMAX(ts, order=(0,0,0), seasonal_order=(0,0,0,12), enforce_stationarity = False).fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1E9JKGyoGlT"
      },
      "source": [
        "bestSARIMA = sm.tsa.SARIMAX(ts, order=(3,1,3), seasonal_order=(0,1,1,12), enforce_stationarity = False, enforce_invertibility = False).fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufljbUvAoGim"
      },
      "source": [
        "print(bestSARIMA.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALi8W6P7oMPz"
      },
      "source": [
        "# 残差のチェック\n",
        "residSARIMA = bestSARIMA.resid\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(residSARIMA, lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(residSARIMA, lags=40, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ7csMxfoN5r"
      },
      "source": [
        "# 予測\n",
        "bestPred = bestSARIMA.predict('1960-01-01', '1961-12-01')\n",
        "\n",
        "# 実データと予測結果の図示\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(ts)\n",
        "plt.plot(bestPred, \"r\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKr9lZXFJ6CT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrxKSM23P7yM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKrJGRoynpZa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH4hgnXSP8Ri"
      },
      "source": [
        "#### <font color=green>**1.2.** </font> サンプルコード　その２"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DIcF2cyP8Rj"
      },
      "source": [
        "## 出典 : https://qiita.com/DS27/items/1e998a58488e76bfcbdc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMJrNPGEkxQK"
      },
      "source": [
        "# 必要なライブラリーのインポート\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 統計モデル\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from matplotlib import pylab as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LnYMk4-xGhE"
      },
      "source": [
        "# データの読み込み\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/jiai-tus/FirstTerm/main/20210518/datasets/AirPassengers.csv')\n",
        "\n",
        "# float型に変換\n",
        "df['#Passengers'] = df['#Passengers'].astype('float64')\n",
        "df = df.rename(columns={'#Passengers': 'Passengers'})\n",
        "\n",
        "# datetime型にしてインデックスにする\n",
        "df.Month = pd.to_datetime(df.Month)\n",
        "df = df.set_index(\"Month\")\n",
        "\n",
        "# データの中身を確認\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJMkJmfyxGez"
      },
      "source": [
        "# データの可視化\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilkpd4k6xGXf"
      },
      "source": [
        "## 移動平均モデル（MA：Moving Average model）\n",
        "# 移動平均\n",
        "df[\"3ma\"]=df[\"Passengers\"].rolling(3).mean().round(1)\n",
        "df[\"5ma\"]=df[\"Passengers\"].rolling(5).mean().round(1)\n",
        "df[\"7ma\"]=df[\"Passengers\"].rolling(7).mean().round(1)\n",
        "\n",
        "# 可視化\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df[\"Passengers\"], label=\"Passengers\")\n",
        "plt.plot(df[\"3ma\"], \"k--\", label=\"SMA(3)\")\n",
        "plt.plot(df[\"5ma\"], \"r--\", label=\"SMA(5)\")\n",
        "plt.plot(df[\"7ma\"], \"g--\", label=\"SMA(7)\")\n",
        "plt.xlabel(\"date\")\n",
        "plt.ylabel(\"quantity\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II5VXi2AOggL"
      },
      "source": [
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 15, 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjJ_XgOQxGFJ"
      },
      "source": [
        "## 自己回帰モデル（AR：Autoregressive Integrated Moving model）\n",
        "\n",
        "# 自己相関を求める\n",
        "df_acf = sm.tsa.stattools.acf(df[\"Passengers\"], nlags=30, fft=False)\n",
        "\n",
        "# 自己相関のグラフ\n",
        "fig = sm.graphics.tsa.plot_acf(df[\"Passengers\"], lags=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqn7lm7szY_K"
      },
      "source": [
        "# 薄青の空間は、真に自己相関がない場合の信頼区間95%の範囲を示します\n",
        "# つまり、この範囲外の値を持つlag地点に（統計的に）有意な自己相関があると分かります"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjKE3XzilCi1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_7bKUEmzY4s"
      },
      "source": [
        "# 偏自己相関を求める\n",
        "df_pacf = sm.tsa.stattools.pacf(df[\"Passengers\"], nlags=20, method='ols')\n",
        "\n",
        "# 偏自己相関を可視化する\n",
        "fig = sm.graphics.tsa.plot_pacf(df[\"Passengers\"], lags=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ARpdguCDr7p"
      },
      "source": [
        "res = sm.tsa.seasonal_decompose(df[\"Passengers\"])\n",
        "fig = res.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDvv6KjIOpmW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19oZmzvbD08L"
      },
      "source": [
        "def invert(data, diff, prepro):\n",
        "  \"\"\"前処理に応じた逆変換をする\n",
        "  Parameters\n",
        "  ----------\n",
        "  data (np.array or np.float) : 原系列のデータ\n",
        "  diff (np.array or np.float) : 変換したデータ\n",
        "  prepro (str) : 前処理の手法（'diff', 'pct', 'logdiff')\n",
        "\n",
        "  \"\"\"\n",
        "  if prepro == 'diff':\n",
        "    return data + diff\n",
        "  elif prepro == 'pct':\n",
        "    return data * diff + data\n",
        "  elif prepro == 'logdiff':\n",
        "    return np.exp(diff) * data\n",
        "  else:\n",
        "    print('{}は対応していない前処理です'.format(prepro))\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0A61DPyD04_"
      },
      "source": [
        "def invert_predict(data, pred, start, prepro, period, span=None):\n",
        "  \"\"\"前処理した系列の予測値を原系列に逆変換する\n",
        "  Parameters\n",
        "  ----------\n",
        "  data (np.array) : 実測値\n",
        "  pred (np.array) : 予測値\n",
        "  start (int) : 逆変換して得る最初の期\n",
        "  prepro (str) : 前処理手法（'diff', 'pct', 'logdiff')\n",
        "  period (int) : 何期先まで予測したか\n",
        "  xlim (turple) : 原系列グラフのx軸の描画範囲\n",
        "  ylim (turple) : 原系列グラフのy軸の描画範囲\n",
        "  \"\"\"\n",
        "  pred_inverted = np.empty_like(pred)\n",
        "  if span == None:\n",
        "    pred_inverted[0] = invert(data[start - 1], pred[0], prepro)\n",
        "    for i in range(1, period):\n",
        "      pred_inverted[i] = invert(pred_inverted[i - 1], pred[i], prepro)\n",
        "  else:\n",
        "    for i in range(span):\n",
        "      pred_inverted[i, 0] = invert(data[start - period + i], pred[i, 0], prepro)\n",
        "      for j in range(1, period):\n",
        "        pred_inverted[i, j] = invert(pred_inverted[i, j-1], pred[i, j], prepro)\n",
        "  return pred_inverted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rokxa5zyD02x"
      },
      "source": [
        "def plot_processed_series(data, pred, start, data_num, interval, prepro, axis):\n",
        "  \"\"\"前処理した系列の実測値及び予測値をグラフに書き出す\n",
        "  Parameters\n",
        "  ----------\n",
        "  data (np.array) : 実測値\n",
        "  pred (np.array) : 予測値\n",
        "  start (int) : プロットの最初の期\n",
        "  data_num (int) : 予測に使用したデータの数\n",
        "  interval (int) : プロットする区間の長さ\n",
        "  prepro (str) : 前処理手法（'diff', 'pct', 'logdiff')\n",
        "  axis (matplotlib.axes.Axes) : 書き出したいグラフのAxesオブジェクト\n",
        "  \"\"\"\n",
        "  if prepro == 'diff':\n",
        "    target = data.diff().values\n",
        "  elif prepro == 'pct':\n",
        "    target = data.pct_change().values\n",
        "  elif prepro == 'logdiff':\n",
        "    target = (np.log(data) - np.log(data.shift(1))).values\n",
        "  axis.plot(np.arange(start-data_num, start), target[start-data_num:start], marker='.')\n",
        "  axis.plot(np.arange(start, start+interval), target[start:start+interval], c='green', label='Actual', marker='.')\n",
        "  axis.plot(np.arange(start, start+interval), pred, c='r', label='Predict', marker='.')\n",
        "  axis.set_ylabel('diff', fontsize=17)\n",
        "  axis.legend()\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBDZF2p4EUvC"
      },
      "source": [
        "def plot_original_series(data, pred, start, interval, axis, xlim, ylim):\n",
        "  '''逆変換した系列の実測値および予測値をグラフに書き出す.\n",
        "  Parameters\n",
        "  ----------\n",
        "  data (np.array) : 実測値\n",
        "  pred (np.array) : 予測値\n",
        "  start (int) : プロットの最初の期\n",
        "  interval (int) : プロットする区間の長さ\n",
        "  axis (matplotlib.axes.Axes) : 書き出したいグラフのAxesオブジェクト\n",
        "  xlim (turple) : 原系列グラフのx軸の描画範囲\n",
        "  ylim (turple) : 原系列グラフのy軸の描画範囲\n",
        "  '''\n",
        "  axis.plot(np.arange(1, start+interval+1), data[:start+interval], c='green', label='Actual', marker='.')\n",
        "  axis.plot(np.arange(start+1, start+interval+1), pred, c='r', label='Predict', marker='.')\n",
        "  axis.set_xlim(xlim)\n",
        "  axis.set_ylim(ylim)\n",
        "  axis.set_ylabel('Passengers', fontsize=17)\n",
        "  axis.legend()\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkyrusKuDr25"
      },
      "source": [
        "def plot_predict(data, pred, start, data_num, prepro, xlim=None, ylim=None):\n",
        "  \"\"\"AR(MA)モデルで予測したデータからグラフを描画する\n",
        "  Parameters\n",
        "  ----------\n",
        "  data (np.array) : 実測値\n",
        "  pred (np.array) : 予測値\n",
        "  start (int) : プロットを始める期（ > data_num)\n",
        "  data_num (int) : データをいくつ利用したか\n",
        "  prepro (str) : データの前処理方法（'diff', 'pct', 'logdiff')\n",
        "  xlim (turple) : 原系列グラフのx軸の描画範囲\n",
        "  ylim (turple) : 原系列グラフのy軸の描画範囲\n",
        "  \"\"\"\n",
        "  period = len(pred) #何期先までの予測をしたか\n",
        "\n",
        "  # 逆変換して原系列の予測を用意\n",
        "  pred_inverted = invert_predict(data, pred, start, prepro, period)\n",
        "\n",
        "  # 処理した系列、原系列について実測値と予測値の比較\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "  plt.rcParams[\"font.size\"] = 12\n",
        "  plt.subplots_adjust(wspace = 0.3)\n",
        "  plot_processed_series(data, pred, start, data_num, period, prepro, axes[0])\n",
        "  plot_original_series(data, pred_inverted, start, period, axes[1], xlim, ylim)\n",
        "\n",
        "  plt.show()\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpRnDKf0zYzm"
      },
      "source": [
        "target = df[\"Passengers\"].diff().values\n",
        "\n",
        "start = 101\n",
        "period = 1\n",
        "data_num = 100\n",
        "\n",
        "pred = sm.tsa.AR(target[start-data_num:start]).fit(maxlag=12).predict(start=data_num, end=data_num+period-1)\n",
        "plot_predict(df[\"Passengers\"], pred, start, data_num, 'diff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suubp3jjxGC8"
      },
      "source": [
        "start = 101\n",
        "period = 1\n",
        "data_num = 100\n",
        "pred_seq = np.arange(start+period-1, len(df))\n",
        "pred_AR = np.empty((len(pred_seq), period), dtype=float)\n",
        "\n",
        "for i, j in enumerate(pred_seq):\n",
        "  pred_AR[i] = sm.tsa.AR(target[j-period+1-data_num:j-period+1]).fit(maxlag=12).predict(start=data_num, end=data_num+period-1)\n",
        "plot_predict(df[\"Passengers\"], pred_AR, start, data_num, 'diff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAsW6etylTJz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCrRx69Erp6"
      },
      "source": [
        "## 自己回帰移動平均モデル（ARMA）\n",
        "\n",
        "#target = df[\"Passengers\"].diff().values\n",
        "\n",
        "start = 101\n",
        "period = 1\n",
        "data_num = 100\n",
        "\n",
        "pred2 = sm.tsa.ARMA(target[start-data_num:start], order=(9, 3)).fit(method='mle').predict(start=data_num, end=data_num+period-1)\n",
        "plot_predict(df[\"Passengers\"], pred2, start, data_num, 'diff')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5QPZIRlErlh"
      },
      "source": [
        "#target = df[\"Passengers\"].diff().values\n",
        "\n",
        "start = 101\n",
        "period = 1\n",
        "data_num = 100\n",
        "pred_seq = np.arange(start+period-1, len(df))\n",
        "pred_ARMA = np.empty((len(pred_seq), period), dtype=float)\n",
        "\n",
        "for i, j in enumerate(pred_seq):\n",
        "  pred_ARMA[i] = sm.tsa.ARMA(target[j-period+1-data_num:j-period+1], order=(9, 3)).fit(method='mle').predict(start=data_num, end=data_num+period-1)\n",
        "plot_predict(df[\"Passengers\"], pred_ARMA, start, data_num, 'diff')\n",
        "\n",
        "########\n",
        "# 12分くらいかかる\n",
        "########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsu_zf6lErgo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyd9Mqb6xGAM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVXuZLh9P8Rj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rurPKbrnftn8"
      },
      "source": [
        "#### <font color=green>**1.3.** </font> ライブラリの公式サンプルコードたち"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HJTUdEYkGI5"
      },
      "source": [
        "#### <font color=green>**1.3.1.** </font> MLE : Maximum Likelihood Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F2AalI62LgW"
      },
      "source": [
        "## Time Series Analysis by State Space Methods statespace\n",
        "# https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/statespace.html\n",
        "# https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyPTpIVWL1Dj"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.signal import lfilter\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_H6kgfuL094"
      },
      "source": [
        "# True model parameters\n",
        "nobs = int(1e3)\n",
        "true_phi = np.r_[0.5, -0.2]\n",
        "true_sigma = 1**0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiggueqWL36u"
      },
      "source": [
        "# Simulate a time series\n",
        "np.random.seed(1234)\n",
        "disturbances = np.random.normal(0, true_sigma, size=(nobs,))\n",
        "endog = lfilter([1], np.r_[1, -true_phi], disturbances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdwGNf5L3zc"
      },
      "source": [
        "# Construct the model\n",
        "class AR2(sm.tsa.statespace.MLEModel):\n",
        "  def __init__(self, endog):\n",
        "    # Initialize the state space model\n",
        "    super(AR2, self).__init__(endog, k_states=2, k_posdef=1,\n",
        "                              initialization='stationary')\n",
        "\n",
        "    # Setup the fixed components of the state space representation\n",
        "    self['design'] = [1, 0]\n",
        "    self['transition'] = [[0, 0],\n",
        "                                  [1, 0]]\n",
        "    self['selection', 0, 0] = 1\n",
        "\n",
        "  # Describe how parameters enter the model\n",
        "  def update(self, params, transformed=True, **kwargs):\n",
        "    params = super(AR2, self).update(params, transformed, **kwargs)\n",
        "\n",
        "    self['transition', 0, :] = params[:2]\n",
        "    self['state_cov', 0, 0] = params[2]\n",
        "\n",
        "  # Specify start parameters and parameter names\n",
        "  @property\n",
        "  def start_params(self):\n",
        "    return [0,0,1]  # these are very simple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VTRI2Ru2XT5"
      },
      "source": [
        "# Create and fit the model\n",
        "mod = AR2(endog)\n",
        "res = mod.fit()\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Bk1vNwg2XRJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts1ScY6l2XPO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmXREmkRgq-S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZerXxoy4kgvS"
      },
      "source": [
        "#### <font color=green>**1.3.2.** </font> SARIMAX : Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3ztBGqR2XNc"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_sarimax_stata.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B0EzPKU2XMB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNvgAeg8MLKE"
      },
      "source": [
        "# Dataset\n",
        "wpi1 = requests.get('http://www.stata-press.com/data/r12/wpi1.dta').content\n",
        "data = pd.read_stata(BytesIO(wpi1))\n",
        "data.index = data.t\n",
        "\n",
        "data['ln_wpi'] = np.log(data['wpi'])\n",
        "data['D.ln_wpi'] = data['ln_wpi'].diff()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0YAqo382XKC"
      },
      "source": [
        "# Fit the model\n",
        "mod = sm.tsa.statespace.SARIMAX(data['wpi'], trend='c', order=(1,1,1))\n",
        "res = mod.fit(disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3BFbas12XFq"
      },
      "source": [
        "# Graph data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15,4))\n",
        "\n",
        "# Levels\n",
        "axes[0].plot(data.index._mpl_repr(), data['wpi'], '-')\n",
        "axes[0].set(title='US Wholesale Price Index')\n",
        "\n",
        "# Log difference\n",
        "axes[1].plot(data.index._mpl_repr(), data['D.ln_wpi'], '-')\n",
        "axes[1].hlines(0, data.index[0], data.index[-1], 'r')\n",
        "axes[1].set(title='US Wholesale Price Index - difference of logs');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDjlPhU62XDf"
      },
      "source": [
        "# Graph data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15,4))\n",
        "\n",
        "fig = sm.graphics.tsa.plot_acf(data.iloc[1:]['D.ln_wpi'], lags=40, ax=axes[0])\n",
        "fig = sm.graphics.tsa.plot_pacf(data.iloc[1:]['D.ln_wpi'], lags=40, ax=axes[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfTADWzN2W-w"
      },
      "source": [
        "# Fit the model\n",
        "mod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\n",
        "res = mod.fit(disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5PXPeGV2W8f"
      },
      "source": [
        "# Dataset\n",
        "air2 = requests.get('http://www.stata-press.com/data/r12/air2.dta').content\n",
        "data = pd.read_stata(BytesIO(air2))\n",
        "data.index = pd.date_range(start=datetime(data.time[0], 1, 1), periods=len(data), freq='MS')\n",
        "data['lnair'] = np.log(data['air'])\n",
        "\n",
        "# Fit the model\n",
        "mod = sm.tsa.statespace.SARIMAX(data['lnair'], order=(2,1,0), seasonal_order=(1,1,0,12), simple_differencing=True)\n",
        "res = mod.fit(disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thVd-sxY2W6h"
      },
      "source": [
        "# Dataset\n",
        "friedman2 = requests.get('http://www.stata-press.com/data/r12/friedman2.dta').content\n",
        "data = pd.read_stata(BytesIO(friedman2))\n",
        "data.index = data.time\n",
        "\n",
        "# Variables\n",
        "endog = data.loc['1959':'1981', 'consump']\n",
        "exog = sm.add_constant(data.loc['1959':'1981', 'm2'])\n",
        "\n",
        "# Fit the model\n",
        "mod = sm.tsa.statespace.SARIMAX(endog, exog, order=(1,0,1))\n",
        "res = mod.fit(disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVtl33Qd2W2q"
      },
      "source": [
        "# Dataset\n",
        "raw = pd.read_stata(BytesIO(friedman2))\n",
        "raw.index = raw.time\n",
        "data = raw.loc[:'1981']\n",
        "\n",
        "# Variables\n",
        "endog = data.loc['1959':, 'consump']\n",
        "exog = sm.add_constant(data.loc['1959':, 'm2'])\n",
        "nobs = endog.shape[0]\n",
        "\n",
        "# Fit the model\n",
        "mod = sm.tsa.statespace.SARIMAX(endog.loc[:'1978-01-01'], exog=exog.loc[:'1978-01-01'], order=(1,0,1))\n",
        "fit_res = mod.fit(disp=False)\n",
        "print(fit_res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHac7Yuc3m-s"
      },
      "source": [
        "mod = sm.tsa.statespace.SARIMAX(endog, exog=exog, order=(1,0,1))\n",
        "res = mod.filter(fit_res.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASW_O38C3m8f"
      },
      "source": [
        "# In-sample one-step-ahead predictions\n",
        "predict = res.get_prediction()\n",
        "predict_ci = predict.conf_int()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QynLsyPl3m5_"
      },
      "source": [
        "# Dynamic predictions\n",
        "predict_dy = res.get_prediction(dynamic='1978-01-01')\n",
        "predict_dy_ci = predict_dy.conf_int()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pp-CcOZ3m3g"
      },
      "source": [
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(9,4))\n",
        "npre = 4\n",
        "ax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n",
        "\n",
        "# Plot data points\n",
        "data.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n",
        "\n",
        "# Plot predictions\n",
        "predict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\n",
        "ci = predict_ci.loc['1977-07-01':]\n",
        "ax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n",
        "predict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\n",
        "ci = predict_dy_ci.loc['1977-07-01':]\n",
        "ax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n",
        "\n",
        "legend = ax.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xixhHBCa3m1M"
      },
      "source": [
        "# Prediction error\n",
        "\n",
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(9,4))\n",
        "npre = 4\n",
        "ax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n",
        "\n",
        "# In-sample one-step-ahead predictions and 95% confidence intervals\n",
        "predict_error = predict.predicted_mean - endog\n",
        "predict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\n",
        "ci = predict_ci.loc['1977-10-01':].copy()\n",
        "ci.iloc[:,0] -= endog.loc['1977-10-01':]\n",
        "ci.iloc[:,1] -= endog.loc['1977-10-01':]\n",
        "ax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n",
        "\n",
        "# Dynamic predictions and 95% confidence intervals\n",
        "predict_dy_error = predict_dy.predicted_mean - endog\n",
        "predict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\n",
        "ci = predict_dy_ci.loc['1977-10-01':].copy()\n",
        "ci.iloc[:,0] -= endog.loc['1977-10-01':]\n",
        "ci.iloc[:,1] -= endog.loc['1977-10-01':]\n",
        "ax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n",
        "\n",
        "legend = ax.legend(loc='lower left');\n",
        "legend.get_frame().set_facecolor('w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxh_odgE3myO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz3OLozpg7XS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aekbWZyg7Rd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq30G5wJkwTD"
      },
      "source": [
        "#### <font color=green>**1.3.3.** </font> SARIMAX : Model selection, missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwE9T3_z3mve"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_sarimax_internet.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nKJ-mVE3mok"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaYDkl5G33zo"
      },
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Download the dataset\n",
        "dk = requests.get('http://www.ssfpack.com/files/DK-data.zip').content\n",
        "f = BytesIO(dk)\n",
        "zipped = ZipFile(f)\n",
        "df = pd.read_table(\n",
        "    BytesIO(zipped.read('internet.dat')),\n",
        "    skiprows=1, header=None, sep='\\s+', engine='python',\n",
        "    names=['internet','dinternet']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuPCKoWp33xj"
      },
      "source": [
        "# Get the basic series\n",
        "dta_full = df.dinternet[1:].values\n",
        "dta_miss = dta_full.copy()\n",
        "\n",
        "# Remove datapoints\n",
        "missing = np.r_[6,16,26,36,46,56,66,72,73,74,75,76,86,96]-1\n",
        "dta_miss[missing] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2KcxhCF33vO"
      },
      "source": [
        "import warnings\n",
        "\n",
        "aic_full = pd.DataFrame(np.zeros((6,6), dtype=float))\n",
        "aic_miss = pd.DataFrame(np.zeros((6,6), dtype=float))\n",
        "\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "# Iterate over all ARMA(p,q) models with p,q in [0,6]\n",
        "for p in range(6):\n",
        "  for q in range(6):\n",
        "    if p == 0 and q == 0:\n",
        "      continue\n",
        "            \n",
        "    # Estimate the model with no missing datapoints\n",
        "    mod = sm.tsa.statespace.SARIMAX(dta_full, order=(p,0,q), enforce_invertibility=False)\n",
        "    try:\n",
        "      res = mod.fit(disp=False)\n",
        "      aic_full.iloc[p,q] = res.aic\n",
        "    except:\n",
        "      aic_full.iloc[p,q] = np.nan\n",
        "        \n",
        "    # Estimate the model with missing datapoints\n",
        "    mod = sm.tsa.statespace.SARIMAX(dta_miss, order=(p,0,q), enforce_invertibility=False)\n",
        "    try:\n",
        "      res = mod.fit(disp=False)\n",
        "      aic_miss.iloc[p,q] = res.aic\n",
        "    except:\n",
        "      aic_miss.iloc[p,q] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfspJ9oE33s-"
      },
      "source": [
        "# Statespace\n",
        "mod = sm.tsa.statespace.SARIMAX(dta_miss, order=(1,0,1))\n",
        "res = mod.fit(disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85RRgqEi33oo"
      },
      "source": [
        "# In-sample one-step-ahead predictions, and out-of-sample forecasts\n",
        "nforecast = 20\n",
        "predict = res.get_prediction(end=mod.nobs + nforecast)\n",
        "idx = np.arange(len(predict.predicted_mean))\n",
        "predict_ci = predict.conf_int(alpha=0.5)\n",
        "\n",
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "ax.xaxis.grid()\n",
        "ax.plot(dta_miss, 'k.')\n",
        "\n",
        "# Plot\n",
        "ax.plot(idx[:-nforecast], predict.predicted_mean[:-nforecast], 'gray')\n",
        "ax.plot(idx[-nforecast:], predict.predicted_mean[-nforecast:], 'k--', linestyle='--', linewidth=2)\n",
        "ax.fill_between(idx, predict_ci[:, 0], predict_ci[:, 1], alpha=0.15)\n",
        "\n",
        "ax.set(title='Figure 8.9 - Internet series');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0gK0Zye33mW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfTGsWc_g-ry"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkeM34Ksg-oT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEdSWt_0k_en"
      },
      "source": [
        "#### <font color=green>**1.3.4.** </font> VARMAX models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StaLa2wj59oi"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_varmax.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWs-WLs959mr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gZPkBuG59j_"
      },
      "source": [
        "dta = sm.datasets.webuse('lutkepohl2', 'http://www.stata-press.com/data/r12/')\n",
        "dta.index = dta.qtr\n",
        "endog = dta.loc['1960-04-01':'1978-10-01', ['dln_inv', 'dln_inc', 'dln_consump']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3ApdJyV59iB"
      },
      "source": [
        "exog = endog['dln_consump']\n",
        "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='nc', exog=exog)\n",
        "res = mod.fit(maxiter=1000, disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OehAwXnP59gL"
      },
      "source": [
        "ax = res.impulse_responses(10, orthogonalized=True).plot(figsize=(13,3))\n",
        "ax.set(xlabel='t', title='Responses to a shock to `dln_inv`');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1fAA47s59dD"
      },
      "source": [
        "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(0,2), error_cov_type='diagonal')\n",
        "res = mod.fit(maxiter=1000, disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJQB0Q759ar"
      },
      "source": [
        "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\n",
        "res = mod.fit(maxiter=1000, disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7wu-fOM6MAI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvAXMALLhU06"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICzUnyLHhUw_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccsJjfeZlAkU"
      },
      "source": [
        "#### <font color=green>**1.3.5.** </font> Dynamic factors and coincident indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iodtgwL6L9w"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_dfm_coincident.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dklZRA_o6L7m"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True, linewidth=120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ6e3tqE6L5d"
      },
      "source": [
        "from pandas_datareader.data import DataReader\n",
        "\n",
        "# Get the datasets from FRED\n",
        "start = '1979-01-01'\n",
        "end = '2014-12-01'\n",
        "indprod = DataReader('IPMAN', 'fred', start=start, end=end)\n",
        "income = DataReader('W875RX1', 'fred', start=start, end=end)\n",
        "sales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\n",
        "emp = DataReader('PAYEMS', 'fred', start=start, end=end)\n",
        "# dta = pd.concat((indprod, income, sales, emp), axis=1)\n",
        "# dta.columns = ['indprod', 'income', 'sales', 'emp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT-uDYsu6L3h"
      },
      "source": [
        "# HMRMT = DataReader('HMRMT', 'fred', start='1967-01-01', end=end)\n",
        "# CMRMT = DataReader('CMRMT', 'fred', start='1997-01-01', end=end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm3w2nDQ6L1X"
      },
      "source": [
        "# HMRMT_growth = HMRMT.diff() / HMRMT.shift()\n",
        "# sales = pd.Series(np.zeros(emp.shape[0]), index=emp.index)\n",
        "\n",
        "# # Fill in the recent entries (1997 onwards)\n",
        "# sales[CMRMT.index] = CMRMT\n",
        "\n",
        "# # Backfill the previous entries (pre 1997)\n",
        "# idx = sales.loc[:'1997-01-01'].index\n",
        "# for t in range(len(idx)-1, 0, -1):\n",
        "#     month = idx[t]\n",
        "#     prev_month = idx[t-1]\n",
        "#     sales.loc[prev_month] = sales.loc[month] / (1 + HMRMT_growth.loc[prev_month].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGvDDrod59YU"
      },
      "source": [
        "dta = pd.concat((indprod, income, sales, emp), axis=1)\n",
        "dta.columns = ['indprod', 'income', 'sales', 'emp']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD4E_iEl59Uc"
      },
      "source": [
        "dta.loc[:, 'indprod':'emp'].plot(subplots=True, layout=(2, 2), figsize=(15, 6));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7u9hpoK59MT"
      },
      "source": [
        "# Create log-differenced series\n",
        "dta['dln_indprod'] = (np.log(dta.indprod)).diff() * 100\n",
        "dta['dln_income'] = (np.log(dta.income)).diff() * 100\n",
        "dta['dln_sales'] = (np.log(dta.sales)).diff() * 100\n",
        "dta['dln_emp'] = (np.log(dta.emp)).diff() * 100\n",
        "\n",
        "# De-mean and standardize\n",
        "dta['std_indprod'] = (dta['dln_indprod'] - dta['dln_indprod'].mean()) / dta['dln_indprod'].std()\n",
        "dta['std_income'] = (dta['dln_income'] - dta['dln_income'].mean()) / dta['dln_income'].std()\n",
        "dta['std_sales'] = (dta['dln_sales'] - dta['dln_sales'].mean()) / dta['dln_sales'].std()\n",
        "dta['std_emp'] = (dta['dln_emp'] - dta['dln_emp'].mean()) / dta['dln_emp'].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ghQg7qY59J7"
      },
      "source": [
        "# Get the endogenous data\n",
        "endog = dta.loc['1979-02-01':, 'std_indprod':'std_emp']\n",
        "\n",
        "# Create the model\n",
        "mod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=2, error_order=2)\n",
        "initial_res = mod.fit(method='powell', disp=False)\n",
        "res = mod.fit(initial_res.params, disp=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IheXoLa33kP"
      },
      "source": [
        "print(res.summary(separate_params=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM6aWhBU3mmf"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(13,3))\n",
        "\n",
        "# Plot the factor\n",
        "dates = endog.index._mpl_repr()\n",
        "ax.plot(dates, res.factors.filtered[0], label='Factor')\n",
        "ax.legend()\n",
        "\n",
        "# Retrieve and also plot the NBER recession indicators\n",
        "rec = DataReader('USREC', 'fred', start=start, end=end)\n",
        "ylim = ax.get_ylim()\n",
        "ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlSqxvti3mkx"
      },
      "source": [
        "res.plot_coefficients_of_determination(figsize=(8,2));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78FlGxqT3miW"
      },
      "source": [
        "usphci = DataReader('USPHCI', 'fred', start='1979-01-01', end='2014-12-01')['USPHCI']\n",
        "usphci.plot(figsize=(13,3));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQgAukMD3mgR"
      },
      "source": [
        "dusphci = usphci.diff()[1:].values\n",
        "def compute_coincident_index(mod, res):\n",
        "  # Estimate W(1)\n",
        "  spec = res.specification\n",
        "  design = mod.ssm['design']\n",
        "  transition = mod.ssm['transition']\n",
        "  ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]\n",
        "  k_states = ss_kalman_gain.shape[0]\n",
        "\n",
        "  W1 = np.linalg.inv(np.eye(k_states) - np.dot(\n",
        "      np.eye(k_states) - np.dot(ss_kalman_gain, design),\n",
        "      transition\n",
        "      )).dot(ss_kalman_gain)[0]\n",
        "\n",
        "  # Compute the factor mean vector\n",
        "  factor_mean = np.dot(W1, dta.loc['1972-02-01':, 'dln_indprod':'dln_emp'].mean())\n",
        "    \n",
        "  # Normalize the factors\n",
        "  factor = res.factors.filtered[0]\n",
        "  factor *= np.std(usphci.diff()[1:]) / np.std(factor)\n",
        "\n",
        "  # Compute the coincident index\n",
        "  coincident_index = np.zeros(mod.nobs+1)\n",
        "  # The initial value is arbitrary; here it is set to\n",
        "  # facilitate comparison\n",
        "  coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()\n",
        "  for t in range(0, mod.nobs):\n",
        "    coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean\n",
        "    \n",
        "  # Attach dates\n",
        "  coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]\n",
        "    \n",
        "  # Normalize to use the same base year as USPHCI\n",
        "  coincident_index *= (usphci.loc['1992-07-01'] / coincident_index.loc['1992-07-01'])\n",
        "    \n",
        "  return coincident_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB--OyHr3meK"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(13,3))\n",
        "\n",
        "# Compute the index\n",
        "coincident_index = compute_coincident_index(mod, res)\n",
        "\n",
        "# Plot the factor\n",
        "dates = endog.index._mpl_repr()\n",
        "ax.plot(dates, coincident_index, label='Coincident index')\n",
        "ax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\n",
        "ax.legend(loc='lower right')\n",
        "\n",
        "# Retrieve and also plot the NBER recession indicators\n",
        "ylim = ax.get_ylim()\n",
        "ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYdDko6u3mZw"
      },
      "source": [
        "from statsmodels.tsa.statespace import tools\n",
        "class ExtendedDFM(sm.tsa.DynamicFactor):\n",
        "  def __init__(self, endog, **kwargs):\n",
        "    # Setup the model as if we had a factor order of 4\n",
        "    super(ExtendedDFM, self).__init__(\n",
        "        endog, k_factors=1, factor_order=4, error_order=2,\n",
        "        **kwargs)\n",
        "\n",
        "    # Note: `self.parameters` is an ordered dict with the\n",
        "    # keys corresponding to parameter types, and the values\n",
        "    # the number of parameters of that type.\n",
        "    # Add the new parameters\n",
        "    self.parameters['new_loadings'] = 3\n",
        "\n",
        "    # Cache a slice for the location of the 4 factor AR\n",
        "    # parameters (a_1, ..., a_4) in the full parameter vector\n",
        "    offset = (self.parameters['factor_loadings'] +\n",
        "                self.parameters['exog'] +\n",
        "                self.parameters['error_cov'])\n",
        "    self._params_factor_ar = np.s_[offset:offset+2]\n",
        "    self._params_factor_zero = np.s_[offset+2:offset+4]\n",
        "\n",
        "  @property\n",
        "  def start_params(self):\n",
        "    # Add three new loading parameters to the end of the parameter\n",
        "    # vector, initialized to zeros (for simplicity; they could\n",
        "    # be initialized any way you like)\n",
        "    return np.r_[super(ExtendedDFM, self).start_params, 0, 0, 0]\n",
        "    \n",
        "  @property\n",
        "  def param_names(self):\n",
        "    # Add the corresponding names for the new loading parameters\n",
        "    #  (the name can be anything you like)\n",
        "    return super(ExtendedDFM, self).param_names + [\n",
        "            'loading.L%d.f1.%s' % (i, self.endog_names[3]) for i in range(1,4)]\n",
        "\n",
        "  def transform_params(self, unconstrained):\n",
        "    # Perform the typical DFM transformation (w/o the new parameters)\n",
        "    constrained = super(ExtendedDFM, self).transform_params(\n",
        "        unconstrained[:-3])\n",
        "\n",
        "    # Redo the factor AR constraint, since we only want an AR(2),\n",
        "    # and the previous constraint was for an AR(4)\n",
        "    ar_params = unconstrained[self._params_factor_ar]\n",
        "    constrained[self._params_factor_ar] = (\n",
        "        tools.constrain_stationary_univariate(ar_params))\n",
        "\n",
        "    # Return all the parameters\n",
        "    return np.r_[constrained, unconstrained[-3:]]\n",
        "\n",
        "  def untransform_params(self, constrained):\n",
        "    # Perform the typical DFM untransformation (w/o the new parameters)\n",
        "    unconstrained = super(ExtendedDFM, self).untransform_params(\n",
        "        constrained[:-3])\n",
        "\n",
        "    # Redo the factor AR unconstraint, since we only want an AR(2),\n",
        "    # and the previous unconstraint was for an AR(4)\n",
        "    ar_params = constrained[self._params_factor_ar]\n",
        "    unconstrained[self._params_factor_ar] = (\n",
        "        tools.unconstrain_stationary_univariate(ar_params))\n",
        "\n",
        "    # Return all the parameters\n",
        "    return np.r_[unconstrained, constrained[-3:]]\n",
        "\n",
        "  def update(self, params, transformed=True, complex_step=False):\n",
        "    # Peform the transformation, if required\n",
        "    if not transformed:\n",
        "      params = self.transform_params(params)\n",
        "    params[self._params_factor_zero] = 0\n",
        "        \n",
        "    # Now perform the usual DFM update, but exclude our new parameters\n",
        "    super(ExtendedDFM, self).update(params[:-3], transformed=True, complex_step=complex_step)\n",
        "\n",
        "    # Finally, set our new parameters in the design matrix\n",
        "    self.ssm['design', 3, 1:4] = params[-3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy1yLPFa2W0F"
      },
      "source": [
        "# Create the model\n",
        "extended_mod = ExtendedDFM(endog)\n",
        "initial_extended_res = extended_mod.fit(maxiter=1000, disp=False)\n",
        "extended_res = extended_mod.fit(initial_extended_res.params, method='nm', maxiter=1000)\n",
        "print(extended_res.summary(separate_params=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HtaMpBt2Wxo"
      },
      "source": [
        "extended_res.plot_coefficients_of_determination(figsize=(8,2));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWCSkOMs2Wvl"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(13,3))\n",
        "\n",
        "# Compute the index\n",
        "extended_coincident_index = compute_coincident_index(extended_mod, extended_res)\n",
        "\n",
        "# Plot the factor\n",
        "dates = endog.index._mpl_repr()\n",
        "ax.plot(dates, coincident_index, '-', linewidth=1, label='Basic model')\n",
        "ax.plot(dates, extended_coincident_index, '--', linewidth=3, label='Extended model')\n",
        "ax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\n",
        "ax.legend(loc='lower right')\n",
        "ax.set(title='Coincident indices, comparison')\n",
        "\n",
        "# Retrieve and also plot the NBER recession indicators\n",
        "ylim = ax.get_ylim()\n",
        "ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgLg2UCfC0tY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC3J4LNHhaWA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8_gUZOvhaK9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCS66hdSlCXx"
      },
      "source": [
        "#### <font color=green>**1.3.6.** </font> Detrending, Stylized Facts and the Business Cycle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdhC8tVOC0rB"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_structural_harvey_jaeger.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh0MTaebC0nu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOUcIT73C0lW"
      },
      "source": [
        "# Datasets\n",
        "from pandas_datareader.data import DataReader\n",
        "\n",
        "# Get the raw data\n",
        "start = '1948-01'\n",
        "end = '2008-01'\n",
        "us_gnp = DataReader('GNPC96', 'fred', start=start, end=end)\n",
        "us_gnp_deflator = DataReader('GNPDEF', 'fred', start=start, end=end)\n",
        "us_monetary_base = DataReader('AMBSL', 'fred', start=start, end=end).resample('QS').mean()\n",
        "recessions = DataReader('USRECQ', 'fred', start=start, end=end).resample('QS').last().values[:,0]\n",
        "\n",
        "# Construct the dataframe\n",
        "dta = pd.concat(map(np.log, (us_gnp, us_gnp_deflator, us_monetary_base)), axis=1)\n",
        "dta.columns = ['US GNP','US Prices','US monetary base']\n",
        "dates = dta.index._mpl_repr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixL_aWCuC0iq"
      },
      "source": [
        "# Plot the data\n",
        "ax = dta.plot(figsize=(13,3))\n",
        "ylim = ax.get_ylim()\n",
        "ax.xaxis.grid()\n",
        "ax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4G36Q5ZC0gJ"
      },
      "source": [
        "# Model specifications\n",
        "\n",
        "# Unrestricted model, using string specification\n",
        "unrestricted_model = {\n",
        "    'level': 'local linear trend', 'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n",
        "    }\n",
        "\n",
        "# Unrestricted model, setting components directly\n",
        "# This is an equivalent, but less convenient, way to specify a\n",
        "# local linear trend model with a stochastic damped cycle:\n",
        "# unrestricted_model = {\n",
        "#     'irregular': True, 'level': True, 'stochastic_level': True, 'trend': True, 'stochastic_trend': True,\n",
        "#     'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n",
        "# }\n",
        "\n",
        "# The restricted model forces a smooth trend\n",
        "restricted_model = {\n",
        "    'level': 'smooth trend', 'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n",
        "    }\n",
        "\n",
        "# Restricted model, setting components directly\n",
        "# This is an equivalent, but less convenient, way to specify a\n",
        "# smooth trend model with a stochastic damped cycle. Notice\n",
        "# that the difference from the local linear trend model is that\n",
        "# `stochastic_level=False` here.\n",
        "# unrestricted_model = {\n",
        "#     'irregular': True, 'level': True, 'stochastic_level': False, 'trend': True, 'stochastic_trend': True,\n",
        "#     'cycle': True, 'damped_cycle': True, 'stochastic_cycle': True\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXmqSzWwC0do"
      },
      "source": [
        "# Output\n",
        "output_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)\n",
        "output_res = output_mod.fit(method='powell', disp=False)\n",
        "\n",
        "# Prices\n",
        "prices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)\n",
        "prices_res = prices_mod.fit(method='powell', disp=False)\n",
        "\n",
        "prices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)\n",
        "prices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)\n",
        "\n",
        "# Money\n",
        "money_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)\n",
        "money_res = money_mod.fit(method='powell', disp=False)\n",
        "\n",
        "money_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)\n",
        "money_restricted_res = money_restricted_mod.fit(method='powell', disp=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_-kIuMhC0Xx"
      },
      "source": [
        "print(output_res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXjVxX5YC0Uo"
      },
      "source": [
        "fig = output_res.plot_components(legend_loc='lower right', figsize=(15, 9));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTeBwJNIC0Sp"
      },
      "source": [
        "# Create Table I\n",
        "table_i = np.zeros((5,6))\n",
        "\n",
        "start = dta.index[0]\n",
        "end = dta.index[-1]\n",
        "time_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\n",
        "models = [\n",
        "          ('US GNP', time_range, 'None'),\n",
        "          ('US Prices', time_range, 'None'),\n",
        "          ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n",
        "          ('US monetary base', time_range, 'None'),\n",
        "          ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n",
        "          ]\n",
        "index = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\n",
        "parameter_symbols = [\n",
        "    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n",
        "    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n",
        "]\n",
        "\n",
        "i = 0\n",
        "for res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n",
        "  if res.model.stochastic_level:\n",
        "    (sigma_irregular, sigma_level, sigma_trend,\n",
        "     sigma_cycle, frequency_cycle, damping_cycle) = res.params\n",
        "  else:\n",
        "    (sigma_irregular, sigma_level,\n",
        "     sigma_cycle, frequency_cycle, damping_cycle) = res.params\n",
        "    sigma_trend = np.nan\n",
        "  period_cycle = 2 * np.pi / frequency_cycle\n",
        "    \n",
        "  table_i[i, :] = [\n",
        "        sigma_level*1e7, sigma_trend*1e7,\n",
        "        sigma_cycle*1e7, damping_cycle, period_cycle,\n",
        "        sigma_irregular*1e7\n",
        "        ]\n",
        "  i += 1\n",
        "    \n",
        "pd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\n",
        "table_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\n",
        "table_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQJoHHT-C0QX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rKDSlLVhd30"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avFlc_hVhdz9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzKcfSOlDpJ"
      },
      "source": [
        "#### <font color=green>**1.3.7.** </font> Trends and cycles in unemployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmKdysVoIzcZ"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_cycles.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnAPvEq0IzaN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsGMQr-yIzX9"
      },
      "source": [
        "from pandas_datareader.data import DataReader\n",
        "endog = DataReader('UNRATE', 'fred', start='1954-01-01')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFWZr_2DIzV1"
      },
      "source": [
        "hp_cycle, hp_trend = sm.tsa.filters.hpfilter(endog, lamb=129600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBQCXzl0IzTw"
      },
      "source": [
        "mod_ucarima = sm.tsa.UnobservedComponents(endog, 'rwalk', autoregressive=4)\n",
        "# Here the powell method is used, since it achieves a\n",
        "# higher loglikelihood than the default L-BFGS method\n",
        "res_ucarima = mod_ucarima.fit(method='powell', disp=False)\n",
        "print(res_ucarima.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfMZlkIXIzRl"
      },
      "source": [
        "mod_uc = sm.tsa.UnobservedComponents(\n",
        "    endog, 'rwalk',\n",
        "    cycle=True, stochastic_cycle=True, damped_cycle=True,\n",
        "    )\n",
        "# Here the powell method gets close to the optimum\n",
        "res_uc = mod_uc.fit(method='powell', disp=False)\n",
        "# but to get to the highest loglikelihood we do a\n",
        "# second round using the L-BFGS method.\n",
        "res_uc = mod_uc.fit(res_uc.params, disp=False)\n",
        "print(res_uc.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml0sqxIZIzOQ"
      },
      "source": [
        "fig, axes = plt.subplots(2, figsize=(13,5));\n",
        "axes[0].set(title='Level/trend component')\n",
        "axes[0].plot(endog.index, res_uc.level.smoothed, label='UC')\n",
        "axes[0].plot(endog.index, res_ucarima.level.smoothed, label='UC-ARIMA(2,0)')\n",
        "axes[0].plot(hp_trend, label='HP Filter')\n",
        "axes[0].legend(loc='upper left')\n",
        "axes[0].grid()\n",
        "\n",
        "axes[1].set(title='Cycle component')\n",
        "axes[1].plot(endog.index, res_uc.cycle.smoothed, label='UC')\n",
        "axes[1].plot(endog.index, res_ucarima.autoregressive.smoothed, label='UC-ARIMA(2,0)')\n",
        "axes[1].plot(hp_cycle, label='HP Filter')\n",
        "axes[1].legend(loc='upper left')\n",
        "axes[1].grid()\n",
        "\n",
        "fig.tight_layout();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kVqqmyiIzA5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOR62SJDhj_9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz_NV-oThj8Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6yEcDeZlEdu"
      },
      "source": [
        "#### <font color=green>**1.3.8.** </font> State space modeling : Local Linear Trends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F5stZPVIy91"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_local_linear_trend.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlsOTEzfIy2f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChkoR9p4Iy0J"
      },
      "source": [
        "\"\"\"\n",
        "Univariate Local Linear Trend Model\n",
        "\"\"\"\n",
        "class LocalLinearTrend(sm.tsa.statespace.MLEModel):\n",
        "  def __init__(self, endog):\n",
        "    # Model order\n",
        "    k_states = k_posdef = 2\n",
        "\n",
        "    # Initialize the statespace\n",
        "    super(LocalLinearTrend, self).__init__(\n",
        "        endog, k_states=k_states, k_posdef=k_posdef,\n",
        "        initialization='approximate_diffuse',\n",
        "        loglikelihood_burn=k_states\n",
        "        )\n",
        "\n",
        "    # Initialize the matrices\n",
        "    self.ssm['design'] = np.array([1, 0])\n",
        "    self.ssm['transition'] = np.array([[1, 1],\n",
        "                                       [0, 1]])\n",
        "    self.ssm['selection'] = np.eye(k_states)\n",
        "\n",
        "    # Cache some indices\n",
        "    self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n",
        "\n",
        "  @property\n",
        "  def param_names(self):\n",
        "    return ['sigma2.measurement', 'sigma2.level', 'sigma2.trend']\n",
        "\n",
        "  @property\n",
        "  def start_params(self):\n",
        "    return [np.std(self.endog)]*3\n",
        "\n",
        "  def transform_params(self, unconstrained):\n",
        "    return unconstrained**2\n",
        "\n",
        "  def untransform_params(self, constrained):\n",
        "    return constrained**0.5\n",
        "\n",
        "  def update(self, params, *args, **kwargs):\n",
        "    params = super(LocalLinearTrend, self).update(params, *args, **kwargs)\n",
        "        \n",
        "    # Observation covariance\n",
        "    self.ssm['obs_cov',0,0] = params[0]\n",
        "\n",
        "    # State covariance\n",
        "    self.ssm[self._state_cov_idx] = params[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_RHgOM0Iyxi"
      },
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "    \n",
        "# Download the dataset\n",
        "ck = requests.get('http://staff.feweb.vu.nl/koopman/projects/ckbook/OxCodeAll.zip').content\n",
        "zipped = ZipFile(BytesIO(ck))\n",
        "df = pd.read_table(\n",
        "    BytesIO(zipped.read('OxCodeIntroStateSpaceBook/Chapter_2/NorwayFinland.txt')),\n",
        "    skiprows=1, header=None, sep='\\s+', engine='python',\n",
        "    names=['date','nf', 'ff']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaQMFotkIyu_"
      },
      "source": [
        "# Load Dataset\n",
        "df.index = pd.date_range(start='%d-01-01' % df.date[0], end='%d-01-01' % df.iloc[-1, 0], freq='AS')\n",
        "\n",
        "# Log transform\n",
        "df['lff'] = np.log(df['ff'])\n",
        "\n",
        "# Setup the model\n",
        "mod = LocalLinearTrend(df['lff'])\n",
        "\n",
        "# Fit it using MLE (recall that we are fitting the three variance parameters)\n",
        "res = mod.fit(disp=False)\n",
        "print(res.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRvZMyRUIysb"
      },
      "source": [
        "# Perform prediction and forecasting\n",
        "predict = res.get_prediction()\n",
        "forecast = res.get_forecast('2014')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptt20DD0IypS"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,4))\n",
        "\n",
        "# Plot the results\n",
        "df['lff'].plot(ax=ax, style='k.', label='Observations')\n",
        "predict.predicted_mean.plot(ax=ax, label='One-step-ahead Prediction')\n",
        "predict_ci = predict.conf_int(alpha=0.05)\n",
        "predict_index = np.arange(len(predict_ci))\n",
        "ax.fill_between(predict_index[2:], predict_ci.iloc[2:, 0], predict_ci.iloc[2:, 1], alpha=0.1)\n",
        "\n",
        "forecast.predicted_mean.plot(ax=ax, style='r', label='Forecast')\n",
        "forecast_ci = forecast.conf_int()\n",
        "forecast_index = np.arange(len(predict_ci), len(predict_ci) + len(forecast_ci))\n",
        "ax.fill_between(forecast_index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], alpha=0.1)\n",
        "\n",
        "# Cleanup the image\n",
        "ax.set_ylim((4, 8));\n",
        "legend = ax.legend(loc='lower left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PEThFdwC0Oa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6zx630ZhnBY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-A6cVjShm9b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSqaYFovlFje"
      },
      "source": [
        "#### <font color=green>**1.3.9.** </font> Autoregressive Moving Average (ARMA) : Sunspots data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ulJIOVBJuXn"
      },
      "source": [
        "## https://thequackdaddy.github.io/statsmodels.github.io/0.9.0/examples/notebooks/generated/statespace_arma_0.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukQvtSTJuVM"
      },
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vglgGlk4JuSx"
      },
      "source": [
        "from statsmodels.graphics.api import qqplot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UT9rMQMJuQe"
      },
      "source": [
        "print(sm.datasets.sunspots.NOTE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkE3MR7oJuOG"
      },
      "source": [
        "dta = sm.datasets.sunspots.load_pandas().data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9LfDS4WJuL-"
      },
      "source": [
        "dta.index = pd.Index(sm.tsa.datetools.dates_from_range('1700', '2008'))\n",
        "del dta[\"YEAR\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZexyMmaJuJq"
      },
      "source": [
        "dta.plot(figsize=(12,4));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mOGIe15JuHI"
      },
      "source": [
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxidNp6UJuEX"
      },
      "source": [
        "arma_mod20 = sm.tsa.statespace.SARIMAX(dta, order=(2,0,0), trend='c').fit(disp=False)\n",
        "print(arma_mod20.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SlaLTCKC0MC"
      },
      "source": [
        "arma_mod30 = sm.tsa.statespace.SARIMAX(dta, order=(3,0,0), trend='c').fit(disp=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY5niiPwC0Jl"
      },
      "source": [
        "print(arma_mod20.aic, arma_mod20.bic, arma_mod20.hqic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si1DMKKMC0HA"
      },
      "source": [
        "print(arma_mod30.params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLGq6T7XC0Ds"
      },
      "source": [
        "print(arma_mod30.aic, arma_mod30.bic, arma_mod30.hqic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlehfRX7KGvW"
      },
      "source": [
        "sm.stats.durbin_watson(arma_mod30.resid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfBY5eAdKGsy"
      },
      "source": [
        "fig = plt.figure(figsize=(12,4))\n",
        "ax = fig.add_subplot(111)\n",
        "ax = plt.plot(arma_mod30.resid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY_fFteJKGov"
      },
      "source": [
        "resid = arma_mod30.resid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87FIR3qdKKe9"
      },
      "source": [
        "stats.normaltest(resid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2raYJQhiKKco"
      },
      "source": [
        "fig = plt.figure(figsize=(12,4))\n",
        "ax = fig.add_subplot(111)\n",
        "fig = qqplot(resid, line='q', ax=ax, fit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYRSGq1cKKaQ"
      },
      "source": [
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(resid, lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEJixYM3KKX3"
      },
      "source": [
        "r,q,p = sm.tsa.acf(resid, qstat=True)\n",
        "data = np.c_[range(1,41), r[1:], q, p]\n",
        "table = pd.DataFrame(data, columns=['lag', \"AC\", \"Q\", \"Prob(>Q)\"])\n",
        "print(table.set_index('lag'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVezhovOKKVf"
      },
      "source": [
        "predict_sunspots = arma_mod30.predict(start='1990', end='2012', dynamic=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30vepPsYKKTG"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "dta.loc['1950':].plot(ax=ax)\n",
        "predict_sunspots.plot(ax=ax, style='r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmUa4SPXKKQr"
      },
      "source": [
        "def mean_forecast_err(y, yhat):\n",
        "  return y.sub(yhat).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO7pHBkJKKNq"
      },
      "source": [
        "mean_forecast_err(dta.SUNACTIVITY, predict_sunspots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqkZXglmCzw8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97obxePbfuLp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz6uyZKYfuCe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_xesarHP8Rj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4zivgWEOLS6"
      },
      "source": [
        "### <font color=blue>**2.** </font> Linear Dynamical System（LDS）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvB-YbugoRkZ"
      },
      "source": [
        "#### <font color=green>**2.1.** </font> 状態空間モデル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkvlITOKPA_C"
      },
      "source": [
        "## 出典 : https://logics-of-blue.com/python-state-space-models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_daShqzkPA9d"
      },
      "source": [
        "# 基本のライブラリを読み込む\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# グラフ描画\n",
        "from matplotlib import pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# グラフを横長にする\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 15, 6\n",
        "\n",
        "# 統計モデル\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVmLQMc7PA7N"
      },
      "source": [
        "# 日付形式で読み込む\n",
        "dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/jiai-tus/FirstTerm/main/20210518/datasets/AirPassengers.csv',\n",
        "                   index_col='Month', date_parser=dateparse, dtype='float')\n",
        "\n",
        "# 日付形式にする\n",
        "ts = data['#Passengers'] \n",
        "ts.head()\n",
        "\n",
        "# プロット\n",
        "plt.plot(ts)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uwbBS0lPA5U"
      },
      "source": [
        "# ローカルレベルモデルの推定\n",
        "mod_local_level = sm.tsa.UnobservedComponents(ts, 'local level')\n",
        "\n",
        "# 最尤法によるパラメタの推定\n",
        "res_local_level = mod_local_level.fit()\n",
        "\n",
        "# 推定されたパラメタ一覧\n",
        "print(res_local_level.summary())\n",
        "\n",
        "# 推定された状態・トレンドの描画\n",
        "rcParams['figure.figsize'] = 15, 15\n",
        "fig = res_local_level.plot_components()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_HpvIYrPA3H"
      },
      "source": [
        "# ローカル線形トレンドモデル\n",
        "\n",
        "mod_trend = sm.tsa.UnobservedComponents(\n",
        "    ts,\n",
        "    'local linear trend'\n",
        ")\n",
        "\n",
        "# 最尤法によるパラメタの推定\n",
        "# ワーニングが出たのでBFGS法で最適化する\n",
        "res_trend = mod_trend.fit(method='bfgs')\n",
        "\n",
        "# 推定されたパラメタ一覧\n",
        "print(res_trend.summary())\n",
        "\n",
        "# 推定された状態・トレンドの描画\n",
        "rcParams['figure.figsize'] = 15, 20\n",
        "fig = res_trend.plot_components()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3bsdAxFPAym"
      },
      "source": [
        "# 季節変動ありのローカルレベルモデル\n",
        "\n",
        "mod_season_local_level = sm.tsa.UnobservedComponents(\n",
        "    ts,\n",
        "    'local level',\n",
        "    seasonal=12\n",
        ")\n",
        "\n",
        "# まずはNelder-Meadでパラメタを推定し、その結果を初期値としてまた最適化する。2回目はBFGSを使用。\n",
        "res_season_local_level = mod_season_local_level.fit(\n",
        "    method='bfgs', \n",
        "    maxiter=500, \n",
        "    start_params=mod_season_local_level.fit(method='nm', maxiter=500).params,\n",
        ")\n",
        "\n",
        "# 推定されたパラメタ一覧\n",
        "print(res_season_local_level.summary())\n",
        "\n",
        "# 推定された状態・トレンド・季節の影響の描画\n",
        "rcParams['figure.figsize'] = 15, 20\n",
        "fig = res_season_local_level.plot_components()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfUmdbJSPAwR"
      },
      "source": [
        "# 季節変動ありのローカル線形トレンドモデル\n",
        "\n",
        "mod_season_trend = sm.tsa.UnobservedComponents(\n",
        "    ts,\n",
        "    'local linear trend',\n",
        "    seasonal=12\n",
        ")\n",
        "\n",
        "# まずはNelder-Meadでパラメタを推定し、その結果を初期値としてまた最適化する。2回目はBFGSを使用。\n",
        "res_season_trend = mod_season_trend.fit(\n",
        "    method='bfgs', \n",
        "    maxiter=500, \n",
        "    start_params=mod_season_trend.fit(method='nm', maxiter=500).params,\n",
        ")\n",
        "\n",
        "# 推定されたパラメタ一覧\n",
        "print(res_season_trend.summary())\n",
        "\n",
        "# 推定された状態・トレンド・季節の影響の描画\n",
        "rcParams['figure.figsize'] = 15, 20\n",
        "fig = res_season_trend.plot_components()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y11UyknJPAts"
      },
      "source": [
        "# 詳細は以下の資料を参照してください\n",
        "# http://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.structural.UnobservedComponents.html\n",
        "\n",
        "# 季節変動ありのローカル線形トレンドモデル\n",
        "# ただし、トレンドの分散は無し\n",
        "\n",
        "mod_season_trend_d = sm.tsa.UnobservedComponents(\n",
        "    ts,\n",
        "    'local linear deterministic trend',\n",
        "    seasonal=12\n",
        ")\n",
        "\n",
        "# まずはNelder-Meadでパラメタを推定し、その結果を初期値としてまた最適化する。2回目はBFGSを使用。\n",
        "res_season_trend_d = mod_season_trend_d.fit(\n",
        "    method='bfgs', \n",
        "    maxiter=500, \n",
        "    start_params=mod_season_trend_d.fit(method='nm', maxiter=500).params,\n",
        ")\n",
        "\n",
        "# 推定されたパラメタ一覧\n",
        "print(res_season_trend_d.summary())\n",
        "\n",
        "# 推定された状態・トレンド・季節の影響の描画\n",
        "rcParams['figure.figsize'] = 15, 20\n",
        "fig = res_season_trend_d.plot_components()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44aOufeAPArI"
      },
      "source": [
        "# 詳細は以下の資料を参照してください\n",
        "# http://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.structural.UnobservedComponents.html\n",
        "\n",
        "# 季節変動ありのローカル線形トレンドモデル\n",
        "# ただし、トレンドと観測誤差の分散は無し\n",
        "\n",
        "mod_season_rw = sm.tsa.UnobservedComponents(\n",
        "    ts,\n",
        "    'random walk with drift',\n",
        "    seasonal=12\n",
        ")\n",
        "\n",
        "# まずはNelder-Meadでパラメタを推定し、その結果を初期値としてまた最適化する。2回目はBFGSを使用。\n",
        "res_season_rw = mod_season_rw.fit(\n",
        "    method='bfgs', \n",
        "    maxiter=500, \n",
        "    start_params=mod_season_rw.fit(method='nm', maxiter=500).params,\n",
        ")\n",
        "\n",
        "# 推定されたパラメタ一覧\n",
        "print(res_season_rw.summary())\n",
        "\n",
        "# 推定された状態・トレンド・季節の影響の描画\n",
        "rcParams['figure.figsize'] = 15, 20\n",
        "fig = res_season_rw.plot_components()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2JWENkzPAiH"
      },
      "source": [
        "# 今まで計算してきたモデルのAICを格納する\n",
        "aic_list = pd.DataFrame(index=range(6), columns=[\"model\", \"aic\"])\n",
        "\n",
        "### .ix -> .loc\n",
        "\n",
        "aic_list.loc[0][\"model\"] = \"res_local_level\"\n",
        "aic_list.loc[0][\"aic\"] = res_local_level.aic\n",
        "\n",
        "aic_list.loc[1][\"model\"] = \"res_trend\"\n",
        "aic_list.loc[1][\"aic\"] = res_trend.aic\n",
        "\n",
        "aic_list.loc[2][\"model\"] = \"res_season_local_level\"\n",
        "aic_list.loc[2][\"aic\"] = res_season_local_level.aic\n",
        "\n",
        "aic_list.loc[3][\"model\"] = \"res_season_trend\"\n",
        "aic_list.loc[3][\"aic\"] = res_season_trend.aic\n",
        "\n",
        "aic_list.loc[4][\"model\"] = \"res_season_trend_d\"\n",
        "aic_list.loc[4][\"aic\"] = res_season_trend_d.aic\n",
        "\n",
        "aic_list.loc[5][\"model\"] = \"res_season_rw\"\n",
        "aic_list.loc[5][\"aic\"] = res_season_rw.aic\n",
        "\n",
        "# 結果の表示\n",
        "aic_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuKEwrm0PAgQ"
      },
      "source": [
        "# 予測\n",
        "pred = res_season_rw.predict('1960-01-01', '1961-12-01')\n",
        "\n",
        "# 実データと予測結果の図示\n",
        "rcParams['figure.figsize'] = 15, 6\n",
        "plt.plot(ts)\n",
        "plt.plot(pred, \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iG1FAtXnpWg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFPjfXLtnpT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGZ6ucs6npQX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmYRiE3APK8K"
      },
      "source": [
        "#### <font color=green>**2.2.** </font> Kalman filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avjudmi6Xqj3"
      },
      "source": [
        "## 出典 : https://qiita.com/matsui_685/items/16b81bf0ad9a24c54e52"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PM_1kcbXxFJ"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB3dVbCVsOKF"
      },
      "source": [
        "# 初期位置\n",
        "initial_xy = [6., 17.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4l9iGCKsOEe"
      },
      "source": [
        "# 計測間隔\n",
        "dt = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq63FqksXzPf"
      },
      "source": [
        "# 位置[x,y]の計測結果\n",
        "measurements = [[7., 15.], \n",
        "                  [8., 14.], \n",
        "                  [9., 13.], \n",
        "                  [10., 12.], \n",
        "                  [11., 11.], \n",
        "                  [12., 10.]] \n",
        "\n",
        "## 計測は位置だけを取得でき、速度は計測不能とします。\n",
        "## ここでは0.1秒間隔で6回計測したとします。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azGGCZnZsUrC"
      },
      "source": [
        "# 初期位置と初期速度を代入した「4次元状態」\n",
        "x = np.array([[initial_xy[0]], \n",
        "              [initial_xy[1]], \n",
        "              [0.], \n",
        "              [0.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dQBZvOcXzMH"
      },
      "source": [
        "# 外部要素\n",
        "u = np.array([[0.], \n",
        "              [0.], \n",
        "              [0.], \n",
        "              [0.]]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH9f-5xZsXcT"
      },
      "source": [
        "# 共分散行列\n",
        "## この値が大きいほど予測が広く分布し、値の精度が悪いことになります\n",
        "P = np.array([[0., 0., 0., 0.], \n",
        "              [0., 0., 0., 0.], \n",
        "              [0., 0., 100., 0.], \n",
        "              [0., 0., 0., 100.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIgbS3R-sXWQ"
      },
      "source": [
        "# 状態遷移行列\n",
        "## 次の時刻での状態を求めるのに使用します\n",
        "F = np.array([[1., 0., dt, 0.], \n",
        "              [0., 1., 0., dt], \n",
        "              [0., 0., 1., 0.], \n",
        "              [0., 0., 0., 1.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcGA8PfpsXSj"
      },
      "source": [
        "# 観測行列\n",
        "## 観測値は位置だけなので、4次元状態から位置だけを抽出する役割を持ちます\n",
        "H = np.array([[1., 0., 0, 0], \n",
        "              [0., 1., 0., 0.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfG2iUhJsfCj"
      },
      "source": [
        "# ノイズ\n",
        "## 計測結果はノイズにより不確かなものとなります\n",
        "R = np.array([[0.1, 0], \n",
        "              [0, 0.1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t9tizJGYSZ6"
      },
      "source": [
        "# 4次元単位行列\n",
        "I = np.identity((len(x)))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eawZSuPvYSXf"
      },
      "source": [
        "def kalman_filter(x, P):\n",
        "  for n in range(len(measurements)):\n",
        "    # 予測\n",
        "    x = np.dot(F, x) + u\n",
        "    P = np.dot(np.dot(F, P), F.T)\n",
        "\n",
        "    # 計測更新\n",
        "    Z = np.array([measurements[n]])\n",
        "    y = Z.T - np.dot(H, x)\n",
        "    S = np.dot(np.dot(H, P), H.T) + R\n",
        "    K = np.dot(np.dot(P, H.T), np.linalg.inv(S))\n",
        "    x = x + np.dot(K, y)        \n",
        "    P = np.dot((I - np.dot(K, H)), P)\n",
        "\n",
        "  x = x.tolist()\n",
        "  P = P.tolist()\n",
        "  return x,P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBhDoErXXh4f"
      },
      "source": [
        "print(\"6回の計測後の位置と速度の予測値：\\n{}\".format(kalman_filter(x, P)[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9HT-M9Tr8nS"
      },
      "source": [
        "# 得られた予測値は、6回目の観測値[12,10]よりわずかに下方へ修正されています。\n",
        "# 速度は10分の1にして0.1秒単位で計算すると、x方向に約1で、y方向に約-1となっており、観測値の変化から推測できる値に近い結果が出ています"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBOIYQ_Vp5Rf"
      },
      "source": [
        "kalman_filter(x, P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0K9-eQMPFwJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROekxQscPFs6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDNBgbWEPLQb"
      },
      "source": [
        "#### <font color=green>**2.3.** </font> Kalman smoother"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgR9ovMDc3mY"
      },
      "source": [
        "## 出典 : https://qiita.com/Kosuke-Szk/items/9f7b7d71dc2d435fe2cf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RktQpyzc3kq"
      },
      "source": [
        "## 衛星の回転運動モデル\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcrAqttCskSY"
      },
      "source": [
        "# step数\n",
        "N = 60\n",
        "\n",
        "# 人工衛星の回転運動を線形近似した4次元システムを考える\n",
        "# 人工衛星の姿勢角度、角速度、角加速度の平均値成分、角加速度のランダム成分\n",
        "nx = 4\n",
        "\n",
        "# 推定したいのは衛星の姿勢角\n",
        "ny = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQhKff5skOu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo-V7ruwc3hT"
      },
      "source": [
        "F = np.matrix([[1, 1, 0.5, 0.5],\n",
        "              [0, 1,   1,   1],\n",
        "              [0, 0,   1,   0],\n",
        "              [0, 0,   0, 0.606]]) # 遷移行列\n",
        "\n",
        "H = np.matrix([1,0,0,0])  # 観測行列\n",
        "\n",
        "R = np.matrix([1]) # 観測ノイズ共分散行列\n",
        "\n",
        "G = np.matrix([0,0,0,1])\n",
        "\n",
        "# ガウスノイズの分散\n",
        "q = 0.0064\n",
        "\n",
        "x = np.zeros([N,nx])  # 状態ベクトル(真値)\n",
        "y = np.zeros([N,ny])  # 観測ベクトル"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLbNmFCHc3fO"
      },
      "source": [
        "x[0,:] = np.array([1.25, 0.06, 0.01, -0.003])\n",
        "\n",
        "for i in range(1,N):\n",
        "  x[i,:] = F.dot(x[i-1]) + G*np.random.normal(0, q)\n",
        "  y[i,:] = H.dot(x[i,:]) + np.random.normal(0, R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5PKxOpUc3aO"
      },
      "source": [
        "# カルマンフィルター\n",
        "xp = np.zeros([N, nx])     # 一期先予測\n",
        "Pp = np.zeros([N, nx, nx]) # 一期先共分散\n",
        "\n",
        "xp[0,:] = np.array([0,0,0,0])  # 初期推定値\n",
        "Pp[0,:,:] = np.diag([10,10,10,10]) # 初期共分散\n",
        "\n",
        "xu = np.copy(xp)   # フィルタ予測\n",
        "Pu = np.copy(Pp)   # フィルタ共分散\n",
        "\n",
        "for i in range(1,N):\n",
        "  # 時間更新ステップ\n",
        "  xp[i,:] = F.dot(xu[i-1,:])\n",
        "  Pp[i,:,:] = F.dot(Pu[i-1,:,:]).dot(F.T) + G.dot(q).dot(G.T)\n",
        "  # 観測更新ステップ\n",
        "  K = Pp[i,:,:].dot(H.T).dot(np.linalg.inv(H.dot(Pp[i,:,:]).dot(H.T)+R))\n",
        "  xu[i,:] = xp[i,:] + K.dot(y[i,:] - H.dot(xp[i,:])).squeeze()\n",
        "  Pu[i,:,:] = Pp[i,:,:] - K.dot(H).dot(Pp[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHHhJ2-Pc3Xh"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(x[:,0], label=\"True $x_t$\")\n",
        "plt.plot(y[:,0], marker=\"x\", alpha=0.8, label=\"Observation $y_t$\")\n",
        "plt.plot(xu[:,0], marker=\".\", alpha=0.8, label=\"Filtered estimate $\\hat{x_{t/t}}$\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"$x_t,y_t,\\hat{x_{t/t}}$\")\n",
        "plt.xlabel(\"Number of steps t\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIAL9hMwc3VH"
      },
      "source": [
        "# カルマンスムーザー\n",
        "xs = np.zeros([N, nx])     \n",
        "Ps = np.zeros([N, nx, nx]) \n",
        "\n",
        "xs[-1,:] = xu[-1,:]\n",
        "Ps[-1,:,:] = Pu[-1,:,:]\n",
        "\n",
        "for i in reversed(range(N-1)):\n",
        "    # 平滑化ステップ\n",
        "    C = Pu[i,:,:].dot(F.T).dot(np.linalg.inv(Pp[i+1,:,:]))\n",
        "    Ps[i,:,:] = Pu[i,:,:] + C.dot(Ps[i+1,:,:]-Pp[i+1,:,:]).dot(C.T)\n",
        "    xs[i,:] = xu[i,:] + C.dot(xs[i+1]-xp[i+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWbAbCqGc3SH"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(x[:,0], label=\"True $x_t$\")\n",
        "plt.plot(y[:,0], marker=\"x\", alpha=0.8, label=\"Observation $y_t$\")\n",
        "plt.plot(xu[:,0], marker=\"^\", alpha=0.8, markerfacecolor=\"None\", label=\"Filtered estimate $\\hat{x_{t/t}}$\")\n",
        "plt.plot(xs[:,0], marker=\"o\", alpha=0.8, markerfacecolor=\"None\", label=\"Smoothed estimate $\\hat{x}_{t/N}^{(1)}$\")\n",
        "plt.legend()\n",
        "plt.ylabel(\"$x_t,y_t,\\hat{x_{t/t}}$\")\n",
        "plt.xlabel(\"Number of steps t\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"ex2_image.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbOCat2aPFkB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Admmbk6wOKRj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLLMUqJ_OKPe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgjH6_0EOM0-"
      },
      "source": [
        "### <font color=blue>**3.** </font> Hidden Marcov Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSGiUjG6PNZi"
      },
      "source": [
        "#### <font color=green>**3.1.** </font> Viterbi algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nBHp2HD0NFh"
      },
      "source": [
        "## Viterbi algorithm\n",
        "## https://ja.m.wikipedia.org/wiki/ビタビアルゴリズム"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQzDNxCj0M8K"
      },
      "source": [
        "def forward_viterbi(y, X, sp, tp, ep):\n",
        "  T = {}\n",
        "  for state in X:\n",
        "    ##          prob.      V. path  V. prob.\n",
        "    T[state] = (sp[state], [state], sp[state])\n",
        "  for output in y:\n",
        "    U = {}\n",
        "    for next_state in X:\n",
        "      total = 0\n",
        "      argmax = None\n",
        "      valmax = 0\n",
        "      for source_state in X:\n",
        "        (prob, v_path, v_prob) = T[source_state]\n",
        "        p = ep[source_state][output] * tp[source_state][next_state]\n",
        "        prob *= p\n",
        "        v_prob *= p\n",
        "        total += prob\n",
        "        if v_prob > valmax:\n",
        "          argmax = v_path + [next_state]\n",
        "          valmax = v_prob\n",
        "      U[next_state] = (total, argmax, valmax)\n",
        "    T = U\n",
        "  ## apply sum/max to the final states:\n",
        "  total = 0\n",
        "  argmax = None\n",
        "  valmax = 0\n",
        "  for state in X:\n",
        "    (prob, v_path, v_prob) = T[state]\n",
        "    total += prob\n",
        "    if v_prob > valmax:\n",
        "      argmax = v_path\n",
        "      valmax = v_prob\n",
        "  return (total, argmax, valmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuD9-zLTMC8t"
      },
      "source": [
        "states = ('Rainy', 'Sunny')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbok6JiSMgpR"
      },
      "source": [
        "start_probability = {\n",
        "    'Rainy': 0.6,\n",
        "    'Sunny': 0.4\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqRuBkzkMC5w"
      },
      "source": [
        "transition_probability = {\n",
        "    'Rainy' : {\n",
        "        'Rainy': 0.7,\n",
        "        'Sunny': 0.3\n",
        "         },\n",
        "    'Sunny' : {\n",
        "        'Rainy': 0.4,\n",
        "        'Sunny': 0.6\n",
        "        }\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYulv1Da0NEG"
      },
      "source": [
        "emission_probability = {\n",
        "    'Rainy' : {\n",
        "        'walk': 0.1, \n",
        "        'shop': 0.4, \n",
        "        'clean': 0.5\n",
        "        },\n",
        "    \n",
        "    'Sunny' : {\n",
        "        'walk': 0.6,\n",
        "        'shop': 0.3, \n",
        "        'clean': 0.1\n",
        "        },\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbW_TtUu0M6H"
      },
      "source": [
        "observations = ('walk', 'clean', 'shop')\n",
        "\n",
        "total, argmax, valmax = forward_viterbi(observations,\n",
        "                                        states,\n",
        "                                        start_probability,\n",
        "                                        transition_probability,\n",
        "                                        emission_probability)\n",
        "\n",
        "print(\"observations \\t: {}\".format(observations))\n",
        "print(\"\\t total \\t: {}\".format(total))\n",
        "print(\"\\t argmax \\t: {}\".format(argmax))\n",
        "print(\"\\t valmax \\t: {}\".format(valmax))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFGRpUjHPwDM"
      },
      "source": [
        "observations = ('walk', 'clean', 'shop', 'walk', 'clean', 'shop')\n",
        "\n",
        "total, argmax, valmax = forward_viterbi(observations,\n",
        "                                        states,\n",
        "                                        start_probability,\n",
        "                                        transition_probability,\n",
        "                                        emission_probability)\n",
        "\n",
        "print(\"observations \\t: {}\".format(observations))\n",
        "print(\"\\t total \\t: {}\".format(total))\n",
        "print(\"\\t argmax \\t: {}\".format(argmax))\n",
        "print(\"\\t valmax \\t: {}\".format(valmax))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtL3T8NPPwAg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k36o111sON8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JGAEkqgON6T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phh_A5OLPOWM"
      },
      "source": [
        "#### <font color=green>**3.2.** </font> イカサマを見抜く"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoOlmAjVCX2M"
      },
      "source": [
        "## 出典 : https://sites.google.com/site/ryunosukehm/study/ml-with-python/impact-ss2017/hmm_toy_example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSFdLpQbUzLG"
      },
      "source": [
        "!pip install hmmlearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwrLykWwCXua"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from hmmlearn import hmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OIN2MGbCXrK"
      },
      "source": [
        "# 1枚のコインを100回投げて出た目の系列を生成し，プロット\n",
        "\n",
        "X = np.empty((0,1), int)\n",
        "for i in range(0,100):\n",
        "  X = np.append(X, np.array([[np.random.binomial(1, p=0.5)]]),axis=0)\n",
        "\n",
        "plt.plot(X)\n",
        "plt.xlabel('Trial')\n",
        "plt.yticks((0,1))\n",
        "ax = plt.gca()\n",
        "ax.set_yticklabels(['Tail','Head'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND8sbaEOTbEs"
      },
      "source": [
        "'''最初，\n",
        "  表が出る確率は 0.7  裏が出る確率は 0.3\n",
        "\n",
        "しかし途中で，\n",
        "  表が出る確率は 0.0  裏が出る確率は 1.0\n",
        "にすり替える\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCIxPpvpCXms"
      },
      "source": [
        "X = np.empty((0,1), int)\n",
        "for i in range(0,50):\n",
        "  X = np.append(X, np.array([[np.random.binomial(1, p=0.7)]]),axis=0)\n",
        "for i in range(0,50):\n",
        "  X = np.append(X, np.array([[np.random.binomial(1, p=0.0)]]),axis=0)\n",
        "\n",
        "plt.plot(X)\n",
        "plt.xlabel('Trial')\n",
        "plt.yticks((0,1))\n",
        "ax = plt.gca()\n",
        "ax.set_yticklabels(['Tail','Head'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erht5aoJCZuE"
      },
      "source": [
        "model = hmm.MultinomialHMM(n_components=2)\n",
        "\n",
        "model.fit(X)\n",
        "L,Z = model.decode(X)\n",
        "\n",
        "plt.plot(Z)\n",
        "plt.xlabel('Trial')\n",
        "plt.yticks((0,1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foJXOs-BCZrI"
      },
      "source": [
        "print(model.emissionprob_)\n",
        "\n",
        "# 1行目： 状態0で裏が出る確率， 状態0で表が出る確率\n",
        "# 2行目： 状態1で裏が出る確率， 状態1で表が出る確率"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP8AKEAjTx-a"
      },
      "source": [
        "'''最初\n",
        "  裏が出る確率 0.3  表が出る確率 0.7\n",
        "すり替え後\n",
        "  裏が出る確率 1.0    表が出る確率 0.0 \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT-4rwIoCZoc"
      },
      "source": [
        "plt.plot(Z)\n",
        "plt.xlabel('Trial')\n",
        "plt.yticks((0,1))\n",
        "ax = plt.gca()\n",
        "ax.set_yticklabels(['Fair','Cheating'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swysTfd6ONzd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxiYhkZIOMG1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwN20kefPOv9"
      },
      "source": [
        "#### <font color=green>**3.3.** </font> Sampling from HMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko5vYLfUvla1"
      },
      "source": [
        "## 出典 : https://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_hmm_sampling.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHy29mkIzhkx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from hmmlearn import hmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAkSr9aczhix"
      },
      "source": [
        "# Prepare parameters for a 4-components HMM Initial population probability\n",
        "\n",
        "startprob = np.array([0.6, 0.3, 0.1, 0.0])\n",
        "\n",
        "# The transition matrix, note that there are no transitions possible\n",
        "# between component 1 and 3\n",
        "transmat = np.array([[0.7, 0.2, 0.0, 0.1],\n",
        "                     [0.3, 0.5, 0.2, 0.0],\n",
        "                     [0.0, 0.3, 0.5, 0.2],\n",
        "                     [0.2, 0.0, 0.2, 0.6]])\n",
        "\n",
        "# The means of each component\n",
        "means = np.array([[0.0,  0.0],\n",
        "                  [0.0, 11.0],\n",
        "                  [9.0, 10.0],\n",
        "                  [11.0, -1.0]])\n",
        "\n",
        "# The covariance of each component\n",
        "covars = .5 * np.tile(np.identity(2), (4, 1, 1))\n",
        "\n",
        "# Build an HMM instance and set parameters\n",
        "model = hmm.GaussianHMM(n_components=4, covariance_type=\"full\")\n",
        "\n",
        "# Instead of fitting it from the data, we directly set the estimated\n",
        "# parameters, the means and covariance of the components\n",
        "model.startprob_ = startprob\n",
        "model.transmat_ = transmat\n",
        "model.means_ = means\n",
        "model.covars_ = covars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGh3fddEzheA"
      },
      "source": [
        "# Generate samples\n",
        "X, Z = model.sample(500)\n",
        "\n",
        "# Plot the sampled data\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(X[:, 0], X[:, 1], \".-\", label=\"observations\", ms=6,\n",
        "         mfc=\"orange\", alpha=0.7)\n",
        "\n",
        "# Indicate the component numbers\n",
        "for i, m in enumerate(means):\n",
        "    plt.text(m[0], m[1], 'Component %i' % (i + 1),\n",
        "             size=17, horizontalalignment='center',\n",
        "             bbox=dict(alpha=.7, facecolor='w'))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRnwQ2qvOPG9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO3Qb8zEOPEh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cA3MktRT7Vr"
      },
      "source": [
        "#### <font color=green>**3.4.** </font> HMMlearn 公式チュートリアル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Irl1Gth0vlxK"
      },
      "source": [
        "# hmmlearn Tutorial\n",
        "# https://hmmlearn.readthedocs.io/en/latest/tutorial.html\n",
        "\n",
        "# 日本語解説記事 :\n",
        "# http://keik-117.hatenablog.com/entry/2016/07/05/213903\n",
        "# https://qiita.com/ryo-ma/items/ac26c78cf8ff99bc329c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0afMzQKFvlvf"
      },
      "source": [
        "## Building HMM and generating samples\n",
        "\n",
        "import numpy as np\n",
        "from hmmlearn import hmm\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlfINgp4vltv"
      },
      "source": [
        "# You can build a HMM instance by passing the parameters described above to the constructor.\n",
        "# Then, you can generate samples from the HMM by calling sample().\n",
        "\n",
        "model = hmm.GaussianHMM(n_components=3,\n",
        "                        covariance_type=\"full\")\n",
        "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
        "model.transmat_ = np.array([[0.7, 0.2, 0.1],\n",
        "                            [0.3, 0.5, 0.2],\n",
        "                            [0.3, 0.3, 0.4]])\n",
        "model.means_ = np.array([[0.0, 0.0],\n",
        "                          [3.0, -3.0],\n",
        "                          [5.0, 10.0]])\n",
        "model.covars_ = np.tile(np.identity(2),\n",
        "                        (3, 1, 1))\n",
        "X, Z = model.sample(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0sPqSO85J3k"
      },
      "source": [
        "'''GaussianHMM\n",
        "Covariance parameters shape depends on covariance_type:\n",
        "\n",
        "(n_components, )                        if \"spherical\",\n",
        "(n_components, n_features)              if \"diag\",\n",
        "(n_components, n_features, n_features)  if \"full\"\n",
        "(n_features, n_features)                if \"tied\",\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waQ-v1BAvlsB"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj3L-2cvvlqY"
      },
      "source": [
        "Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSp8R-3avloq"
      },
      "source": [
        "# The transition probability matrix need not to be ergodic.\n",
        "# For instance, a left-right HMM can be defined as follows:\n",
        "\n",
        "lr = hmm.GaussianHMM(n_components=3,\n",
        "                     covariance_type=\"diag\",\n",
        "                     init_params=\"cm\",\n",
        "                     params=\"cmt\")\n",
        "lr.startprob_ = np.array([1.0, 0.0, 0.0])\n",
        "lr.transmat_ = np.array([[0.5, 0.5, 0.0],\n",
        "                         [0.0, 0.5, 0.5],\n",
        "                         [0.0, 0.0, 1.0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFk1V6hGvlnE"
      },
      "source": [
        "# If any of the required parameters are missing,\n",
        "# sample() will raise an exception:\n",
        "\n",
        "model = hmm.GaussianHMM(n_components=3)\n",
        "X, Z = model.sample(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IQwCJZNvllC"
      },
      "source": [
        "'''Each HMM parameter has a character code \n",
        " which can be used to customize its initialization and estimation.\n",
        "The EM algorithm needs a starting point to proceed,\n",
        " thus prior to training each parameter is assigned a value either\n",
        "  random or computed from the data. \n",
        "It is possible to hook into this process and provide a starting point explicitly. \n",
        "To do so\n",
        "1. ensure that the character code for the parameter is missing from init_params\n",
        " and then\n",
        "2. set the parameter to the desired value.\n",
        "\n",
        "For example, consider a HMM with an explicitly initialized transition probability matrix:\n",
        "'''\n",
        "\n",
        "model = hmm.GaussianHMM(n_components=3,\n",
        "                        n_iter=100,\n",
        "                        init_params=\"mcs\")\n",
        "model.transmat_ = np.array([[0.7, 0.2, 0.1],\n",
        "                            [0.3, 0.5, 0.2],\n",
        "                            [0.3, 0.3, 0.4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogjoLOovlhj"
      },
      "source": [
        "# A similar trick applies to parameter estimation.\n",
        "# If you want to fix some parameter at a specific value,\n",
        "# remove the corresponding character from params and set the parameter value before training."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLe6Eyd4zhaI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZmepBx9zhWi"
      },
      "source": [
        "## Training HMM parameters and inferring the hidden states\n",
        "\n",
        "# This time, the input is a single sequence of observed values.\n",
        "# Note, the states in remodel will have a different order than those in the generating model.\n",
        "\n",
        "remodel = hmm.GaussianHMM(n_components=3,\n",
        "                          covariance_type=\"full\",\n",
        "                          n_iter=100)\n",
        "remodel.fit(X)\n",
        "Z2 = remodel.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV4fqwW-zhPR"
      },
      "source": [
        "Z2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAnNqRGV1UW2"
      },
      "source": [
        "## Monitoring convergence\n",
        "# You can use the monitor_ attribute to diagnose convergence:\n",
        "\n",
        "remodel.monitor_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFeC9kPb1ZQk"
      },
      "source": [
        "remodel.monitor_.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX5KKyDvzhNR"
      },
      "source": [
        "remodel.monitor_.converged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBA7gLoTzhK4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiGl5A6hzhHq"
      },
      "source": [
        "## Working with multiple sequences\n",
        "\n",
        "# Consider two 1D sequences:\n",
        "X1 = [[0.5], [1.0], [-1.0], [0.42], [0.24]]\n",
        "X2 = [[2.4], [4.2], [0.5], [-0.24]]\n",
        "\n",
        "# To pass both sequences to fit() or predict(), first concatenate them\n",
        "# into a single array and then compute an array of sequence lengths:\n",
        "X = np.concatenate([X1, X2])\n",
        "lengths = [len(X1), len(X2)]\n",
        "\n",
        "# Finally, just call the desired method with X and lengths:\n",
        "hmm.GaussianHMM(n_components=3).fit(X, lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYmN-qXAzhF2"
      },
      "source": [
        "## Saving and loading HMM\n",
        "# After training, a HMM can be easily persisted for future use with the standard pickle module:\n",
        "\n",
        "import pickle\n",
        "with open(\"filename.pkl\", \"wb\") as file: pickle.dump(remodel, file)\n",
        "with open(\"filename.pkl\", \"rb\") as file: pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCiz2DDT4anH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZtKmJtH4ako"
      },
      "source": [
        "# Use custom convergence criteria by subclassing ConvergenceMonitor\n",
        "# and redefining the converged method.\n",
        "# The resulting subclass can be used by creating an instance\n",
        "# and pointing a model’s monitor_ attribute to it prior to fitting.\n",
        "\n",
        "from hmmlearn.base import ConvergenceMonitor\n",
        "from hmmlearn import hmm\n",
        "\n",
        "class ThresholdMonitor(ConvergenceMonitor):\n",
        "  @property\n",
        "  def converged(self):\n",
        "    return (self.iter == self.n_iter or \n",
        "            self.history[-1] >= self.tol)\n",
        "\n",
        "model = hmm.GaussianHMM(n_components=2, tol=5, verbose=True)\n",
        "model.monitor_ = ThresholdMonitor(model.monitor_.tol,\n",
        "                                  model.monitor_.n_iter,\n",
        "                                  model.monitor_.verbose)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a43jZvkI4ahq"
      },
      "source": [
        "from hmmlearn.hmm import GaussianHMM\n",
        "GaussianHMM(n_components=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elazIIwZ4ae6"
      },
      "source": [
        "'''GMMHMM\n",
        "Covariance parameters shape depends on covariance_type:\n",
        "\n",
        "(n_components, n_mix)                          if \"spherical\",\n",
        "(n_components, n_mix, n_features)              if \"diag\",\n",
        "(n_components, n_mix, n_features, n_features)  if \"full\"\n",
        "(n_components, n_features, n_features)         if \"tied\",\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5leR3Mgo51g-"
      },
      "source": [
        "from hmmlearn.hmm import MultinomialHMM\n",
        "MultinomialHMM(n_components=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9INSQJCdOPCi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2oBLZ9SOO_P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1k3jTRnOKLZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBa2R-_BT7tj"
      },
      "source": [
        "#### <font color=green>**3.5.** </font> 自然言語処理への適用例 : 品詞推定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DzHsAYl9De8"
      },
      "source": [
        "## 出典 : \n",
        "# https://www.kabuku.co.jp/developers/hmm\n",
        "# https://github.com/takafumihoriuchi/natural_language_processing/blob/master/viterbi_pos_estimate.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryc_ht6c6JPj"
      },
      "source": [
        "\"\"\"Created on May 17, 2018\n",
        "@author: Takafumi Horiuchi\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfBOsaJF6JMf"
      },
      "source": [
        "# for POS estimation\n",
        "import numpy as np\n",
        "import nltk\n",
        "import math\n",
        "\n",
        "# for printing progress-bar\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6MBAs4D6JKE"
      },
      "source": [
        "# remove sentence boundaries from raw bigrams made by nltk\n",
        "def make_tagged_word_bigrams(sents):\n",
        "  return filter(lambda x: x != (('_end', '</s>'), ('start_', '<s>')),\n",
        "                nltk.bigrams(make_sent_words(sents)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cth8uqo36JGo"
      },
      "source": [
        "# remake word list\n",
        "def make_sent_words(sents):\n",
        "  words = []\n",
        "  for i in range(len(sents)):\n",
        "    words += mod_sent(sents[i])\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEAUilOl6W6h"
      },
      "source": [
        "# add dummy tokens (for beginning and ending) to each sentence\n",
        "def mod_sent(tokens):\n",
        "  tokens.insert(0, ('start_', '<s>'))\n",
        "  tokens.append(('_end', '</s>'))\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIhHekhq6W2M"
      },
      "source": [
        "# p(w|t)\n",
        "# word emission probability with add-α smoothing\n",
        "def p_t_w(t_w, tag, word, alpha=0.01):\n",
        "  return (t_w[tag][word] + alpha ) / (t_w[tag].N() + alpha * t_w[tag].B())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkyhfWHY6Wzz"
      },
      "source": [
        "# p(t_i|t_i-1)\n",
        "# POS transition probability with add-α smoothing\n",
        "def p_t_t(t_t, tag1, tag2, alpha=0.01):\n",
        "  return (t_t[tag1][tag2] + alpha ) / (t_t[tag1].N() + alpha * t_t[tag1].B())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayEl7_gV6eYm"
      },
      "source": [
        "# helper method of viterbi()\n",
        "def calc_table(S, T, V, i, j, pos_tags, tokens, t_w, t_t):\n",
        "  max_prob = -np.inf\n",
        "  max_k = 0\n",
        "  for k in range(S):\n",
        "    prob = V[k][i-1] + math.log(p_t_w(t_w, pos_tags[j], tokens[i])) + math.log(p_t_t(t_t, pos_tags[k], pos_tags[j]))\n",
        "    if prob > max_prob:\n",
        "      max_prob, max_k = prob, k\n",
        "  return max_prob, max_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntmc7AhG6eVC"
      },
      "source": [
        "def viterbi(sentence, pos_tags, t_w, t_t):\n",
        "  tokens = nltk.word_tokenize(sentence)   # ['Time', 'flies', 'like', 'an', 'arrow', '.']\n",
        "  tokens.insert(0, '<s>')                 # ['<s>', 'Time', 'flies', 'like', 'an', 'arrow', '.']\n",
        "    \n",
        "  S = len(pos_tags)                       # S: number of POS (47)\n",
        "  T = len(tokens)                         # T: number of tokens\n",
        "  V = np.zeros((S, T), dtype=np.float32)  # V: probability table\n",
        "  B = np.zeros((S, T), dtype=int)         # B: back-pointer table\n",
        "\n",
        "  ## induction\n",
        "  for i in range(1, T):\n",
        "    for j in range(S):\n",
        "      V[j][i], B[j][i] = calc_table(S, T, V, i, j, pos_tags, tokens, t_w, t_t)\n",
        "\n",
        "  ## termination and path-readout\n",
        "  X = np.zeros((T), dtype=int)\n",
        "  max_prob = -np.inf\n",
        "  for j in range(S):\n",
        "    if V[j][T-1] > max_prob:\n",
        "      max_prob = V[j][T-1]\n",
        "      X[T-1] = j\n",
        "  for i in range(T-2, -1, -1):\n",
        "    X[i] = B[X[i+1]][i+1]\n",
        "\n",
        "  # convert POS-index to POS-tag\n",
        "  pos_seq = []\n",
        "  for pos_idx in X:\n",
        "    pos_seq.append(pos_tags[pos_idx])\n",
        "\n",
        "  return list(zip(tokens[1:], pos_seq[1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnorXx6i6eSa"
      },
      "source": [
        "def setup_progbar(width):\n",
        "  sys.stdout.write(\"[%s]\" % (\" \" * width))\n",
        "  sys.stdout.flush()\n",
        "  sys.stdout.write(\"\\b\" * (width+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk5SWEUP6xy2"
      },
      "source": [
        "def update_progbar():\n",
        "  sys.stdout.write(\"=\")\n",
        "  sys.stdout.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8vjTPw16xvN"
      },
      "source": [
        "def calc_accuracy(tagged_sents, pos_tags, t_w, t_t):\n",
        "  ## setup progress bar\n",
        "  test_size = len(tagged_sents)\n",
        "  max_width = 78\n",
        "  progbar_width = test_size if test_size < max_width else max_width\n",
        "  setup_progbar(progbar_width)\n",
        "  prog_step = test_size / progbar_width\n",
        "  prog_cnt = progbar_width\n",
        "\n",
        "  ## create test sentences from 'tagged_sents_test'\n",
        "  test_sent_list = []\n",
        "  ans_tagged_sents = []\n",
        "  for each_tagged_sent in tagged_sents:\n",
        "    sentence = []\n",
        "    for each_tagged_word in each_tagged_sent:\n",
        "      sentence.append(each_tagged_word[0])\n",
        "    test_sent_list.append(\" \".join(str(x) for x in sentence))\n",
        "    ans_tagged_sents.append(each_tagged_sent)\n",
        "\n",
        "  ## setup dictionary for POS specific accuracy\n",
        "  pos_accuracy = dict()\n",
        "  for pos in pos_tags:\n",
        "    pos_accuracy[pos] = {'correct': 0, 'total': 0, 'accuracy': 0.0}\n",
        "\n",
        "  ## evaluate created HMM\n",
        "  total_word_cnt = 0\n",
        "  correct_word_cnt = 0\n",
        "  correct_sent_cnt = 0\n",
        "  for sentence, answer in zip(test_sent_list, ans_tagged_sents):\n",
        "    token_pos = viterbi(sentence, pos_tags, t_w, t_t)\n",
        "    all_pos_matched = True\n",
        "    for pred, ans in zip(token_pos, answer):\n",
        "      if (pred[1] == ans[1]):\n",
        "        correct_word_cnt += 1\n",
        "        pos_accuracy[pred[1]]['correct'] += 1\n",
        "      else:\n",
        "        all_pos_matched = False\n",
        "      total_word_cnt += 1\n",
        "      pos_accuracy[ans[1]]['total'] += 1\n",
        "    if all_pos_matched is True:\n",
        "      correct_sent_cnt += 1\n",
        "\n",
        "    # update progress bar\n",
        "    prog_cnt += 1\n",
        "    if (prog_cnt >= prog_step):\n",
        "      update_progbar()\n",
        "      prog_cnt = 0\n",
        "\n",
        "  # calculate POS specific accuracy\n",
        "  for pos in pos_tags:\n",
        "    if pos_accuracy[pos]['total'] == 0:\n",
        "      pos_accuracy[pos]['accuracy'] = None\n",
        "      continue\n",
        "    pos_accuracy[pos]['accuracy'] = pos_accuracy[pos]['correct'] / pos_accuracy[pos]['total']\n",
        "    \n",
        "  accuracy_token = correct_word_cnt / total_word_cnt\n",
        "  accuracy_sent = correct_sent_cnt / len(ans_tagged_sents)\n",
        "  return accuracy_token, accuracy_sent, pos_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da4WAb2J73zU"
      },
      "source": [
        "print(\"\\n+------------------------------------------------------------------------------+\\n\",\n",
        "      \"This is a HMM based POS estimator created by Takafumi Horiuchi in May of 2018.\\n\",\n",
        "      \"Input of the sentence \\\"We choose to go to the Moon.\\\" could output the following:\\n\",\n",
        "      \"[('We', 'PRP'), ('choose', 'VB'), ('to', 'TO'), ('go', 'VB'), ('to', 'TO'), ('the', 'DT'), ('moon', 'NN'), ('.', '.')]\\n\",\n",
        "      \"\\nloading POS tagsets (may consume few seconds) ...\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMmPV0QU73xC"
      },
      "source": [
        "## load POS tagset from Penn Treebank\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "tagged_sents = nltk.corpus.treebank.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jeSqqM73uJ"
      },
      "source": [
        "## split tagset to train and test\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(tagged_sents) * train_ratio)\n",
        "tagged_sents_train = tagged_sents[:train_size]\n",
        "tagged_sents_test = tagged_sents[train_size:]\n",
        "\n",
        "tagged_word_bigrams = list(make_tagged_word_bigrams(tagged_sents_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TBVGHrT7_z0"
      },
      "source": [
        "## word emission count (t_w[tag][word])\n",
        "t_w = nltk.ConditionalFreqDist([(d[0][1], d[0][0]) for d in tagged_word_bigrams])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psqy3D2J7_xY"
      },
      "source": [
        "## state transition count (t_t[t1][t2])\n",
        "t_t = nltk.ConditionalFreqDist([(d[0][1], d[1][1]) for d in tagged_word_bigrams])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGMlcyGe8DYZ"
      },
      "source": [
        "## a list of possible pos tags (</s> is not included)\n",
        "pos_tags = list(t_t.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6QLg5N8DWV"
      },
      "source": [
        "## sentence to evaluate POS\n",
        "\n",
        "#sentence = input(\"input a sentence: \")\n",
        "sentence = \"I have a pen.\"  ###\n",
        "\n",
        "token_pos = viterbi(sentence, pos_tags, t_w, t_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSHPQgzrk-bZ"
      },
      "source": [
        "#######\n",
        "# すごく時間かかる\n",
        "#######\n",
        "\n",
        "## show results\n",
        "print(\"POS estimation result:\")\n",
        "for each_token_pos in token_pos:\n",
        "  print(each_token_pos)\n",
        "\n",
        "  print(\"\\nConditions: Penn-Treebank as POS tagset; train : test = %.2f : %.2f\" % (train_ratio, 1 - train_ratio))\n",
        "  print(\"--------------------------------------------------------------------------------\")\n",
        "\n",
        "  ## test model precision\n",
        "  print(\"measuring precision of model (may consume several minutes) ...\")\n",
        "  prec_token, prec_sent, pos_acc = calc_accuracy(tagged_sents_test, pos_tags, t_w, t_t)\n",
        "  print(\"\\nmodel precision\")\n",
        "  print(\"token based accuracy    :\", prec_token)\n",
        "  print(\"sentence based accuracy :\", prec_sent)   \n",
        "  print(\"\\nPOS specific accuracy   :\")\n",
        "\n",
        "  for each_pos in pos_tags:\n",
        "    print(each_pos, \"\\t---\\t\", pos_acc[each_pos])\n",
        "    \n",
        "  print(\"+------------------------------------------------------------------------------+\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofLfKAJAW3Zi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU80yDeXW3We"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGUepPo5W3TN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIPhJuyzXeQs"
      },
      "source": [
        "#### <font color=green>**3.6.** </font> HDP-HMM（階層ディリクレ過程隠れマルコフモデル）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeUuoq3_Xbri"
      },
      "source": [
        "# ライブラリ : https://github.com/bnpy/bnpy\n",
        "\n",
        "# サンプルコード : https://bnpy.readthedocs.io/en/latest/examples/08_mocap6/plot-02-demo=merge_moves_for_hdphmm.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ipSooSQZ-79"
      },
      "source": [
        "!git clone https://github.com/bnpy/bnpy.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3i2q6q1aEnj"
      },
      "source": [
        "!pip install -e /content/bnpy/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DkAjX4VYFjB"
      },
      "source": [
        "## install成功したらruntimeを再起動させる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gAM_Ts1WJL6"
      },
      "source": [
        "## Merge moves with HDP-HMM\n",
        "# https://bnpy.readthedocs.io/en/latest/examples/08_mocap6/plot-02-demo=merge_moves_for_hdphmm.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5lBmANYWI1x"
      },
      "source": [
        "import bnpy\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from matplotlib import pylab\n",
        "import seaborn as sns\n",
        "\n",
        "FIG_SIZE = (10, 5)\n",
        "pylab.rcParams['figure.figsize'] = FIG_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H50t8fCYFoH"
      },
      "source": [
        "# Setup: Load data\n",
        "\n",
        "dataset_path = os.path.join(bnpy.DATASET_PATH, 'mocap6')\n",
        "dataset = bnpy.data.GroupXData.read_npz(\n",
        "    os.path.join(dataset_path, 'dataset.npz'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6CI6Tz1eHs8"
      },
      "source": [
        "'''データの説明 : https://github.com/bnpy/bnpy/tree/master/bnpy/datasets/mocap6\n",
        "\n",
        "Six sequences were collected from files available at mocap.cs.cmu.edu:\n",
        "  Subject 13: trials 29, 30, and 31 Subject 14: trials 6, 14, and 20\n",
        "\n",
        "Each of the six sequences has been annotated to indicate which of a set of 12 possible exercises is being performed at each timestep.\n",
        "\n",
        "The raw AMC mocap sensor data from these sequences was post-processed as follows:\n",
        "  - 12 sensor channels were kept as representative of gross motor behavior. Remaining channels were discarded.\n",
        "  - Each sensor channel was adjusted to have zero-mean.\n",
        "  - Each channel was block-averaged to a final frame rate of 10 fps (down from 120 fps in the raw data).\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roDu1pPCaeuv"
      },
      "source": [
        "# Setup: Initialization hyperparameters\n",
        "\n",
        "init_kwargs = dict(\n",
        "    K=20,\n",
        "    initname='randexamples',\n",
        "    )\n",
        "\n",
        "alg_kwargs = dict(\n",
        "    nLap=29,\n",
        "    nTask=1, nBatch=1, convergeThr=0.0001,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeeFXQr4aerr"
      },
      "source": [
        "# Setup: HDP-HMM hyperparameters\n",
        "\n",
        "hdphmm_kwargs = dict(\n",
        "    gamma = 5.0,       # top-level Dirichlet concentration parameter\n",
        "    transAlpha = 0.5,  # trans-level Dirichlet concentration parameter\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8dxLbAHbbp0"
      },
      "source": [
        "# Setup: Gaussian observation model hyperparameters\n",
        "\n",
        "gauss_kwargs = dict(\n",
        "    sF = 1.0,          # Set prior so E[covariance] = identity\n",
        "    ECovMat = 'eye',\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzugQPj_bbmW"
      },
      "source": [
        "# All-Pairs : Try all possible pairs of merges every 10 laps\n",
        "\n",
        "allpairs_merge_kwargs = dict(\n",
        "    m_startLap = 10,\n",
        "    # Set limits to number of merges attempted each lap.\n",
        "    # This value specifies max number of tries for each cluster\n",
        "    # Setting this very high (to 50) effectively means try all pairs\n",
        "    m_maxNumPairsContainingComp = 50,\n",
        "    # Set \"reactivation\" limits\n",
        "    # So that each cluster is eligible again after 10 passes thru dataset\n",
        "    # Or when it's size changes by 400%\n",
        "    m_nLapToReactivate = 10,\n",
        "    m_minPercChangeInNumAtomsToReactivate = 400 * 0.01,\n",
        "    # Specify how to rank pairs (determines order in which merges are tried)\n",
        "    # 'total_size' and 'descending' means try largest combined clusters first\n",
        "    m_pair_ranking_procedure = 'total_size',\n",
        "    m_pair_ranking_direction = 'descending',\n",
        "    )\n",
        "\n",
        "allpairs_trained_model, allpairs_info_dict = bnpy.run(\n",
        "    dataset, 'HDPHMM', 'DiagGauss', 'memoVB',\n",
        "    output_path='/tmp/mocap6/trymerge-K=20-model=HDPHMM+DiagGauss-ECovMat=1*eye-merge_strategy=all_pairs/',\n",
        "    moves='merge,shuffle',\n",
        "    **dict(\n",
        "        sum(map(list,   [alg_kwargs.items(),\n",
        "                        init_kwargs.items(),\n",
        "                        hdphmm_kwargs.items(),\n",
        "                        gauss_kwargs.items(),\n",
        "                        allpairs_merge_kwargs.items()]),[]))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt-UaB5jbbiq"
      },
      "source": [
        "# Large-Pairs : Try 5-largest-size pairs of merges every 10 laps\n",
        "\n",
        "largepairs_merge_kwargs = dict(\n",
        "    m_startLap = 10,\n",
        "    # Set limits to number of merges attempted each lap.\n",
        "    # This value specifies max number of tries for each cluster\n",
        "    m_maxNumPairsContainingComp = 5,\n",
        "    # Set \"reactivation\" limits\n",
        "    # So that each cluster is eligible again after 10 passes thru dataset\n",
        "    # Or when it's size changes by 400%\n",
        "    m_nLapToReactivate = 10,\n",
        "    m_minPercChangeInNumAtomsToReactivate = 400 * 0.01,\n",
        "    # Specify how to rank pairs (determines order in which merges are tried)\n",
        "    # 'total_size' and 'descending' means try largest size clusters first\n",
        "    m_pair_ranking_procedure = 'total_size',\n",
        "    m_pair_ranking_direction = 'descending',\n",
        "    )\n",
        "\n",
        "\n",
        "largepairs_trained_model, largepairs_info_dict = bnpy.run(\n",
        "    dataset, 'HDPHMM', 'DiagGauss', 'memoVB',\n",
        "    output_path='/tmp/mocap6/trymerge-K=20-model=HDPHMM+DiagGauss-ECovMat=1*eye-merge_strategy=large_pairs/',\n",
        "    moves='merge,shuffle',\n",
        "    **dict(\n",
        "        sum(map(list,   [alg_kwargs.items(),\n",
        "                        init_kwargs.items(),\n",
        "                        hdphmm_kwargs.items(),\n",
        "                        gauss_kwargs.items(),\n",
        "                        largepairs_merge_kwargs.items()]),[])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI49a1Y5bbcu"
      },
      "source": [
        "# Good-ELBO-Pairs : Rank pairs of merges by improvement to observation model\n",
        "\n",
        "goodelbopairs_merge_kwargs = dict(\n",
        "    m_startLap = 10,\n",
        "    # Set limits to number of merges attempted each lap.\n",
        "    # This value specifies max number of tries for each cluster\n",
        "    m_maxNumPairsContainingComp = 5,\n",
        "    # Set \"reactivation\" limits\n",
        "    # So that each cluster is eligible again after 10 passes thru dataset\n",
        "    # Or when it's size changes by 400%\n",
        "    m_nLapToReactivate = 10,\n",
        "    m_minPercChangeInNumAtomsToReactivate = 400 * 0.01,\n",
        "    # Specify how to rank pairs (determines order in which merges are tried)\n",
        "    # 'obsmodel_elbo' means rank pairs by improvement to observation model ELBO\n",
        "    m_pair_ranking_procedure = 'obsmodel_elbo',\n",
        "    m_pair_ranking_direction = 'descending',\n",
        "    )\n",
        "\n",
        "\n",
        "goodelbopairs_trained_model, goodelbopairs_info_dict = bnpy.run(\n",
        "    dataset, 'HDPHMM', 'DiagGauss', 'memoVB',\n",
        "    output_path='/tmp/mocap6/trymerge-K=20-model=HDPHMM+DiagGauss-ECovMat=1*eye-merge_strategy=good_elbo_pairs/',\n",
        "    moves='merge,shuffle',\n",
        "    **dict(\n",
        "        sum(map(list,   [alg_kwargs.items(),\n",
        "                        init_kwargs.items(),\n",
        "                        hdphmm_kwargs.items(),\n",
        "                        gauss_kwargs.items(),\n",
        "                        goodelbopairs_merge_kwargs.items()]),[])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sanAGBTKaeoH"
      },
      "source": [
        "# Compare loss function vs wallclock time\n",
        "\n",
        "pylab.figure()\n",
        "for info_dict, color_str, label_str in [\n",
        "        (allpairs_info_dict, 'k', 'all_pairs'),\n",
        "        (largepairs_info_dict, 'g', 'large_pairs'),\n",
        "        (goodelbopairs_info_dict, 'b', 'good_elbo_pairs')]:\n",
        "    pylab.plot(\n",
        "        info_dict['elapsed_time_sec_history'],\n",
        "        info_dict['loss_history'],\n",
        "        '.-',\n",
        "        color=color_str,\n",
        "        label=label_str)\n",
        "pylab.legend(loc='upper right')\n",
        "pylab.xlabel('elapsed time (sec)')\n",
        "pylab.ylabel('loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtotm5TGXbor"
      },
      "source": [
        "# Compare number of active clusters vs wallclock time\n",
        "\n",
        "pylab.figure()\n",
        "for info_dict, color_str, label_str in [\n",
        "        (allpairs_info_dict, 'k', 'all_pairs'),\n",
        "        (largepairs_info_dict, 'g', 'large_pairs'),\n",
        "        (goodelbopairs_info_dict, 'b', 'good_elbo_pairs')]:\n",
        "    pylab.plot(\n",
        "        info_dict['elapsed_time_sec_history'],\n",
        "        info_dict['K_history'],\n",
        "        '.-',\n",
        "        color=color_str,\n",
        "        label=label_str)\n",
        "pylab.legend(loc='upper right')\n",
        "pylab.xlabel('elapsed time (sec)')\n",
        "pylab.ylabel('num. clusters (K)')\n",
        "\n",
        "pylab.show(block=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycHax3nMXbla"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y-R0jQnXbiQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}