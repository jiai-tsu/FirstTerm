{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training_20210105.ipynb","provenance":[{"file_id":"https://gist.github.com/jiaiaraki/bf6576d98b1a88edabce24d6f2a7289a#file-training_20201222-ipynb","timestamp":1609788354982}],"collapsed_sections":["jJzACJGg3fSX","sFH_BoOslwlf","msACi9uVx72q","Cy3PJupCneXt","SCx804x7pPXY","oNm50GKgpH-a","LnBN_161-IaJ","ok9nms5N-avy","BlJkSHEc-yNw","J-L97_Yy_vIv","KsUx4NliAFha","-jUONoG7AwDE","_d2Mj-PxkTKC","TBQu8YOYBWTF","AGUY2jd7BtWO","tY9a7biuBpIc","K7eJdJWXDsX1","ecRKgvIlD-Hg","9pXGyr6gFBvQ","5D4ajI3bCF2V","PyUtgsLhKKAO","GAPpdrg8KZnq","30glNtLO7hH7","75muI_FIKo2M"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jJzACJGg3fSX"},"source":["## 前回（20201222）のおさらい"]},{"cell_type":"markdown","metadata":{"id":"sFH_BoOslwlf"},"source":["### 21. データ解析を行う際のTips"]},{"cell_type":"markdown","metadata":{"id":"msACi9uVx72q"},"source":["#### <font color = blue>**1.** </font>EDA (探索的データ解析)"]},{"cell_type":"code","metadata":{"id":"JIvQ9uZ9KX1M"},"source":["## irisの例で実演\n","# データを読み込む\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","# DataFrame形式に変換\n","import pandas as pd\n","iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhtyhu5EfKTI","collapsed":true},"source":["'''\n","# データの先頭表示\n","iris_df.head(10)\n","\n","# データの末尾表示\n","iris_df.tail()\n","\n","# データの要約表示\n","iris_df.info()\n","\n","# データの次元数（何行/何列）表示\n","print('There are {} rows and {} columns in data'.format(iris_df.shape[0], iris_df.shape[1]))\n","\n","# データのカラム（列）名を取得\n","iris_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLGxqAVajlwN"},"source":["'''\n","# 後々利用する可能性があるので、型タイプに合わせたカラム名のリストを保持しておくと便利かも\n","# includeを型タイプ（int, float64, etc...）にすることも可能\n","\n","obj_columns = iris_df.select_dtypes(include=['object']).columns\n","print(\"obj_columns : \", obj_columns, \"\\n\")\n","\n","num_columns = iris_df.select_dtypes(include=['number']).columns\n","print(\"num_columns : \\n\", num_columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"frmgbQnzfKJ5"},"source":["'''\n","# 各列データの欠損値の個数をカウント\n","iris_df.isnull().sum()\n","\n","# 各列データの型タイプ確認\n","iris_df.dtypes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wo8zmOVmlt3h"},"source":["## データを図示して各特徴量の分布を確認\n","# 外れ値の有無/割合など\n","\n","# 各列（特徴量）のヒストグラムの場合\n","import matplotlib.pyplot as plt\n","iris_df.hist(bins=10, figsize=(10,10))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OD2M35Tilyo8"},"source":["## 相関行列のヒートマップ\n","import seaborn as sns\n","sns.heatmap(iris_df.corr(),\n","            annot=True,\n","            cmap='YlGnBu')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFIyCAIgmxqj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sW0pS-YS4dpd"},"source":["##### pandas_profiling"]},{"cell_type":"code","metadata":{"id":"ojATeJ1NoJ0R"},"source":["## なお、ここまでの作業をほぼ全て自動でまとめてくれる便利なライブラリが公開されている"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZYtXmIJRzKS"},"source":["!pip show pandas_profiling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHMm0MkeTrEz"},"source":["!pip install git+https://github.com/pandas-profiling/pandas-profiling.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpjmvlEoRzOA"},"source":["#!pip install pandas_profiling\n","from pandas_profiling import ProfileReport\n","\n","#import warnings\n","#warnings.filterwarnings('ignore')\n","\n","profile = ProfileReport(iris_df)\n","profile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9ljiSlpok3l"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cy3PJupCneXt"},"source":["#### <font color = blue>**2.** </font>Feature Engineering (特徴量エンジニアリング / 特徴選択）"]},{"cell_type":"code","metadata":{"id":"rSAlyz1-mZJE"},"source":["## 特徴量の重要度の推定\n","# どの特徴量（説明変数）が、target（目的変数）にとって重要であるかを、ランダムフォレスト(RandomForest)を用いて算出する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OAKt_QW00KlM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fa1Dv5zfzUKD"},"source":["## 外れ値処理\n","# 特徴量に外れ値が含まれている場合、一般的にモデルの精度に影響しやすいので、外れ値処理は必須\n","# データ自体を使わない（切り捨てる）、規格化/正規化/標準化する、など"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLxlXVU60K58"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyPBp4gDzTJE"},"source":["## target（目的変数）の分布を確認\n","# 正規分布にしたがっているかどうか\n","# 目的変数が正規分布に従っているか否かは、一般的に機械学習モデルに大きく影響するため重要\n","# 対数変換および差分変換を行ない、正規分布に従うように加工/整形する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i06AA3CgKHim"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCx804x7pPXY"},"source":["#### <font color=red>**advanced task** : </font> 以上に挙げた作業の流れで　Feature Engineering　を行う\n","\n","$\\downarrow \\downarrow$ 適当な化学的データが手元になければこちらでどうぞ $\\downarrow \\downarrow$\\\n","https://raw.githubusercontent.com/jiai-tus/FirstTerm/main/20201222/datasets/Grisoni_et_al_2016_EnvInt88.csv\n","\n","出展 : https://archive.ics.uci.edu/ml/datasets/QSAR+Bioconcentration+classes+dataset"]},{"cell_type":"code","metadata":{"id":"XSh-ZxMVKPYQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCPpcQ1eKPR_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gb1jZ5jKzTGO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNm50GKgpH-a"},"source":["### 22. k近傍法 (k-NearestNeighbor)\n","<font color = blue>**1.** </font>分類 (classification)\n","\n","ライブラリ : sklearn.neighbors.KNeighborsClassifier\n","\n","$\\downarrow \\downarrow$ 公式リファレンス $\\downarrow \\downarrow$\\\n","https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"]},{"cell_type":"markdown","metadata":{"id":"LnBN_161-IaJ"},"source":["#### <font color=green> **1.1.** </font> データの準備"]},{"cell_type":"code","metadata":{"id":"5WAOyNBxoaZH"},"source":["# irisデータをロード\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","# データフレーム形式に変換\n","# カラム名を説明変数に設定\n","import pandas as pd\n","iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n","\n","#iris_df.head( )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9LsgbgoooUv"},"source":["'''\n","# 目的変数を確認\n","iris.target\n","\n","## 目的変数（＝アヤメの種類）は iris.target にセットされているが、数字の状態\n","# 人間が読める(=human-redable)種類名の表記としての目的変数は iris.target_names\n","iris.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adfU5QwTERE4"},"source":["# 目的変数部分だけをデータフレーム形式に変換\n","iris_target_data = pd.DataFrame(iris.target, columns=['Species'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntV5B5LGlbmg"},"source":["'''\n","# データの確認\n","print(iris_df)\n","print(iris_target_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vR7k8I6gFcMp"},"source":["'''\n","## 目的変数を数値 [0 1 2] ではなく種類名 ['setosa' 'versicolor' 'virginica'] としたい場合\n","iris_df2 = pd.DataFrame(iris.data, columns=iris.feature_names)\n","\n","iris_df2['species'] = pd.Categorical.from_codes(iris.target, iris.target_names) \n","\n","print(iris_df2.head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDVoMG5HdEBa"},"source":["## データの前処理（の一例）\n","## 特徴量を抽出/選別 (feature engineering)\n","\n","# 説明変数ごとに、その変数の意味を踏まえて、必要に応じて Data Cleaning\n","# >> 目的変数に対する偏りを補正\n","# >> 非数値であれば何らかのルールに従い数値に変換\n","# >> 欠損値があれば何らかのルールに従い補充\n","# >> 規格化、標準化、正規化、など\n","\n","# 説明変数の次元を圧縮\n","# >> PCAなど\n","\n","## iris はそのまま使用して十分な精度が出るようにすでに整えられたデータセット"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ok9nms5N-avy"},"source":["#### <font color=green> **1.2.** </font> データセットの分割（ホールドアウト）"]},{"cell_type":"code","metadata":{"id":"TChZyMPQ-lZB"},"source":["# ライブラリをインポート\n","from sklearn.model_selection import train_test_split\n","\n","# ホールドアウト実行\n","X_train, X_test, Y_train, Y_test = train_test_split(iris_df, iris_target_data)\n","\n","# デフォルトでは 学習用データ部分 ： テストデータ部分 = 3 : 1 に分かれる (test_size=0.25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iF28fd9pshhx"},"source":["'''\n","# データ型を確認\n","print(\"X_train : \", type(X_train), \"\\n\",\n","      \"X_test : \", type(X_test), \"\\n\",\n","      \"Y_train : \", type(Y_train), \"\\n\",\n","      \"Y_train : \", type(Y_train), \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bRbuEpKqvcf"},"source":["'''\n","# データの中身を見てみる\n","print(X_train, \"\\n\")\n","print(X_test, \"\\n\")\n","print(Y_train, \"\\n\")\n","print(Y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ecuzk0Fv2_L"},"source":["'''\n","# 分割された様子を図示する\n","X_df = X_train.copy()\n","X_df['Species'] = Y_train\n","\n","import seaborn as sns\n","sns.pairplot(X_df, hue='Species', height=2)\n","\n","X_df2 = X_test.copy()\n","X_df2['Species'] = Y_test\n","\n","sns.pairplot(X_df2, hue='Species', height=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aFO_DEtq3U_"},"source":["## 目的変数をDataFrame形式からnumpy配列へ変換\n","# 目的変数が1次元の場合、scikit-learn は行ベクトル（1d array）を推奨してくるため\n","\n","import numpy as np\n","Y2_train = np.array(Y_train.Species)\n","\n","#print(Y2_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlJkSHEc-yNw"},"source":["#### <font color=green> **1.3.** </font> k近傍法を実行"]},{"cell_type":"code","metadata":{"id":"p5u6cZzF_C7l"},"source":["# ライブラリのインポート\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# 試しに k=6 でk近傍法をしてみよう\n","knn = KNeighborsClassifier(n_neighbors=6)\n","\n","# 学習用データのみを与えて .fit 実行\n","knn.fit(X_train, Y2_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GL_DAKfb5Mj7"},"source":["# テスト用データによる予測結果を取得\n","Y_pred = knn.predict(X_test)\n","\n","#Y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbEUqOQl_K4i"},"source":["## どれくらいの精度で予測できているのかを確認しよう\n","# ライブラリのインポート\n","from sklearn import metrics\n","import pandas.testing as tm\n","\n","# 正解（テスト用データの目的変数）に対する予測結果の正答率を計算して表示する\n","metrics.accuracy_score(Y_test, Y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-L97_Yy_vIv"},"source":["#### <font color=green> **1.4.** </font> ハイパーパラメータ k について"]},{"cell_type":"code","metadata":{"id":"20qLc1kp5SUn"},"source":["## k の値によって正答率がどう変わるかを調べよう\n","\n","# 正答率を保存する入れ物を作成\n","accuracy_list = []\n","\n","# k = 1〜100 でk近傍法を行い、それぞれの正答率を取得して保存（格納）\n","k_range = range(1, 101)\n","\n","for k in k_range:\n","  knn = KNeighborsClassifier(n_neighbors=k)\n","  Y2_train = np.array(Y_train.Species)\n","  knn.fit(X_train, Y2_train)\n","  Y_pred = knn.predict(X_test)\n","  accuracy_list.append(metrics.accuracy_score(Y_test, Y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S79pFEcIopde"},"source":["# わかりやすくグラフで図示する\n","import matplotlib.pyplot as plt\n","\n","# 背景や罫線、目盛りスケールを自動でいい感じにしてくれる\n","sns.set()\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(k_range, accuracy_list)\n","plt.xlabel('k-nn')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KsUx4NliAFha"},"source":["#### <font color=green> **1.5.** </font> 交差検証（Cross-validation）とグリッドサーチ(Grid Search) \n","https://scikit-learn.org/stable/modules/cross_validation.html"]},{"cell_type":"code","metadata":{"id":"pVdQYZG6KL1n"},"source":["# 学習用データ/テスト用データ の分割を網羅的に行う\n","# 上記の分割のそれぞれにおいて、学習用データをさらに訓練データ/検証データへ網羅的に分割し、比較/統合することで予測精度を高める\n","\n","## k近傍法の場合、ハイパーパラメータである k に大きく依存するので、もっとも精度が高くなるようにしたい"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4b-Zk9FHKLxq"},"source":["## 目的変数を全種類満遍なく含みつつ、ランダムシャッフルされる分割で行ってみる\n","# ライブラリのインポート\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","## 学習用データ/テスト用データ の分割\n","# 学習用 : テスト用 = 120 : 30 で５通り\n","sss1 = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n","\n","## 学習用データをさらに訓練データ/検証データへ網羅的に分割\n","# 訓練用 : 検証用 = 100 : 20 で6通り\n","sss2 = StratifiedShuffleSplit(n_splits=6, test_size=20, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pnRmr3yol1t"},"source":["## 上記の 5通り × 6通り = 30通り 全てで最適な k の値を検証\n","\n","i = 1\n","plt.figure(figsize=(24, 30))\n","\n","for train_index1, test_index1 in sss1.split(iris_df, iris_target_data):\n","  #print(\"TRAIN:\", train_index1, \"\\n TEST:\", test_index1, \"\\n\")\n","  X_learn, X_test = iris_df.loc[train_index1], iris_df.loc[test_index1]\n","  y_learn, y_test = iris_target_data.loc[train_index1], iris_target_data.loc[test_index1]\n","\n","  for train_index2, test_index2 in sss2.split(X_learn, y_learn):\n","    X_train, X_verify = iris_df.loc[train_index2], iris_df.loc[test_index2]\n","    y_train, y_verify = iris_target_data.loc[train_index2], iris_target_data.loc[test_index2]\n","\n","    # 正答率を保存する入れ物\n","    accuracy_list = []\n","\n","    # k = 1〜20 でk近傍法を行い正答率を取得\n","    k_range = range(1, 21)\n","\n","    for k in k_range:\n","      knn = KNeighborsClassifier(n_neighbors=k)\n","      y2_train = np.array(y_train.Species)\n","      knn.fit(X_train, y2_train)\n","      y_pred = knn.predict(X_verify)\n","      # 網羅的に分割した1つの場合における、訓練用データを学習に使用し検証用データで正答率を算出\n","      accuracy_list.append(metrics.accuracy_score(y_verify, y_pred))\n","\n","    # 背景や罫線、目盛りスケールを自動でいい感じにしてくれる\n","    sns.set()\n","\n","    plt.subplot(5,6,i)\n","    plt.plot(k_range, accuracy_list)\n","    #plt.title((\"TRAIN : \", train_index, \" TEST : \", test_index))\n","    plt.xlabel('k-nn')\n","    plt.ylabel('accuracy')\n","    i += 1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PD70blTCAnta"},"source":["## k=3,4,5 が候補か。。。"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcnUsmaAols-"},"source":["# 学習用/テスト用に網羅的に分割した各場合において、学習用データで学習しテスト用データで正答率を算出\n","\n","i = 1\n","plt.figure(figsize=(25, 6))\n","\n","for train_index, test_index in sss1.split(iris_df, iris_target_data):\n","  #print(\"TRAIN:\", train_index1, \"\\n TEST:\", test_index1, \"\\n\")\n","  X_learn, X_test = iris_df.loc[train_index], iris_df.loc[test_index]\n","  y_learn, y_test = iris_target_data.loc[train_index], iris_target_data.loc[test_index]\n","\n","  # 正答率を保存する入れ物\n","  accuracy_list = []\n","\n","  # k = 2〜6 でk近傍法を行い正答率を取得\n","  k_range = range(2, 7)\n","\n","  for k in k_range:\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    y2_learn = np.array(y_learn.Species)\n","    knn.fit(X_learn, y2_learn)\n","    y_pred = knn.predict(X_test)\n","    accuracy_list.append(metrics.accuracy_score(y_test, y_pred))\n","\n","  # 背景や罫線、目盛りスケールを自動でいい感じにしてくれる\n","  sns.set()\n","\n","  plt.subplot(1,5,i)\n","  plt.plot(k_range, accuracy_list)\n","  #plt.title((\"TRAIN : \", train_index, \" TEST : \", test_index))\n","  plt.xlabel('k-nn')\n","  #plt.ylabel('accuracy')\n","  i += 1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcnAxrFU2-G_"},"source":["## k = 3 or 5 とするのがもっとも予測精度が高い\n","# しかしあくまで手元にある150個のデータに対して、の話\n","# 全く未知の iris のデータに対する予測精度（汎化性能）は、やってみないと誰にもわからない"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoipZxWInz0m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"juYW3pAIq6Iv"},"source":["\n","\n","####\n","## 用意されているライブラリにお任せするなら...\n","####\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCErxQk4nzwj"},"source":["## 交差検証のスコアのみ知りたい場合\n","from sklearn.model_selection import cross_val_score\n","\n","scores_k3 = cross_val_score(KNeighborsClassifier(n_neighbors=3),  # 学習モデルを指定\n","                            iris.data,  # 説明変数のデータ\n","                            iris.target,  # 目的変数のデータ\n","                            cv = 5  # ホールドアウトの分割数。default=5\n","                            )\n","\n","scores_k3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gLS-Y2Hapsm6"},"source":["scores_k5 = cross_val_score(KNeighborsClassifier(n_neighbors=5),  # 学習モデルを指定\n","                            iris.data,  # 説明変数のデータ\n","                            iris.target,  # 目的変数のデータ\n","                            cv = 5  # ホールドアウトの分割数。default=5\n","                            )\n","\n","scores_k5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqXz2IPGqzm4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgbktZxCq1Ig"},"source":["## 交差検証のスコアだけでなく、学習結果（予測モデル）を利用したい場合\n","from sklearn.model_selection import cross_validate\n","\n","cv_results = cross_validate(KNeighborsClassifier(n_neighbors=3),  # 学習モデルを指定\n","                            iris.data,  # 説明変数のデータ\n","                            iris.target,  # 目的変数のデータ\n","                            cv = 5, # ホールドアウトの分割数。default=5\n","                            return_train_score = True,  # 学習データのスコアを返すかどうか。default=False\n","                            return_estimator = True # 予測モデルを返すかどうか。default=False\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zo3Lzpe80SSl"},"source":["'''\n","# データを取り出すkey値一覧を表示\n","sorted(cv_results.keys())\n","\n","# 計算に要した時間。普通は不要\n","print(\"fit_time \\t\\t: \", cv_results['fit_time'])\n","print(\"score_time \\t: \", cv_results['score_time'])\n","\n","# テストデータに対する正答率\n","cv_results['test_score']\n","\n","# 学習データに対する正答率\n","cv_results['train_score']\n","\n","## ホールドアウトの分割ごとに作成された各モデルオブジェクトが、tupleで格納されている\n","cv_results['estimator']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yk5eV_SvzV3v"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrmNxxehvCBx"},"source":["## グリッドサーチ\n","from sklearn.model_selection import GridSearchCV\n","\n","clf = GridSearchCV(KNeighborsClassifier(),  # 学習モデルのメソッドを指定\n","                   cv = 5,  # ホールドアウトの分割数。default=5\n","                   param_grid={\"n_neighbors\" : range(1,21)} # 指定したメソッドの引数名とグリッドサーチしたい範囲をdictionary形式で指定\n","                   )\n","\n","# 教師データを渡して学習実行\n","clf.fit(iris.data, iris.target)                   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttWRsq0AvB-x"},"source":["'''\n","# 結果を取り出すkey値一覧を表示\n","sorted(clf.cv_results_.keys())\n","\n","# 結果一覧\n","clf.cv_results_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmytyHQhvB7Q"},"source":["# 最適な予測モデル\n","clf.best_estimator_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcFzT13HvB3L"},"source":["# 最適な予測モデルの正答率\n","clf.best_score_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jUONoG7AwDE"},"source":["#### <font color=green> **1.6.** </font> 混同行列（Confusion Matrix)"]},{"cell_type":"code","metadata":{"id":"cqy005DfP1iN"},"source":["## k = 3 and 5 の時に混同行列を求めてみる\n","\n","# 改めてホールドアウト\n","X_train, X_test, Y_train, Y_test = train_test_split(iris_df, iris_target_data)\n","\n","# 目的変数を行ベクトル（1d array）化\n","Y2_train = np.array(Y_train.Species)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-PQJCpn8qy7"},"source":["# k = 3 のk近傍法を実行し予測結果取得\n","knn_3 = KNeighborsClassifier(n_neighbors=3)\n","knn_3.fit(X_train, Y2_train)\n","Y_pred_3 = knn_3.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LgIm2B2yP1fk"},"source":["# k = 5 のk近傍法を実行し予測結果を取得 \n","knn_5 = KNeighborsClassifier(n_neighbors=5)\n","knn_5.fit(X_train, Y2_train)\n","Y_pred_5 = knn_5.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cYJAulTd9Bmr"},"source":["# 混同行列を計算してくれるライブラリをインポート\n","from sklearn.metrics import confusion_matrix\n","\n","# k = 3 の混同行列を算出\n","cm_3 = confusion_matrix(Y_test, Y_pred_3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKXRC3qC9Ido"},"source":["# k = 3 の混同行列を表示 or 図示\n","cm_3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7z7SVw39thf"},"source":["sns.heatmap(cm_3, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Onk1yMR3PB6W"},"source":["# k = 5 の混同行列を算出\n","cm_5 = confusion_matrix(Y_test, Y_pred_5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqQOITBG9e9n"},"source":["# k = 5の混同行列を表示 or 図示\n","cm_5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcExFCic9fYL"},"source":["sns.heatmap(cm_5, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xWXqOoAA_7N"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4rgRG1W9EiY"},"source":["# なお、混同行列の計算結果を図示する専用ライブラリがある（seaborn.heatmapと同じ感じ）\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","# k = 3 の混同行列を表示\n","\n","# 念のため各種パラメータをデフォルトに\n","sns.set_style(\"ticks\")\n","\n","print(\"k = 3\")\n","cm_display = ConfusionMatrixDisplay(cm_3, display_labels=iris.target_names).plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDTCcgX5Qs13"},"source":["# k = 5の混同行列を表示\n","\n","# 念のため各種パラメータをデフォルトに\n","sns.set_style(\"ticks\")\n","\n","print(\"k = 5\")\n","cm_display = ConfusionMatrixDisplay(cm_5, display_labels=iris.target_names).plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDjotOMb5OoY"},"source":["## 本日の内容（続きから）\n"]},{"cell_type":"markdown","metadata":{"id":"_d2Mj-PxkTKC"},"source":["### 22. k近傍法 (k-NearestNeighbor)\n","<font color = blue>**2.** </font>回帰 (regression)\n","\n","ライブラリ : sklearn.neighbors.KNeighborsRegressor\n","\n","$\\downarrow \\downarrow$ 公式リファレンス $\\downarrow \\downarrow$\\\n","https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"]},{"cell_type":"markdown","metadata":{"id":"TBQu8YOYBWTF"},"source":["##### <font color=green> **2.1.** </font> データの準備"]},{"cell_type":"code","metadata":{"id":"akYBrTHeEvbn"},"source":["# bostonデータをロード\n","from sklearn.datasets import load_boston\n","boston = load_boston()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoTMi52yEvZB"},"source":["# データフレーム形式に変換\n","# カラム名を説明変数に設定\n","import pandas as pd\n","boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)\n","\n","print(boston_df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkbLy8PfE97i"},"source":["# 目的変数を確認\n","print(boston.target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6ctP_5zE9_z"},"source":["# 説明を表示\n","print(boston.DESCR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hyst3W74L7pl"},"source":["# 目的変数部分だけをデータフレーム形式に変換\n","boston_target_data = pd.DataFrame(boston.target, columns=['MEDV'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"29eVHKDTGr0X"},"source":["# 説明変数 vs 目的変数 の散布図を表示\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(25, 15))\n","sns.set()\n","plt.rcParams['lines.markersize'] = 2\n","\n","i = 1\n","for columns in boston.feature_names:\n","  plt.subplot(3,5,i)\n","  plt.scatter(boston_df[columns], boston_target_data)\n","  plt.xlabel(\"{}\".format(columns))\n","  i += 1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgKj2LcxBtCX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGUY2jd7BtWO"},"source":["##### <font color=red> task : </font> boston のデータの前処理を行う\n","\n","正解はないので自由な発想で様々試みてみてほしい"]},{"cell_type":"code","metadata":{"id":"xMYKleMS-Oj4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohkwKiJcH2sS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tY9a7biuBpIc"},"source":["##### <font color=green> **2.2.** </font> データセットの分割（ホールドアウト）"]},{"cell_type":"code","metadata":{"id":"yFsqwfNrH2xG"},"source":["# ライブラリのインポート\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HN3N-Zt_DE_B"},"source":["# ホールドアウト実行\n","X_train, X_test, Y_train, Y_test = train_test_split(boston_df, boston.target)\n","\n","# デフォルトでは 学習用データ部分 ： テストデータ部分 = 3 : 1 に分かれる (test_size=0.25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilDV7r2wDpaH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K7eJdJWXDsX1"},"source":["##### <font color=red> task : </font> 分割された様子を図示する"]},{"cell_type":"code","metadata":{"id":"U3UHRHDO-NUh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JPRNJBXDokJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecRKgvIlD-Hg"},"source":["##### <font color=green> **2.3.** </font> k近傍法を実行"]},{"cell_type":"code","metadata":{"id":"k21P06HUDTdu"},"source":["## 今回はコードの紹介がメインなので、前処理などはせずそのままのデータを使用"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5-uMBWVOLf2"},"source":["# ライブラリのインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5ebURYAEEy3"},"source":["# デフォルト値（k=5）で回帰するインスタンス生成\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jz2i-7EFPe29"},"source":["# 学習用データのみを与えて .fit 実行\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsvvlgVUPe06"},"source":["# テスト用データによる予測結果を取得\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueY09qxtE0BL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6l2mRYOEcGZ"},"source":["## 予測結果と正解（テスト用データの目的変数）との誤差を計算する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8-UgYaVExbS"},"source":["## 平均二乗誤差\n","# ライブラリのインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y7j2bqCEH22A"},"source":["# 計算実行\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMiBQOX0Emeu"},"source":["# 計算結果を表示\n","print('平均二乗誤差\\t: ', mse)\n","print('その平方根\\t: ', mse**(1/2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjKqbEsyEmPI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2IPGXkHyE59B"},"source":["## 平均絶対誤差\n","# ライブラリのインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzVy6DrvE54X"},"source":["# 計算実行\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqnT57EkH246"},"source":["# 計算結果を表示\n","print('平均絶対誤差\\t: ', mae)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EMooryLBGa0X"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1j2dtLe6Gax5"},"source":["## 予測結果を縦軸、正解（テスト用データの目的変数）を横軸として散布図を表示\n","# 予測と正解が完全に一致していれば、 y = x の直線上に分布するので、その線も引く\n","\n","plt.figure(figsize=(8, 8))\n","sns.set()\n","plt.rcParams['lines.markersize'] = 5\n","\n","plt.scatter(      )\n","plt.plot(       , color=\"red\")\n","plt.xlabel(   )\n","plt.ylabel(   )\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f-rMxfuPFBeK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9pXGyr6gFBvQ"},"source":["##### <font color=green> **2.4.** </font> ハイパーパラメータ k について"]},{"cell_type":"code","metadata":{"id":"5S1wP4gwFNGL"},"source":["## kの値による誤差の変化を調べる\n","\n","# 平均絶対誤差を保存する入れ物を作成\n","mae_list = []\n","\n","# 平均二乗誤差を保存する入れ物を作成\n","mse_list = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjiefcSbGr2n"},"source":["# k=1〜100 でk近傍法を行い、それぞれの誤差２種を取得して保存（格納）\n","k_range = range(1, 101)\n","for k in k_range:\n","  knn = KNeighborsRegressor(n_neighbors=k)\n","  knn.fit(X_train, Y_train)\n","  Y_pred = knn.predict(X_test)\n","  mae_list.append(mean_absolute_error(Y_test, Y_pred))\n","  mse_list.append(mean_squared_error(Y_test, Y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYl7n4PBopyS"},"source":["# 並べて図示\n","plt.figure(figsize=(20, 8))\n","sns.set()\n","\n","# 左側に平均絶対誤差:mae\n","plt.subplot(1,2,1)\n","plt.plot(k_range, mae_list)\n","plt.xlabel('k-nn')\n","plt.ylabel(\"mae\")\n","\n","# 右側に平均二乗誤差:mse\n","plt.subplot(1,2,2)\n","plt.plot(k_range, mse_list)\n","plt.xlabel('k-nn')\n","plt.ylabel(\"mse\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tw6ID1KVMp1"},"source":["# k=1〜20 を拡大表示（抽出）\n","\n","# 並べて図示\n","plt.figure(figsize=(16, 6))\n","sns.set()\n","\n","# 左側に平均絶対誤差:mae\n","plt.subplot(1,2,1)\n","plt.plot(k_range[0:20], mae_list[0:20], \"bo-\")\n","plt.xlabel('k-nn')\n","plt.ylabel(\"mae\")\n","\n","# 右側に平均二乗誤差:mse\n","plt.subplot(1,2,2)\n","plt.plot(k_range[0:20], mse_list[0:20], \"bo-\")\n","plt.xlabel('k-nn')\n","plt.ylabel(\"mse\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3mRXFICVMdb"},"source":["## k=3,4 が候補か。。。"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"urvbnNuA1jJ9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5D4ajI3bCF2V"},"source":["#### <font color=red> task : </font> boston のデータで交差検証/グリッドサーチを行い、最適な k を決定する\n","\n","実行した前処理ごとにさらに比較できているとなお良い"]},{"cell_type":"code","metadata":{"id":"VdZSX7rPGRYB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqAjVrQjGRVA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eApwiOQecZV9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAtTr_HMHEAi"},"source":["## 23. 決定木 (DecisionTrees) と ランダムフォレスト (RandomForest)"]},{"cell_type":"markdown","metadata":{"id":"PyUtgsLhKKAO"},"source":["### <font color = blue>**1.** </font>分類木\n","ライブラリ : sklearn.tree.DecisionTreeClassifier\n","\n","$\\downarrow \\downarrow$ 公式リファレンス $\\downarrow \\downarrow$\\\n","https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"]},{"cell_type":"markdown","metadata":{"id":"qwZm5EC_2O01"},"source":["##### <font color=green> **1.1.** </font> データの準備"]},{"cell_type":"code","metadata":{"id":"Lm7N_NUZ2zin"},"source":["# criterion : 分割基準。 gini or entropy を選択。(デフォルトでジニ係数)\n","# max_depth : 木の深さ。木が深くなるほど過学習し易いので、適当なしきい値を設定してあげる\n","# max_features ： 最適な分割をする際の特徴量の数\n","# min_samples_split ： 分岐する際のサンプル数\n","# random_state ： ランダムseedの設定。seedを設定しないと、毎回モデル結果が変わる"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TNno2vpZSFa"},"source":["# irisデータをロード\n","from sklearn.datasets import load_iris\n","iris = load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FPAHCy_Zhys"},"source":["# データフレーム形式に変換\n","# カラム名を説明変数に設定\n","import pandas as pd\n","iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7sN89wMZhva"},"source":["## ちなみにこのように書くと目的変数を一発で合体できる\n","iris_df[\"species\"] = pd.Categorical.from_codes(iris.target, iris.target_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"282W4U7VCJCf"},"source":["iris_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8mf8i_Ai2kea"},"source":["## 以下、1つのデータフレームに説明変数と目的変数がまとめられている時のコード例を想定"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vxuHZ6WM2wMM"},"source":["##### <font color=green> **1.2.** </font> データセットの分割（ホールドアウト）"]},{"cell_type":"code","metadata":{"id":"qrf5M2NJ2wMN"},"source":["# ライブラリをインポート\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALCv-AkdaK1d"},"source":["# 説明変数を切り出す\n","features = iris_df.loc[:, iris.feature_names]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZRwMWTq3Rl_"},"source":["# 目的変数を切り出す\n","label = iris_df[\"species\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN8CyWgM2wMO"},"source":["# ホールドアウト実行\n","df_train, df_test, label_train, label_test = train_test_split(features, label)\n","\n","# デフォルトでは 学習用データ部分 ： テストデータ部分 = 3 : 1 に分かれる (test_size=0.25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apL2jovE3RfV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ocJDiwYP3smw"},"source":["##### <font color=green> **1.3.** </font> 決定木を実行"]},{"cell_type":"code","metadata":{"id":"dO8zlHz-30nl"},"source":["# ライブラリのインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LrpGY1Nr30lD"},"source":["##  モデルのインスタンス生成\n","# 引数はあえて適当に設定してみる\n","dtc = DecisionTreeClassifier(max_leaf_nodes=  ,\n","                            max_depth=  ,\n","                            min_samples_split=  ,\n","                            min_samples_leaf= ,\n","                            random_state= ,\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SlWEDD8x4Gq9"},"source":["# 学習用データのみを与えて .fit 実行\n","dtc.fit(      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evYbgGr9aKy3"},"source":["# テスト用データによる予測結果を表示\n","print(\"予測精度\\t : \", dtc.score(df_test, label_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1h3gZrQe4nfB"},"source":["##### <font color=green> **1.4.** </font> モデルを可視化"]},{"cell_type":"code","metadata":{"id":"zzUT6tSt4wm8"},"source":["## 決定木モデルをDOTデータという規格に変換する\n","# ライブラリのインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7nYjmYTfyS4"},"source":["# 変換実行\n","dot_data = export_graphviz(   ,\n","                           filled=True,\n","                           rounded=True,\n","                           feature_names= ,\n","                           class_names= ,\n","                           special_characters=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRtc0xg842zP"},"source":["## DOTデータを描画するライブラリをインポート\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HG1mRvMM5HAh"},"source":["# ダイアグラムを描画\n","graph = pdp.graph_from_dot_data(dot_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWgwPoP65Jnh"},"source":["'''\n","画像ファイルとして出力（保存）したい場合\n","\n","file_name = \"/content/tree_visualization.png\"\n","graph.write_pdf(file_name)\n","\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKRmBHTV5Vcj"},"source":["## ダイアグラムを表示するためのライブラリをインポート\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Kr1HkpbjTL5"},"source":["# 画面（コンソール）にダイアグラムを表示\n","Image(graph.create_png())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTkfRvRYcaLc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmdFu897GuIi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GAPpdrg8KZnq"},"source":["### <font color = blue>**2.** </font>回帰木\n","ライブラリ : sklearn.tree.DecisionTreeRegressor\n","\n","$\\downarrow \\downarrow$ 公式リファレンス $\\downarrow \\downarrow$\\\n","https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"]},{"cell_type":"markdown","metadata":{"id":"JgOniiud7hHu"},"source":["##### <font color=green> **2.1.** </font> データの準備"]},{"cell_type":"code","metadata":{"id":"CM6YazQn7hHu"},"source":["# bostonデータをロード\n","from sklearn.datasets import load_boston\n","boston = load_boston()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sv76EGv37hHu"},"source":["# データフレーム形式に変換\n","# カラム名を説明変数に設定\n","import pandas as pd\n","boston_df = pd.DataFrame(boston.data, columns = boston.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejO6Uefx7hHw"},"source":["# 目的変数を追加\n","boston_df['MEDV'] = boston.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBOOQmj28xM5"},"source":["# 先頭５行を表示\n","print(boston_df.head()) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lNYRZexv7hHx"},"source":["# データの形状を確認\n","print(boston_df.shape) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ta4E-2DI7hHy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMupCdCk7hHy"},"source":["##### <font color=green> **2.2.** </font> データセットの分割（ホールドアウト）"]},{"cell_type":"code","metadata":{"id":"SaX4MfHs9AHi"},"source":["# データセットをNumpy配列に変換\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGKXdJANkIVY"},"source":["# 説明変数と目的変数に分ける\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLENBQc27hHy"},"source":["# ライブラリのインポート\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XgL7o4K1kIHZ"},"source":["# 今回は70%のデータを学習用データ、残りの30%のデータをテストデータとします\n","X_train, X_test, Y_train, Y_test = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfvFa7gc7hHz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bv-ACYZR7hH0"},"source":["##### <font color=green> **2.3.** </font> 決定木を実行"]},{"cell_type":"code","metadata":{"id":"6LdlGCgS9tzv"},"source":["# ライブラリのインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FQ2S7fv_9tuo"},"source":["##  モデルのインスタンス生成\n","# 引数はあえて適当に設定してみる\n","dtr = DecisionTreeRegressor(max_leaf_nodes= ,\n","                            max_depth=  ,\n","                            min_samples_split=  ,\n","                            min_samples_leaf= ,\n","                            random_state= ,\n","                            )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4XxMj3c_-VqL"},"source":["# 学習実行、結果を返り値で受け取る場合\n","model = dtr.fit(      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gqzmIrF_H0M"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqB9GSjU_0vf"},"source":["##### <font color=green> **2.4.** </font> モデル評価"]},{"cell_type":"code","metadata":{"id":"k44NWbGZ-qQS"},"source":["## 学習したモデルによる予測の妥当性を評価したい\n","# 全サンプル（506個）の中からランダムに1個を抽出し、その特徴量から予測される価格と実際に観測された価格を比較する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzZYnGC_-9V9"},"source":["# 乱数生成\n","import random\n","random.seed(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCvYZZfx-9Sa"},"source":["# ランダムにidを選定\n","id = random.randrange(0, X.shape[0], 1)\n","print(id)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXbOw6yO_SPP"},"source":["# 元のデータセットから該当サンプルを抽出\n","x = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxBrEgMQ_SMh"},"source":["# 行ベクトル（1d array）に変換\n","x = x.reshape(1,13)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Oscw1w0_lHm"},"source":["# 説明変数から住宅価格を予測\n","Y_pred = model.predict(   )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0h2bqmp-qNH"},"source":["# 実際のデータの住宅価格と比較\n","print(\"実際の価格\\t : {}\".format(boston_df.at[id, \"MEDV\"]),\n","      \"\\n予測価格\\t\\t : {}\".format(float(Y_pred)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jA9t5hvE_ObQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhQ-zQ_B_OXp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gT_o66eKo_wi"},"source":["## モデル自体の汎用性 : モデルの予測値がどのくらい観測値の情報量を説明できているか\n","# 汎用性の指標として決定係数を確認する\n","# 決定係数R2 : 回帰分析において、観測値に対する予測値の説明力を表す指標。寄与率ともいう\n","# 0 から 1 までの値をとり、R2 が 1 に近いほどモデルが有効であることを意味します"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alYKvWTySfpe"},"source":["# 決定係数を算出する関数をインポート\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLvlCdrokPmv"},"source":["# テスト用の説明変数(X_test)をモデルに渡して予測値を算出\n","Y_pred2 = model.predict(    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FC7WZrpBkPpI"},"source":["# これらの予測値とテスト用の目的変数(Y_test)を渡して決定係数を算出\n","r2 = r2_score(      )\n","print(\"R2 score = \", r2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dwV1hN-n_OSj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30glNtLO7hH7"},"source":["##### <font color=green> **2.5.** </font> モデルを可視化"]},{"cell_type":"code","metadata":{"id":"nBSRhCX6kPK6"},"source":["## 分類木の可視化と全く同じです\n","# ライブラリをインポート\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfIgwDhSvuZh"},"source":["# モデルをDOTデータに変換\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbDTCxNTBFp2"},"source":["# ダイアグラムを描画\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXi1PMI5kP0r"},"source":["# ダイアグラムを表示\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3t0Pih6Milkh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"75muI_FIKo2M"},"source":["### <font color = blue>**3.** </font>ランダムフォレスト\n","分類 $\\rightarrow$ sklearn.ensemble.RandomForestClassifier\n","\n","公式リファレンス $\\rightarrow$ https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","\n","\\\n","回帰 $\\rightarrow$ sklearn.ensemble.RandomForestRegressor\n","\n","公式リファレンス $\\rightarrow$ https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"]},{"cell_type":"markdown","metadata":{"id":"HdCE8TDdvMjS"},"source":["#### <font color = green>**3.1.** </font> ランダムフォレスト(Random Forests)のポイント"]},{"cell_type":"markdown","metadata":{"id":"RFiBC-hivfmJ"},"source":["##### ★ランダムフォレストの特徴\n","\n","- ランダムフォレストは簡単に言うと沢山の決定木を作成して、その多数決/平均をとるアルゴリズム\n"," - 過学習を抑える効果がある\n"," - 何千もの入力変数を削除せずにそのまま扱うことができる\n"," - 各変数の事前のスケーリングが不要\n"," - 交差検証や別個のテストデータでの検証をせずとも、アルゴリズム内で未知データに対する精度を推定することができる\n"," - 欠落値を推定する方法を持っているため、欠損値の多いデータでも精度を維持することができる\n"," - データ間の関係について情報を得ることができる"]},{"cell_type":"code","metadata":{"id":"PTrxcg3NBqE1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5N8Bm1HwIBk"},"source":["##### ★ポイント① ブートストラップ法 (Bootstrap Sampling)\n","\n","https://ja.wikipedia.org/wiki/ブートストラップ法\n","\n","- ランダムフォレストは決定木を沢山作って集合体である森にしている\n"," - その1つ1つの決定木を作成する際の訓練データの選び方がポイント\n"," - 全データの中から重複を許してサンプリングを行い（ブートストラッピング）、そのデータを決定木を作成する際の訓練データとする\n"," - 重複を許してサンプリングを行なうので、訓練データの中に同一データが含まれても構わない"]},{"cell_type":"code","metadata":{"id":"ib-Be1zsBqeS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1bUg4sGvwcQ9"},"source":["##### ★ポイント② ノード（分岐点）作成する際に使用する変数を絞る\n","- データ全量の中に ***p*** 個の変数がある時、その全てを各ノード作成に使うのではなく、***m*** 個の変数をランダムに選んで使用する\n"," - 個数としては $\\ m = \\sqrt{p}\\ $ 程度の数がよく用いられる\n","\n","- この工程を繰り返しながら大量（通常は100本以上）の決定木を作成\n","- 作成された決定木の多様さが、ランダムフォレストをより効果的にしてくれる"]},{"cell_type":"code","metadata":{"id":"u4zHY5RsBrKs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQJxviWWzgYk"},"source":["##### ★ポイント③ バギング (Bagging : bootstrap aggregating)\n","- 1本1本の決定木による予測結果をアンサンブル（分類なら多数決、回帰なら平均）して最終的な出力とする\n","- 分散を減らし、過剰適合を避ける効果があるため、どんな手法にも使うことができるモデル平均化手法の一種"]},{"cell_type":"code","metadata":{"id":"EP645ukfBrns"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YUjQqRxgzwfL"},"source":["##### ★ポイント④ OOB (out-of-bag) 検証\n","- ブートストラップ法で選ばれなかったデータのことをOOB (out-of-bag) データと呼ぶ\n"," - 元データ数の大体 $\\dfrac{1}{3}$ 程度の量になる\n","- このOOBデータによって、未知データに対する精度（汎化性能）を交差検証や別個のテストデータを要さず推定することができる\n"," - 作成したランダムフォレストにOOBデータを分類した際の分類誤差（OOBエラー : out-of-bag-error）\n"," - 回帰の場合は最小二乗誤差 (MSE) 等の指標\n","- この誤差指標に応じて使用する変数の数を調整することで精度の最大化を目指す"]},{"cell_type":"code","metadata":{"id":"rVxBJd0fBsIj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RC9Np0ixBfm3"},"source":["#### <font color = green>**3.2.** </font> iris の分類"]},{"cell_type":"code","metadata":{"id":"Vv0Ku9MBCfKN"},"source":["# 必要ライブラリのインポート\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hya9t-S8Ces1"},"source":["# データをロード\n","iris = load_iris()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6rbU5FLCm_X"},"source":["# 説明変数\n","data = iris['data']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3GidizYCm8r"},"source":["# 目的変数\n","target = iris['target']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGvw6IslCzxi"},"source":["# ホールドアウト\n","X_train, X_test ,Y_train, Y_test = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt3GJ55RCzuT"},"source":["# 学習モデルのインスタンス生成\n","rfc = RandomForestClassifier(n_estimators=    , # 使用する決定木の数\n","                            oob_score =True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xDC0iL2Czrn"},"source":["# 学習実行\n","rfc.fit(      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0pyQRrVosAa6"},"source":["# 正答率を表示\n","print('test_data_accuracy \\t : ', rfc.score(X_test, Y_test))\n","print('oob_data_accuracy \\t : ', rfc.score(X_train, rfc.oob_decision_function_.argmax(axis = 1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmjb5IALsFW-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obnFHOMbyHuJ"},"source":["#### <font color = green>**3.3.** </font> boston の回帰"]},{"cell_type":"code","metadata":{"id":"t-XmL0AnHDBA"},"source":["# 必要ライブラリのインポート\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mg6Vv82_G_-w"},"source":["# bostonデータをロード\n","boston = load_boston()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cowuzlkiHLag"},"source":["# 説明変数\n","data2 = boston['data']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0uGQaSoHLah"},"source":["# 目的変数\n","target2 = boston['target']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3HXiuMXHgXA"},"source":["# ホールドアウト\n","X_train, X_test ,Y_train, Y_test = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOyoD6DhHgXB"},"source":["# 学習モデルのインスタンス生成\n","rfr = RandomForestRegressor(n_estimators=     , # 使用する決定木の数\n","                            oob_score =True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ry1GRqS5HgXB"},"source":["# 学習実行\n","rfr.fit(      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfLxSASvHgXS"},"source":["# R2 score を表示\n","print('test_data_R2 \\t: ', rfr.score(X_test, Y_test))\n","print('oob_data_R2 \\t: ', rfr.score(X_train, rfr.oob_prediction_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgOCMyHsG_-0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljh6RTkVNNuG"},"source":[""],"execution_count":null,"outputs":[]}]}