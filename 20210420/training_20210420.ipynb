{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_20210420.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ltPJCG6pAUoc",
        "tBkE25Nl6ecO",
        "JO3yU576Myu9",
        "fWeIA_pH3FF4",
        "cPw5xFcq1kpw",
        "fJjM5KbzGowM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE56zYEz4WYC"
      },
      "source": [
        "## 44. サンプリング法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltPJCG6pAUoc"
      },
      "source": [
        "### <font color=blue>**1.** </font> A Tour of TensorFlow Probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gt2-W1-iTl7"
      },
      "source": [
        "## 出典： https://www.tensorflow.org/probability/examples/A_Tour_of_TensorFlow_Probability\n",
        "\n",
        "## Copyright 2019 The TensorFlow Probability Authors.\n",
        "## Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "# In this Colab, we explore some of the fundamental features of TensorFlow Probability. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UYdUIGU5KJ6"
      },
      "source": [
        "### Dependencies & Prerequisites\n",
        "\n",
        "## Import\n",
        "\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko3oWX0pEvnf"
      },
      "source": [
        "tf.enable_v2_behavior()\n",
        "\n",
        "sns.reset_defaults()\n",
        "sns.set_context(context='talk',font_scale=0.7)\n",
        "plt.rcParams['image.cmap'] = 'viridis'\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T555L1mW_p_H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2t6mr5Wp4Qp"
      },
      "source": [
        "## Vectorization\n",
        "# Vectorization makes things fast!\n",
        "# It also means we think a lot about shapes\n",
        "  \n",
        "mats = tf.random.uniform(shape=[1000, 10, 10])\n",
        "vecs = tf.random.uniform(shape=[1000, 10, 1])\n",
        "\n",
        "def for_loop_solve():\n",
        "  return np.array(\n",
        "    [tf.linalg.solve(mats[i, ...], vecs[i, ...]) for i in range(1000)])\n",
        "\n",
        "def vectorized_solve():\n",
        "  return tf.linalg.solve(mats, vecs)\n",
        "\n",
        "# Vectorization for the win!\n",
        "%timeit for_loop_solve()\n",
        "%timeit vectorized_solve()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ichaTMFv_yPv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCLkCcWpuKKh"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "a = tf.constant(np.pi)\n",
        "b = tf.constant(np.e)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch([a, b])\n",
        "  c = .5 * (a**2 + b**2)\n",
        "\n",
        "grads = tape.gradient(c, [a, b])\n",
        "print(grads[0])\n",
        "print(grads[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfSGRVBZ_zjF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPh4HEwuCnRr"
      },
      "source": [
        "### A simple scalar-variate `Distribution`\n",
        "\n",
        "## A standard normal\n",
        "\n",
        "normal = tfd.Normal(loc=0., scale=1.)\n",
        "print(normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIylCtMDCwnl"
      },
      "source": [
        "## Plot 1000 samples from a standard normal\n",
        "\n",
        "samples = normal.sample(1000)\n",
        "sns.distplot(samples)\n",
        "plt.title(\"Samples from a standard Normal\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wyn6XnmSh_iI"
      },
      "source": [
        "## Compute the log_prob of a point in the event space of `normal`\n",
        "\n",
        "normal.log_prob(0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q67IaJ-qie1u"
      },
      "source": [
        "## Compute the log_prob of a few points\n",
        "\n",
        "normal.log_prob([-1., 0., 1.])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsaOUOnE_-6M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfY1B4GqDDJo"
      },
      "source": [
        "## A *batch* of scalar-variate `Distributions`\n",
        "# Batches are like \"vectorized\" distributions: independent instances whose computations happen in parallel.\n",
        "\n",
        "# Create a batch of 3 normals, and plot 1000 samples from each\n",
        "normals = tfd.Normal([-2.5, 0., 2.5], 1.)  # The scale parameter broadacasts!\n",
        "print(\"Batch shape:\", normals.batch_shape)\n",
        "print(\"Event shape:\", normals.event_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNzTGgKqE5CT"
      },
      "source": [
        "# Samples' shapes go on the left!\n",
        "samples = normals.sample(1000)\n",
        "print(\"Shape of samples:\", samples.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIRbVOS7OP-g"
      },
      "source": [
        "# Sample shapes can themselves be more complicated\n",
        "print(\"Shape of samples:\", normals.sample([10, 10, 10]).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqXQ6DEDFsLU"
      },
      "source": [
        "# A batch of normals gives a batch of log_probs.\n",
        "print(normals.log_prob([-2.5, 0., 2.5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1uSOXO8GQB4"
      },
      "source": [
        "# The computation broadcasts, so a batch of normals applied to a scalar\n",
        "# also gives a batch of log_probs.\n",
        "print(normals.log_prob(0.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxAGVtnPGkQ4"
      },
      "source": [
        "# Normal numpy-like broadcasting rules apply!\n",
        "xs = np.linspace(-6, 6, 200)\n",
        "try:\n",
        "  normals.log_prob(xs)\n",
        "except Exception as e:\n",
        "  print(\"TFP error:\", e.message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieF2lRhPHxgd"
      },
      "source": [
        "# That fails for the same reason this does:\n",
        "try:\n",
        "  np.zeros(200) + np.zeros(3)\n",
        "except Exception as e:\n",
        "  print(\"Numpy error:\", e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS9qPHdeH0gz"
      },
      "source": [
        "# But this would work:\n",
        "a = np.zeros([200, 1]) + np.zeros(3)\n",
        "print(\"Broadcast shape:\", a.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w64N4YQH2r5"
      },
      "source": [
        "# And so will this!\n",
        "xs = np.linspace(-6, 6, 200)[..., np.newaxis]\n",
        "# => shape = [200, 1]\n",
        "\n",
        "lps = normals.log_prob(xs)\n",
        "print(\"Broadcast log_prob shape:\", lps.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfsG1F0FFAWS"
      },
      "source": [
        "# Summarizing visually\n",
        "for i in range(3):\n",
        "  sns.distplot(samples[:, i], kde=False, norm_hist=True)\n",
        "  \n",
        "plt.plot(np.tile(xs, 3), normals.prob(xs), c='k', alpha=.5)\n",
        "plt.title(\"Samples from 3 Normals, and their PDF's\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU47WxF3AMS3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zEm7UpFi2S_"
      },
      "source": [
        "## A vector-variate `Distribution`\n",
        "\n",
        "mvn = tfd.MultivariateNormalDiag(loc=[0., 0.], scale_diag = [1., 1.])\n",
        "print(\"Batch shape:\", mvn.batch_shape)\n",
        "print(\"Event shape:\", mvn.event_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfT5qzyRjGfg"
      },
      "source": [
        "samples = mvn.sample(1000)\n",
        "print(\"Samples shape:\", samples.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKdi0DDqjRiA"
      },
      "source": [
        "g = sns.jointplot(samples[:, 0], samples[:, 1], kind='scatter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ-lxBPhATfG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHOIJipjItGG"
      },
      "source": [
        "## A matrix-variate `Distribution`\n",
        "\n",
        "lkj = tfd.LKJ(dimension=10, concentration=[1.5, 3.0])\n",
        "print(\"Batch shape: \", lkj.batch_shape)\n",
        "print(\"Event shape: \", lkj.event_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkOqJiT6PcOT"
      },
      "source": [
        "samples = lkj.sample()\n",
        "print(\"Samples shape: \", samples.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYvY7_2xPecj"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
        "sns.heatmap(samples[0, ...], ax=axes[0], cbar=False)\n",
        "sns.heatmap(samples[1, ...], ax=axes[1], cbar=False)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUM35tQ8AaUY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_IpyZlUTlBr"
      },
      "source": [
        "## Gaussian Processes\n",
        "\n",
        "kernel = tfp.math.psd_kernels.ExponentiatedQuadratic()\n",
        "xs = np.linspace(-5., 5., 200).reshape([-1, 1])\n",
        "gp = tfd.GaussianProcess(kernel, index_points=xs)\n",
        "print(\"Batch shape:\", gp.batch_shape)\n",
        "print(\"Event shape:\", gp.event_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgI2dtZERCSX"
      },
      "source": [
        "upper, lower = gp.mean() + [2 * gp.stddev(), -2 * gp.stddev()]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(xs, gp.mean())\n",
        "plt.fill_between(xs[..., 0], upper, lower, color='k', alpha=.1)\n",
        "for _ in range(5):\n",
        "  plt.plot(xs, gp.sample(), c='r', alpha=.3)\n",
        "plt.title(r\"GP prior mean, $2\\sigma$ intervals, and samples\")\n",
        "plt.show()\n",
        "\n",
        "#    *** Bonus question ***\n",
        "# Why do so many of these functions lie outside the 95% intervals?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V39QKWlaAf_P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xgOdM2XstI"
      },
      "source": [
        "## GP Regression\n",
        "\n",
        "# Suppose we have some observed data\n",
        "obs_x = [[-3.], [0.], [2.]]  # Shape 3x1 (3 1-D vectors)\n",
        "obs_y = [3., -2., 2.]        # Shape 3   (3 scalars)\n",
        "\n",
        "gprm = tfd.GaussianProcessRegressionModel(kernel, xs, obs_x, obs_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqICnZOcUX8y"
      },
      "source": [
        "upper, lower = gprm.mean() + [2 * gprm.stddev(), -2 * gprm.stddev()]\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(xs, gprm.mean())\n",
        "plt.fill_between(xs[..., 0], upper, lower, color='k', alpha=.1)\n",
        "for _ in range(5):\n",
        "  plt.plot(xs, gprm.sample(), c='r', alpha=.3)\n",
        "plt.scatter(obs_x, obs_y, c='k', zorder=3)\n",
        "plt.title(r\"GP posterior mean, $2\\sigma$ intervals, and samples\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9GyrFrfAng4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6GCupiD5r5C"
      },
      "source": [
        "<font size=5>Bijectors</font>\n",
        "\n",
        "Bijectors represent (mostly) invertible, smooth functions. They can be used to transform distributions, preserving the ability to take samples and compute log_probs. They can be in the `tfp.bijectors` module.\n",
        "\n",
        "Each bijector implements at least 3 methods: \n",
        "  * `forward`,\n",
        "  * `inverse`, and\n",
        "  * (at least) one of `forward_log_det_jacobian` and `inverse_log_det_jacobian`.\n",
        "\n",
        "With these ingredients, we can transform a distribution and still get samples and log probs from the result!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATHL6ojeA3Y5"
      },
      "source": [
        "In Math, somewhat sloppily\n",
        "\n",
        "* $X$ is a random variable with pdf $p(x)$\n",
        "* $g$ is a smooth, invertible function on the space of $X$'s\n",
        "* $Y = g(X)$ is a new, transformed random variable\n",
        "* $p(Y=y) = p(X=g^{-1}(y)) \\cdot |\\nabla g^{-1}(y)|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0_O_yXqA7Io"
      },
      "source": [
        "Caching\n",
        "\n",
        "Bijectors also cache the forward and inverse computations, and log-det-Jacobians, which allows us to save\n",
        "repeating potentially very expensive operations!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M-asft7Asbl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChhpOmKiYres"
      },
      "source": [
        "## A Simple `Bijector`\n",
        "\n",
        "normal_cdf = tfp.bijectors.NormalCDF()\n",
        "xs = np.linspace(-4., 4., 200)\n",
        "plt.plot(xs, normal_cdf.forward(xs))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfLTIp12UGjn"
      },
      "source": [
        "plt.plot(xs, normal_cdf.forward_log_det_jacobian(xs, event_ndims=0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mKM9njHBDH2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4DeJGmlCJ7j"
      },
      "source": [
        "## A `Bijector` transforming a `Distribution`\n",
        "\n",
        "exp_bijector = tfp.bijectors.Exp()\n",
        "log_normal = exp_bijector(tfd.Normal(0., .5))\n",
        "\n",
        "samples = log_normal.sample(1000)\n",
        "xs = np.linspace(1e-10, np.max(samples), 200)\n",
        "sns.distplot(samples, norm_hist=True, kde=False)\n",
        "plt.plot(xs, log_normal.prob(xs), c='k', alpha=.75)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0urwjzZBIY2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrbhJ9uhZry9"
      },
      "source": [
        "## Batching `Bijectors`\n",
        "\n",
        "# Create a batch of bijectors of shape [3,]\n",
        "softplus = tfp.bijectors.Softplus(\n",
        "  hinge_softness=[1., .5, .1])\n",
        "print(\"Hinge softness shape:\", softplus.hinge_softness.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk_wOqJKXUaS"
      },
      "source": [
        "# For broadcasting, we want this to be shape [200, 1]\n",
        "xs = np.linspace(-4., 4., 200)[..., np.newaxis]\n",
        "ys = softplus.forward(xs)\n",
        "print(\"Forward shape:\", ys.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UazXAovhXHF-"
      },
      "source": [
        "# Visualization\n",
        "lines = plt.plot(np.tile(xs, 3), ys)\n",
        "for line, hs in zip(lines, softplus.hinge_softness):\n",
        "  line.set_label(\"Softness: %1.1f\" % hs)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUeyINH1BPOZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g20uziHzqdJw"
      },
      "source": [
        "## Caching\n",
        "\n",
        "# This bijector represents a matrix outer product on the forward pass,\n",
        "# and a cholesky decomposition on the inverse pass. The latter costs O(N^3)!\n",
        "bij = tfb.CholeskyOuterProduct()\n",
        "\n",
        "size = 2500\n",
        "# Make a big, lower-triangular matrix\n",
        "big_lower_triangular = tf.eye(size)\n",
        "# Squaring it gives us a positive-definite matrix\n",
        "big_positive_definite = bij.forward(big_lower_triangular)\n",
        "\n",
        "# Caching for the win!\n",
        "%timeit bij.inverse(big_positive_definite)\n",
        "%timeit tf.linalg.cholesky(big_positive_definite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BzcHfoBBU5p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iz5lD0wh-41"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBkE25Nl6ecO"
      },
      "source": [
        "### <font color=blue>**2.** </font> ライブラリの使用例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiAdvz307SPw"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51_zDYMz7SHL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib3LMLDt7sXW"
      },
      "source": [
        "## Metropolis Hastings\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "dtype = np.float32\n",
        "\n",
        "target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
        "\n",
        "\n",
        "rwm_kernel = tfp.mcmc.RandomWalkMetropolis(target.log_prob)\n",
        "\n",
        "samples = tfp.mcmc.sample_chain(\n",
        "  num_results=1000,\n",
        "  current_state=dtype(1),\n",
        "  kernel=rwm_kernel,\n",
        "  num_burnin_steps=500,\n",
        "  trace_fn=None)\n",
        "\n",
        "sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
        "sample_std = tf.sqrt(\n",
        "    tf.math.reduce_mean(\n",
        "        tf.math.squared_difference(samples, sample_mean),\n",
        "        axis=0))\n",
        "\n",
        "print('予測平均: {}'.format(sample_mean))\n",
        "print('予測標準偏差: {}'.format(sample_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQnOWyhElyF8"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.plot(x, norm.pdf(x))\n",
        "sns.distplot(samples)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFMc6uYaDKmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbeKfv6RDKjS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yOzvWwuqc8T"
      },
      "source": [
        "## Hamiltonian Monte Carlo\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "dtype = np.float32\n",
        "\n",
        "target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
        "\n",
        "\n",
        "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "  target_log_prob_fn=target.log_prob,\n",
        "  step_size=np.float64(.5),\n",
        "  num_leapfrog_steps=np.float64(2))\n",
        "\n",
        "samples = tfp.mcmc.sample_chain(\n",
        "  num_results=1000,\n",
        "  current_state=dtype(1),\n",
        "  kernel=hmc_kernel,\n",
        "  num_burnin_steps=500,\n",
        "  trace_fn=None)\n",
        "\n",
        "sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
        "sample_std = tf.sqrt(\n",
        "    tf.math.reduce_mean(\n",
        "        tf.math.squared_difference(samples, sample_mean),\n",
        "        axis=0))\n",
        "\n",
        "print('予測平均: {}'.format(sample_mean))\n",
        "print('予測標準偏差: {}'.format(sample_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uRzJg6Eqc_3"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.plot(x, norm.pdf(x))\n",
        "sns.distplot(samples)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyLFKe7vDPqQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsYkbTrtDiI1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dChlwPsnqdGW"
      },
      "source": [
        "## Simple Step Size Adaptation\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "dtype = np.float32\n",
        "\n",
        "target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
        "\n",
        "\n",
        "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "  target_log_prob_fn=target.log_prob,\n",
        "  step_size=np.float64(.5),\n",
        "  num_leapfrog_steps=np.float64(2))\n",
        "\n",
        "sssa_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "    inner_kernel=hmc_kernel, num_adaptation_steps=int(1000 * 0.8),\n",
        "    target_accept_prob=0.66)\n",
        "\n",
        "samples = tfp.mcmc.sample_chain(\n",
        "  num_results=1000,\n",
        "  current_state=dtype(1),\n",
        "  kernel=sssa_kernel,\n",
        "  num_burnin_steps=500,\n",
        "  trace_fn=None)\n",
        "\n",
        "sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
        "sample_std = tf.sqrt(\n",
        "    tf.math.reduce_mean(\n",
        "        tf.math.squared_difference(samples, sample_mean),\n",
        "        axis=0))\n",
        "\n",
        "print('予測平均: {}'.format(sample_mean))\n",
        "print('予測標準偏差: {}'.format(sample_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTAxhHcSqdJY"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.plot(x, norm.pdf(x))\n",
        "sns.distplot(samples)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6R8jUsxDTJw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRas-tRwDdXD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeU7A9gYqdPv"
      },
      "source": [
        "## Dual Averaging Step Size Adaptation\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "dtype = np.float32\n",
        "\n",
        "target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
        "\n",
        "\n",
        "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "  target_log_prob_fn=target.log_prob,\n",
        "  step_size=np.float64(.5),\n",
        "  num_leapfrog_steps=np.float64(2))\n",
        "\n",
        "dassa_kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
        "    inner_kernel=hmc_kernel, num_adaptation_steps=int(500),\n",
        "    target_accept_prob=0.66)\n",
        "\n",
        "samples = tfp.mcmc.sample_chain(\n",
        "  num_results=1000,\n",
        "  current_state=dtype(1),\n",
        "  kernel=dassa_kernel,\n",
        "  num_burnin_steps=500,\n",
        "  trace_fn=None)\n",
        "\n",
        "sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
        "sample_std = tf.sqrt(\n",
        "    tf.math.reduce_mean(\n",
        "        tf.math.squared_difference(samples, sample_mean),\n",
        "        axis=0))\n",
        "\n",
        "print('予測平均: {}'.format(sample_mean))\n",
        "print('予測標準偏差: {}'.format(sample_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff4AFeyu4tw-"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.plot(x, norm.pdf(x))\n",
        "sns.distplot(samples)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyxT4276DfG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCSf-D8sDWFy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRnlwULXtayq"
      },
      "source": [
        "## Metropolis Adjusted Langevin Algorithm\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "dtype = np.float32\n",
        "\n",
        "target = tfd.Normal(loc=dtype(0), scale=dtype(1))\n",
        "\n",
        "\n",
        "mala_kernel = tfp.mcmc.MetropolisAdjustedLangevinAlgorithm(\n",
        "        target_log_prob_fn=target.log_prob,\n",
        "        step_size=0.75)\n",
        "samples = tfp.mcmc.sample_chain(\n",
        "  num_results=1000,\n",
        "  current_state=dtype(1),\n",
        "  kernel=mala_kernel,\n",
        "  num_burnin_steps=500,\n",
        "  trace_fn=None)\n",
        "\n",
        "sample_mean = tf.math.reduce_mean(samples, axis=0)\n",
        "sample_std = tf.sqrt(\n",
        "    tf.math.reduce_mean(\n",
        "        tf.math.squared_difference(samples, sample_mean),\n",
        "        axis=0))\n",
        "\n",
        "print('予測平均: {}'.format(sample_mean))\n",
        "print('予測標準偏差: {}'.format(sample_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpoqlvshlyIv"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.plot(x, norm.pdf(x))\n",
        "sns.distplot(samples)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAuzM5-GlyML"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfWkPQUk6cGw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO3yU576Myu9"
      },
      "source": [
        "#### <font color=red>task : </font> ガンマ分布の場合について、同様に上記5つの各メソッドによりサンプリングし図示してみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHx6YzFoMSDD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqLWnXm5MRzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZql0cFyMRwH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwS7vUU4MRtM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZNEQaS8MRp4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e28SPouPMRmI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD5Gu9eKMRi_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX3N8SjUMRdw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWeIA_pH3FF4"
      },
      "source": [
        "### <font color=blue>**3.** </font> ギブスサンプリング（MCMC : Markov chain Monte Carlo methods）による画像のノイズ除去"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZDyRwGe5Ne6"
      },
      "source": [
        "## 出典: https://ichi.pro/gibusu-sanpuringu-mcmc-niyoru-gazo-no-noizu-jokyo-19602944876883"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g-RmNgbY2WJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPUY2v1zZYfU"
      },
      "source": [
        "img = cv2.imread(\"/content/img.png\", 0)\n",
        "img_noisy = cv2.imread(\"/content/img_noisy.png\", 0)\n",
        "\n",
        "plt.figure(figsize=(16,7))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(img, cmap = 'gray')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(img_noisy, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG8o03Hu3Er5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t4AksKw-K4a"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qUL2iuD-K12"
      },
      "source": [
        "def load_image(filename):\n",
        "  ## PNG画像をnumpy配列に読み取り\n",
        "  my_img = plt.imread(filename)\n",
        "  \n",
        "  ## グレースケールに変換\n",
        "  img_gray = np.dot(my_img[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "  \n",
        "  ## ピクセルを{-1、1}に再スケーリング\n",
        "  img_gray = np.where(img_gray > 0.5, 1, -1)\n",
        "  \n",
        "  ## 各ピクセルの隣接ピクセルを検索するときにコーナーケースを処理できるように、エッジに0個のパディングを追加\n",
        "  img_padded = np.zeros([img_gray.shape[0] + 2, img_gray.shape[1] + 2])\n",
        "  img_padded[1:-1, 1:-1] = img_gray\n",
        "  return img_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNhYne9zoU12"
      },
      "source": [
        "$\\log{P(Y|X)} = \\log{P(X|Y)} + \\log{P(Y)} - \\log{P(X)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPjUpOQ3oUyz"
      },
      "source": [
        "$\\displaystyle P(Y, X) = \\dfrac{1}{Z} \\exp \\left( \\eta \\sum_{i=1}^{N} \\sum_{j=1}^{M}{x_{ij}y_{ij}} + \\beta \\sum_{i'j' \\in N(ij)}^{} {y_{ij}y_{i'j'}} \\right)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLKMBmNS-Ky5"
      },
      "source": [
        "def sample_y(i, j, Y, X):\n",
        "  ## 行と列のインデックス i と j\n",
        "  ## 復元された画像配列Y\n",
        "  ## ノイズの多い画像配列X\n",
        "\n",
        "  ## yij の近傍 yij_neighbors を検索し、条件付き確率P（yij = 1 | yij_neighbors）を計算\n",
        "  markov_blanket = [Y[i - 1, j], Y[i, j - 1], Y[i, j + 1], Y[i + 1, j], X[i, j]]\n",
        "  w = ETA * markov_blanket[-1] + BETA * sum(markov_blanket[:4])\n",
        "\n",
        "  ## 条件付き確率でサンプリングされた yij の値（1または-1）を返す\n",
        "  prob = 1 / (1 + math.exp(-2*w))\n",
        "  return (np.random.rand() < prob) * 2 - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLS6bxm6oUu6"
      },
      "source": [
        "$\\displaystyle P(y_{ij} = 1 | y_{N(ij)}, x_{i, j}) = \\cdots = \\dfrac{1}{1 + \\exp (-2w_{ij})}$\n",
        "\n",
        "$\\displaystyle w_{ij} = \\eta x_{ij} + \\beta \\sum_{N(ij)} y_{N(ij)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbiNTWKqHVCv"
      },
      "source": [
        "def get_posterior(filename, burn_in_steps, total_samples, logfile):\n",
        "  ## ノイズの多い画像Xをロード\n",
        "  X = load_image(filename)\n",
        "  \n",
        "  posterior = np.zeros(X.shape)\n",
        "  print(\"img shape: {}\".format(X.shape))\n",
        "  \n",
        "  ## 復元された画像Yをランダムに初期化\n",
        "  Y = np.random.choice([1, -1], size=X.shape)\n",
        "  energy_list = list()\n",
        "  \n",
        "  ## Yをサンプリングし、事後確率P（Y | Y_neighbor）を計算\n",
        "  for step in range(burn_in_steps + total_samples):\n",
        "    if step % 10 == 0:\n",
        "      print(\"{}th step start\".format(step+1))\n",
        "    for i in range(1, Y.shape[0]-1):\n",
        "      for j in range(1, Y.shape[1]-1):\n",
        "        ## Yの各ピクセルをサンプリング\n",
        "        y = sample_y(i, j, Y, X)\n",
        "\n",
        "        ## サンプリングされた値でYを更新\n",
        "        Y[i, j] = y\n",
        "\n",
        "        ## バーンイン期間が終了すると、Yのyijについて、yij = 1というイベントの発生総数を合計\n",
        "        if y == 1 and step >= burn_in_steps:\n",
        "          posterior[i, j] += 1\n",
        "    ## 収束を視覚化できるように、エネルギーを追跡\n",
        "    energy = -np.sum(np.multiply(Y, X))*ETA-(np.sum(np.multiply(Y[:-1], Y[1:]))+np.sum(np.multiply(Y[:, :-1], Y[:, 1:])))*BETA\n",
        "    if step < burn_in_steps:\n",
        "      energy_list.append(str(step) + \"\\t\" + str(energy) + \"\\tB\")\n",
        "    else:\n",
        "      energy_list.append(str(step) + \"\\t\" + str(energy) + \"\\tS\")\n",
        "  ## サンプリングが完了したら、モンテカルロ法を使用して事後確率を取得\n",
        "  ## 事後確率は、基本的にYの集計値を合計サンプル数で除算\n",
        "  posterior = posterior / total_samples\n",
        "\n",
        "  file = open(logfile, 'w')\n",
        "  for element in energy_list:\n",
        "    file.writelines(element)\n",
        "    file.write('\\n')\n",
        "  file.close()\n",
        "  return posterior"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcipQqmRHU_Z"
      },
      "source": [
        "## 入力関数\n",
        "\n",
        "def denoise_image(filename, burn_in_steps, total_samples, logfile):\n",
        "  ## 推定事後確率p（Y = 1 | Y_neighbor）を取得\n",
        "  posterior = get_posterior(filename, burn_in_steps, total_samples, logfile=logfile)\n",
        "  \n",
        "  denoised = np.zeros(posterior.shape, dtype=np.float64)\n",
        "  \n",
        "  ## しきい値を0.5に設定すると、復元された画像配列Yを後方から取得\n",
        "  denoised[posterior > 0.5] = 1\n",
        "  \n",
        "  ## 画像配列のエッジを取り除いて返す\n",
        "  return denoised[1:-1, 1:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3ov88rhHoha"
      },
      "source": [
        "def plot_energy(filename):\n",
        "  x = np.genfromtxt(filename, dtype=None, encoding='utf8')\n",
        "  its, energies, phases = zip(*x)\n",
        "  its = np.asarray(its)\n",
        "  energies = np.asarray(energies)\n",
        "  phases = np.asarray(phases)\n",
        "  burn_mask = (phases == 'B')\n",
        "  samp_mask = (phases == 'S')\n",
        "  assert np.sum(burn_mask) + np.sum(samp_mask) == len(x), 'Found bad phase'\n",
        "  its_burn, energies_burn = its[burn_mask], energies[burn_mask]\n",
        "  its_samp, energies_samp = its[samp_mask], energies[samp_mask]\n",
        "  p1, = plt.plot(its_burn, energies_burn, 'r')\n",
        "  p2, = plt.plot(its_samp, energies_samp, 'b')\n",
        "  plt.title(\"energy\")\n",
        "  plt.xlabel('iteration number')\n",
        "  plt.ylabel('energy')\n",
        "  plt.legend([p1, p2], ['burn in', 'sampling'])\n",
        "  plt.show()  ###\n",
        "\n",
        "  plt.savefig('%s.png' % filename[:-4])\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgXiy9BMHofK"
      },
      "source": [
        "def save_image(denoised_image):\n",
        "  plt.figure(figsize=(8,7))  ###\n",
        "  plt.imshow(denoised_image, cmap='gray')\n",
        "  plt.title(\"denoised image\")\n",
        "  plt.show()  ###\n",
        "\n",
        "  plt.savefig('/content/denoise_image.png') ###\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbWAcC4jHocn"
      },
      "source": [
        "## ハイパーパラメータ η と β\n",
        "ETA = 1\n",
        "BETA = 1\n",
        "\n",
        "## サンプリングステップ\n",
        "total_samples = 180  ###\n",
        "\n",
        "## 書き込みステップ\n",
        "burn_in_steps = 20 ###\n",
        "\n",
        "logfile = \"/content/log_energy.txt\" ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUGAFi127hN"
      },
      "source": [
        "time1 = time.time()\n",
        "denoised_img = denoise_image(\"/content/img_noisy.png\",  ###\n",
        "                             burn_in_steps = burn_in_steps,\n",
        "                             total_samples = total_samples, \n",
        "                             logfile = logfile\n",
        "                             )\n",
        "print(\"total time: {}\".format(time.time() - time1))\n",
        "save_image(denoised_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtPiDyaw27bt"
      },
      "source": [
        "# log = open(\"/content/log_energy.txt\")\n",
        "plot_energy(logfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYBM6J7W25qq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW7M25m-x7R-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPw5xFcq1kpw"
      },
      "source": [
        "### <font color=blue>**4.** </font> Eight schools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MzjGu_O7HwY"
      },
      "source": [
        "The eight schools problem ([Rubin 1981](https://www.jstor.org/stable/1164617)) considers the effectiveness of SAT coaching programs conducted in parallel at eight schools. It has become a classic problem ([Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/), [Stan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)) that illustrates the usefulness of hierarchical modeling for sharing information between exchangeable groups.\n",
        "\n",
        "The implemention below is an adaptation of an Edward 1.0 [tutorial](https://github.com/blei-lab/edward/blob/master/notebooks/eight_schools.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mV1NXbVco6h"
      },
      "source": [
        "## Copyright 2018 The TensorFlow Probability Authors.\n",
        "## Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "# 日本人による解説記事：https://qiita.com/aoki-h/items/b8281823146b0e6c3ac2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMTEI6ep4D_S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import warnings\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAtEieVY-Ol8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB0VpvLK-Odz"
      },
      "source": [
        "'''The Data\n",
        "From Bayesian Data Analysis, section 5.5 (Gelman et al. 2013):\n",
        " A study was performed for the Educational Testing Service to analyze the effects of special\n",
        " coaching programs for SAT-V (Scholastic Aptitude Test-Verbal) in each of eight high schools.\n",
        " The outcome variable in each study was the score on a special administration of the SAT-V, \n",
        " a standardized multiple choice test administered by the Educational Testing Service and used to \n",
        " help colleges make admissions decisions; the scores can vary between 200 and 800, \n",
        " with mean about 500 and standard deviation about 100. \n",
        " The SAT examinations are designed to be resistant to short-term efforts directed specifically toward \n",
        " improving performance on the test; instead they are designed to reflect knowledge acquired and abilities \n",
        " developed over many years of education. Nevertheless, each of the eight schools in this study considered \n",
        " its short-term coaching program to be very successful at increasing SAT scores. \n",
        " Also, there was no prior reason to believe that any of the eight programs was more effective than any other \n",
        " or that some were more similar in effect to each other than to any other.\n",
        "\n",
        "For each of the eight schools ($J = 8$), we have an estimated treatment effect $y_j$ and a standard error of the effect estimate $\\sigma_j$. \n",
        "The treatment effects in the study were obtained by a linear regression on the treatment group using PSAT-M and PSAT-V scores as control variables. \n",
        "As there was no prior belief that any of the schools were more or less similar or that any of the coaching programs would be more effective, \n",
        "we can consider the treatment effects as [exchangeable](https://en.wikipedia.org/wiki/Exchangeable_random_variables).\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSngqHwAKv_j"
      },
      "source": [
        "num_schools = 8  # number of schools\n",
        "treatment_effects = np.array(\n",
        "    [28, 8, -3, 7, -1, 1, 18, 12], dtype=np.float32)  # treatment effects\n",
        "treatment_stddevs = np.array(\n",
        "    [15, 10, 16, 11, 9, 11, 10, 18], dtype=np.float32)  # treatment SE\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.bar(range(num_schools), treatment_effects, yerr=treatment_stddevs)\n",
        "plt.title(\"8 Schools treatment effects\")\n",
        "plt.xlabel(\"School\")\n",
        "plt.ylabel(\"Treatment effect\")\n",
        "fig.set_size_inches(10, 8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ODB05En-co-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRl21GEK-cd-"
      },
      "source": [
        "## Model\n",
        "# To capture the data, we use a hierarchical normal model. It follows the generative process,"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Yj8WEDwI3L"
      },
      "source": [
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mu &\\sim \\text{Normal}(\\text{loc}{=}0,\\ \\text{scale}{=}10) \\\\\n",
        "\\log\\tau &\\sim \\text{Normal}(\\text{loc}{=}5,\\ \\text{scale}{=}1) \\\\\n",
        "\\text{for } & i=1\\ldots 8:\\\\\n",
        "& \\theta_i \\sim \\text{Normal}\\left(\\text{loc}{=}\\mu,\\ \\text{scale}{=}\\tau \\right) \\\\\n",
        "& y_i \\sim \\text{Normal}\\left(\\text{loc}{=}\\theta_i,\\ \\text{scale}{=}\\sigma_i \\right)\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2An0IIHC-rBI"
      },
      "source": [
        "where $\\mu$ represents the prior average treatment effect and $\\tau$ controls how much variance there is between schools. The $y_i$ and $\\sigma_i$ are observed. As $\\tau \\rightarrow \\infty$, the model approaches the no-pooling model, i.e., each of the school treatment effect estimates are allowed to be more independent. As $\\tau \\rightarrow 0$, the model approaches the complete-pooling model, i.e., all of the school treatment effects are closer to the group average $\\mu$. To restrict the standard deviation to be positive, we draw $\\tau$ from a lognormal distribution (which is equivalent to drawing $log(\\tau)$ from a normal distribution).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh7hs3yl-ubB"
      },
      "source": [
        "Following [Diagnosing Biased Inference with Divergences](http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html), we transform the model above into an equivalent non-centered model:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mu &\\sim \\text{Normal}(\\text{loc}{=}0,\\ \\text{scale}{=}10) \\\\\n",
        "\\log\\tau &\\sim \\text{Normal}(\\text{loc}{=}5,\\ \\text{scale}{=}1) \\\\\n",
        "\\text{for } & i=1\\ldots 8:\\\\\n",
        "& \\theta_i' \\sim \\text{Normal}\\left(\\text{loc}{=}0,\\ \\text{scale}{=}1 \\right) \\\\\n",
        "& \\theta_i = \\mu + \\tau \\theta_i' \\\\\n",
        "& y_i \\sim \\text{Normal}\\left(\\text{loc}{=}\\theta_i,\\ \\text{scale}{=}\\sigma_i \\right) \n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We reify this model as a [JointDistributionSequential](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/JointDistributionSequential) instance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBT0_T5N-isa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiEtvl1zokAG"
      },
      "source": [
        "model = tfd.JointDistributionSequential([\n",
        "  tfd.Normal(loc=0., scale=10., name=\"avg_effect\"),  # `mu` above\n",
        "  tfd.Normal(loc=5., scale=1., name=\"avg_stddev\"),  # `log(tau)` above\n",
        "  tfd.Independent(tfd.Normal(loc=tf.zeros(num_schools),\n",
        "                             scale=tf.ones(num_schools),\n",
        "                             name=\"school_effects_standard\"),  # `theta_prime` \n",
        "                  reinterpreted_batch_ndims=1),\n",
        "  lambda school_effects_standard, avg_stddev, avg_effect: (\n",
        "      tfd.Independent(tfd.Normal(loc=(avg_effect[..., tf.newaxis] +\n",
        "                                      tf.exp(avg_stddev[..., tf.newaxis]) *\n",
        "                                      school_effects_standard),  # `theta` above\n",
        "                                 scale=treatment_stddevs),\n",
        "                      name=\"treatment_effects\",  # `y` above\n",
        "                      reinterpreted_batch_ndims=1))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xemzqd5RNcoz"
      },
      "source": [
        "def target_log_prob_fn(avg_effect, avg_stddev, school_effects_standard):\n",
        "  \"\"\"Unnormalized target density as a function of states.\"\"\"\n",
        "  return model.log_prob((\n",
        "      avg_effect, avg_stddev, school_effects_standard, treatment_effects))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZUU5F3z-x-a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZmo2ZPe-0Eg"
      },
      "source": [
        "## Bayesian Inference\n",
        "# Given data, we perform Hamiltonian Monte Carlo (HMC) to calculate the posterior distribution over the model's parameters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBW06qvHNfxR"
      },
      "source": [
        "num_results = 5000\n",
        "num_burnin_steps = 3000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh9sA_qoNft1"
      },
      "source": [
        "# Improve performance by tracing the sampler using `tf.function`\n",
        "# and compiling it using XLA.\n",
        "@tf.function(autograph=False, experimental_compile=True)\n",
        "def do_sampling():\n",
        "  return tfp.mcmc.sample_chain(\n",
        "      num_results=num_results,\n",
        "      num_burnin_steps=num_burnin_steps,\n",
        "      current_state=[\n",
        "          tf.zeros([], name='init_avg_effect'),\n",
        "          tf.zeros([], name='init_avg_stddev'),\n",
        "          tf.ones([num_schools], name='init_school_effects_standard'),\n",
        "      ],\n",
        "      kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
        "          target_log_prob_fn=target_log_prob_fn,\n",
        "          step_size=0.4,\n",
        "          num_leapfrog_steps=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaTOBp05Nknm"
      },
      "source": [
        "states, kernel_results = do_sampling()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPEFP7AtNklD"
      },
      "source": [
        "avg_effect, avg_stddev, school_effects_standard = states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-66vCUVrQRnb"
      },
      "source": [
        "school_effects_samples = (\n",
        "    avg_effect[:, np.newaxis] +\n",
        "    np.exp(avg_stddev)[:, np.newaxis] * school_effects_standard)\n",
        "\n",
        "num_accepted = np.sum(kernel_results.is_accepted)\n",
        "print('Acceptance rate: {}'.format(num_accepted / num_results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-iMMOcFvE03"
      },
      "source": [
        "fig, axes = plt.subplots(8, 2, sharex='col', sharey='col')\n",
        "fig.set_size_inches(12, 10)\n",
        "for i in range(num_schools):\n",
        "  axes[i][0].plot(school_effects_samples[:,i].numpy())\n",
        "  axes[i][0].title.set_text(\"School {} treatment effect chain\".format(i))\n",
        "  sns.kdeplot(school_effects_samples[:,i].numpy(), ax=axes[i][1], shade=True)\n",
        "  axes[i][1].title.set_text(\"School {} treatment effect distribution\".format(i))\n",
        "axes[num_schools - 1][0].set_xlabel(\"Iteration\")\n",
        "axes[num_schools - 1][1].set_xlabel(\"School effect\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4t9XLxSszBe"
      },
      "source": [
        "print(\"E[avg_effect] = {}\".format(np.mean(avg_effect)))\n",
        "print(\"E[avg_stddev] = {}\".format(np.mean(avg_stddev)))\n",
        "print(\"\\nE[school_effects_standard] =\")\n",
        "print(np.mean(school_effects_standard[:, ]))\n",
        "print(\"\\nE[school_effects] =\")\n",
        "print(np.mean(school_effects_samples[:, ], axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxp1uFW6RWMW"
      },
      "source": [
        "# Compute the 95% interval for school_effects\n",
        "school_effects_low = np.array([\n",
        "    np.percentile(school_effects_samples[:, i], 2.5) for i in range(num_schools)\n",
        "])\n",
        "\n",
        "school_effects_med = np.array([\n",
        "    np.percentile(school_effects_samples[:, i], 50) for i in range(num_schools)\n",
        "])\n",
        "\n",
        "school_effects_hi = np.array([\n",
        "    np.percentile(school_effects_samples[:, i], 97.5)\n",
        "    for i in range(num_schools)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY-qBFTotd3F"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
        "ax.scatter(np.array(range(num_schools)), school_effects_med, color='red', s=60)\n",
        "ax.scatter(\n",
        "    np.array(range(num_schools)) + 0.1, treatment_effects, color='blue', s=60)\n",
        "\n",
        "plt.plot([-0.2, 7.4], [np.mean(avg_effect),\n",
        "                       np.mean(avg_effect)], 'k', linestyle='--')\n",
        "\n",
        "ax.errorbar(\n",
        "    np.array(range(8)),\n",
        "    school_effects_med,\n",
        "    yerr=[\n",
        "        school_effects_med - school_effects_low,\n",
        "        school_effects_hi - school_effects_med\n",
        "    ],\n",
        "    fmt='none')\n",
        "\n",
        "ax.legend(('avg_effect', 'HMC', 'Observed effect'), fontsize=14)\n",
        "\n",
        "plt.xlabel('School')\n",
        "plt.ylabel('Treatment effect')\n",
        "plt.title('HMC estimated school treatment effects vs. observed data')\n",
        "fig.set_size_inches(10, 8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Rp_R87N07h"
      },
      "source": [
        "## We can observe the shrinkage toward the group `avg_effect` above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcljZ1prD91d"
      },
      "source": [
        "print(\"Inferred posterior mean: {0:.2f}\".format(\n",
        "    np.mean(school_effects_samples[:,])))\n",
        "print(\"Inferred posterior mean se: {0:.2f}\".format(\n",
        "    np.std(school_effects_samples[:,])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te3DHaM_-7Jx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPCzgk7IMgt"
      },
      "source": [
        "<font size=5>Criticism</font>\n",
        "\n",
        "To get the posterior predictive distribution, i.e., a model of new data $y^*$ given the observed data $y$:\n",
        "\n",
        "$$ p(y^*|y) \\propto \\int_\\theta p(y^* | \\theta)p(\\theta |y)d\\theta$$\n",
        "\n",
        "we override the values of the random variables in the model to set them to the mean of the posterior distribution, and sample from that model to generate new data $y^*$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eV4Cx0HQeMU"
      },
      "source": [
        "sample_shape = [5000]\n",
        "\n",
        "_, _, _, predictive_treatment_effects = model.sample(\n",
        "    value=(tf.broadcast_to(np.mean(avg_effect, 0), sample_shape),\n",
        "           tf.broadcast_to(np.mean(avg_stddev, 0), sample_shape),\n",
        "           tf.broadcast_to(np.mean(school_effects_standard, 0),\n",
        "                           sample_shape + [num_schools]),\n",
        "           None))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3c8W--fPmph"
      },
      "source": [
        "fig, axes = plt.subplots(4, 2, sharex=True, sharey=True)\n",
        "fig.set_size_inches(12, 10)\n",
        "fig.tight_layout()\n",
        "for i, ax in enumerate(axes):\n",
        "  sns.kdeplot(predictive_treatment_effects[:, 2*i].numpy(),\n",
        "              ax=ax[0], shade=True)\n",
        "  ax[0].title.set_text(\n",
        "      \"School {} treatment effect posterior predictive\".format(2*i))\n",
        "  sns.kdeplot(predictive_treatment_effects[:, 2*i + 1].numpy(),\n",
        "              ax=ax[1], shade=True)\n",
        "  ax[1].title.set_text(\n",
        "      \"School {} treatment effect posterior predictive\".format(2*i + 1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATOOfzg0HMII"
      },
      "source": [
        "# The mean predicted treatment effects for each of the eight schools.\n",
        "prediction = np.mean(predictive_treatment_effects, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkwASzOLSgbs"
      },
      "source": [
        "We can look at the residuals between the treatment effects data and the predictions of the model posterior. These correspond with the plot above which shows the shrinkage of the estimated effects toward the population average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulqqNf_AHMBm"
      },
      "source": [
        "treatment_effects - prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KMqrBaGRo4S"
      },
      "source": [
        "Because we have a distribution of predictions for each school, we can consider the distribution of residuals as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j9RAYhIRDDz"
      },
      "source": [
        "residuals = treatment_effects - predictive_treatment_effects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW1RKYtBRIhd"
      },
      "source": [
        "fig, axes = plt.subplots(4, 2, sharex=True, sharey=True)\n",
        "fig.set_size_inches(12, 10)\n",
        "fig.tight_layout()\n",
        "for i, ax in enumerate(axes):\n",
        "  sns.kdeplot(residuals[:, 2*i].numpy(), ax=ax[0], shade=True)\n",
        "  ax[0].title.set_text(\n",
        "      \"School {} treatment effect residuals\".format(2*i))\n",
        "  sns.kdeplot(residuals[:, 2*i + 1].numpy(), ax=ax[1], shade=True)\n",
        "  ax[1].title.set_text(\n",
        "      \"School {} treatment effect residuals\".format(2*i + 1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIReUYcT0CEZ"
      },
      "source": [
        "Acknowledgements\n",
        "\n",
        "This tutorial was originally written in Edward 1.0 ([source](https://github.com/blei-lab/edward/blob/master/notebooks/eight_schools.ipynb)). We thank all contributors to writing and revising that version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqwd0vOUVmIF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVgUWGI0UoO4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJjM5KbzGowM"
      },
      "source": [
        "### <font color=blue>**5.** </font> A Tour of TensorFlow Probability　　続き"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bt2gZV-HC0t"
      },
      "source": [
        "### Dependencies & Prerequisites\n",
        "\n",
        "## Import\n",
        "\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8EObNfoHC0w"
      },
      "source": [
        "tf.enable_v2_behavior()\n",
        "\n",
        "sns.reset_defaults()\n",
        "sns.set_context(context='talk',font_scale=0.7)\n",
        "plt.rcParams['image.cmap'] = 'viridis'\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e2Vl5f7HFGZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AXI1ShABaY7"
      },
      "source": [
        "## MCMC\n",
        "\n",
        "# TFP has built in support for some standard Markov chain Monte Carlo algorithms, including Hamiltonian Monte Carlo."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pYoWgpwH3fK"
      },
      "source": [
        "## Generate a data set\n",
        "\n",
        "# Generate some data\n",
        "def f(x, w):\n",
        "  # Pad x with 1's so we can add bias via matmul\n",
        "  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)\n",
        "  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])\n",
        "  result = linop.matmul(x, adjoint=True)\n",
        "  return result[..., 0, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lZhXdbgbSwP"
      },
      "source": [
        "num_features = 2\n",
        "num_examples = 50\n",
        "noise_scale = .5\n",
        "true_w = np.array([-1., 2., 3.])\n",
        "\n",
        "xs = np.random.uniform(-1., 1., [num_features, num_examples])\n",
        "ys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmaqGeiraFSj"
      },
      "source": [
        "# Visualize the data set\n",
        "plt.scatter(*xs, c=ys, s=100, linewidths=0)\n",
        "\n",
        "grid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))\n",
        "xs_grid = np.stack(grid, axis=0)\n",
        "fs_grid = f(xs_grid.reshape([num_features, -1]), true_w)\n",
        "fs_grid = np.reshape(fs_grid, [100, 100])\n",
        "plt.colorbar()\n",
        "plt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRSgFvq7BiB1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY56HpEaduUV"
      },
      "source": [
        "## Define our joint log-prob function\n",
        "# The unnormalized posterior is the result of closing over the data to form a\n",
        "# partial application of the joint log prob.\n",
        "\n",
        "# Define the joint_log_prob function, and our unnormalized posterior.\n",
        "def joint_log_prob(w, x, y):\n",
        "  # Our model in maths is\n",
        "  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))\n",
        "  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N\n",
        "\n",
        "  rv_w = tfd.MultivariateNormalDiag(\n",
        "    loc=np.zeros(num_features + 1),\n",
        "    scale_diag=np.ones(num_features + 1))\n",
        "\n",
        "  rv_y = tfd.Normal(f(x, w), noise_scale)\n",
        "  return (rv_w.log_prob(w) +\n",
        "          tf.reduce_sum(rv_y.log_prob(y), axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgL8c1nKjSi8"
      },
      "source": [
        "# Create our unnormalized target density by currying x and y from the joint.\n",
        "def unnormalized_posterior(w):\n",
        "  return joint_log_prob(w, xs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqBVA2-nBszx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Myqb0Yjph3"
      },
      "source": [
        "## Build HMC TransitionKernel and call sample_chain\n",
        "\n",
        "# Create an HMC TransitionKernel\n",
        "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "  target_log_prob_fn=unnormalized_posterior,\n",
        "  step_size=np.float64(.1),\n",
        "  num_leapfrog_steps=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBuIs-IbedWo"
      },
      "source": [
        "# We wrap sample_chain in tf.function, telling TF to precompile a reusable\n",
        "# computation graph, which will dramatically improve performance.\n",
        "@tf.function\n",
        "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\n",
        "  return tfp.mcmc.sample_chain(\n",
        "    num_results=num_results,\n",
        "    num_burnin_steps=num_burnin_steps,\n",
        "    current_state=initial_state,\n",
        "    kernel=hmc_kernel,\n",
        "    trace_fn=lambda current_state, kernel_results: kernel_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PUR6FOZ75z0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxsVzw_-6vpf"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR_fZpvY0dUJ"
      },
      "source": [
        "initial_state = np.zeros(num_features + 1)\n",
        "samples, kernel_results = run_chain(initial_state)\n",
        "print(\"Acceptance rate:\", kernel_results.is_accepted.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CKVvkq0uLi"
      },
      "source": [
        "*That's not great! We'd like an acceptance rate closer to .65.*\n",
        "\n",
        "(see [\"Optimal Scaling for Various Metropolis-Hastings Algorithms\"](https://projecteuclid.org/euclid.ss/1015346320), Roberts & Rosenthal, 2001)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCQp771zB02Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBz-dRRcIHVP"
      },
      "source": [
        "## Adaptive step sizes\n",
        "\n",
        "# We can wrap our HMC TransitionKernel in a `SimpleStepSizeAdaptation` \"meta-kernel\", \n",
        "# which will apply some (rather simple heuristic) logic to adapt the HMC step size during burnin.\n",
        "# We allot 80% of burnin for adapting step size, and then let the remaining 20% go just to mixing."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzAocJeU0wib"
      },
      "source": [
        "# Apply a simple step size adaptation during burnin\n",
        "@tf.function\n",
        "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\n",
        "  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "      hmc_kernel,\n",
        "      num_adaptation_steps=int(.8 * num_burnin_steps),\n",
        "      target_accept_prob=np.float64(.65))\n",
        "\n",
        "  return tfp.mcmc.sample_chain(\n",
        "    num_results=num_results,\n",
        "    num_burnin_steps=num_burnin_steps,\n",
        "    current_state=initial_state,\n",
        "    kernel=adaptive_kernel,\n",
        "    trace_fn=lambda cs, kr: kr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZwdWBF114E2"
      },
      "source": [
        "samples, kernel_results = run_chain(\n",
        "  initial_state=np.zeros(num_features+1))\n",
        "print(\"Acceptance rate:\", kernel_results.inner_results.is_accepted.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkEqA1ISIQgi"
      },
      "source": [
        "# Trace plots\n",
        "colors = ['b', 'g', 'r']\n",
        "for i in range(3):\n",
        "  plt.plot(samples[:, i], c=colors[i], alpha=.3)\n",
        "  plt.hlines(true_w[i], 0, 1000, zorder=4, color=colors[i], label=\"$w_{}$\".format(i))\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEcns1FQI4_P"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore', FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di0E8pYy0UOD"
      },
      "source": [
        "# Histogram of samples\n",
        "for i in range(3):\n",
        "  sns.distplot(samples[:, i], color=colors[i])\n",
        "  #sns.displot(samples[:, i], color=colors[i], kde=True)\n",
        "ymax = plt.ylim()[1]\n",
        "for i in range(3):\n",
        "  plt.vlines(true_w[i], 0, ymax, color=colors[i])\n",
        "plt.ylim(0, ymax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PpXRKlHB6Nu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgBUkL1OKJn4"
      },
      "source": [
        "Diagnostics\n",
        "\n",
        "Trace plots are nice, but diagnostics are nicer!\n",
        "\n",
        "First we need to run multiple chains. This is as simple as giving a batch of\n",
        "`initial_state` tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqPcDhD7H3Dw"
      },
      "source": [
        "# Instead of a single set of initial w's, we create a batch of 8.\n",
        "num_chains = 8\n",
        "initial_state = np.zeros([num_chains, num_features + 1])\n",
        "\n",
        "chains, kernel_results = run_chain(initial_state)\n",
        "\n",
        "r_hat = tfp.mcmc.potential_scale_reduction(chains)\n",
        "print(\"Acceptance rate:\", kernel_results.inner_results.is_accepted.numpy().mean())\n",
        "print(\"R-hat diagnostic (per latent variable):\", r_hat.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPn_alyBB80o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RLmSKYpCIkv"
      },
      "source": [
        "## Sampling the noise scale\n",
        "\n",
        "# Define the joint_log_prob function, and our unnormalized posterior.\n",
        "def joint_log_prob(w, sigma, x, y):\n",
        "  # Our model in maths is\n",
        "  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))\n",
        "  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N\n",
        "\n",
        "  rv_w = tfd.MultivariateNormalDiag(\n",
        "    loc=np.zeros(num_features + 1),\n",
        "    scale_diag=np.ones(num_features + 1))\n",
        "  \n",
        "  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))\n",
        "\n",
        "  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])\n",
        "  return (rv_w.log_prob(w) +\n",
        "          rv_sigma.log_prob(sigma) +\n",
        "          tf.reduce_sum(rv_y.log_prob(y), axis=-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjxpVwHDCIXF"
      },
      "source": [
        "# Create our unnormalized target density by currying x and y from the joint.\n",
        "def unnormalized_posterior(w, sigma):\n",
        "  return joint_log_prob(w, sigma, xs, ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpM82XxeCIHq"
      },
      "source": [
        "# Create an HMC TransitionKernel\n",
        "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "  target_log_prob_fn=unnormalized_posterior,\n",
        "  step_size=np.float64(.1),\n",
        "  num_leapfrog_steps=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AzAnPCBCQPC"
      },
      "source": [
        "# Create a TransformedTransitionKernl\n",
        "transformed_kernel = tfp.mcmc.TransformedTransitionKernel(\n",
        "    inner_kernel=hmc_kernel,\n",
        "    bijector=[tfb.Identity(),    # w\n",
        "              tfb.Invert(tfb.Softplus())])   # sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcdwzzg8CQC2"
      },
      "source": [
        "# Apply a simple step size adaptation during burnin\n",
        "@tf.function\n",
        "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\n",
        "  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "      transformed_kernel,\n",
        "      num_adaptation_steps=int(.8 * num_burnin_steps),\n",
        "      target_accept_prob=np.float64(.75))\n",
        "\n",
        "  return tfp.mcmc.sample_chain(\n",
        "    num_results=num_results,\n",
        "    num_burnin_steps=num_burnin_steps,\n",
        "    current_state=initial_state,\n",
        "    kernel=adaptive_kernel,\n",
        "    seed=(0, 1),\n",
        "    trace_fn=lambda cs, kr: kr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WLdLT875OpQ"
      },
      "source": [
        "# Instead of a single set of initial w's, we create a batch of 8.\n",
        "num_chains = 8\n",
        "initial_state = [np.zeros([num_chains, num_features + 1]),\n",
        "                 .54 * np.ones([num_chains], dtype=np.float64)]\n",
        "\n",
        "chains, kernel_results = run_chain(initial_state)\n",
        "\n",
        "r_hat = tfp.mcmc.potential_scale_reduction(chains)\n",
        "print(\"Acceptance rate:\", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())\n",
        "print(\"R-hat diagnostic (per w variable):\", r_hat[0].numpy())\n",
        "print(\"R-hat diagnostic (sigma):\", r_hat[1].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYVmTIgNCP3m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYalW6EGVczF"
      },
      "source": [
        "w_chains, sigma_chains = chains\n",
        "\n",
        "# Trace plots of w (one of 8 chains)\n",
        "colors = ['b', 'g', 'r', 'teal']\n",
        "fig, axes = plt.subplots(4, num_chains, figsize=(4 * num_chains, 8))\n",
        "for j in range(num_chains):\n",
        "  for i in range(3):\n",
        "    ax = axes[i][j]\n",
        "    ax.plot(w_chains[:, j, i], c=colors[i], alpha=.3)\n",
        "    ax.hlines(true_w[i], 0, 1000, zorder=4, color=colors[i], label=\"$w_{}$\".format(i))\n",
        "    ax.legend(loc='upper right')\n",
        "  ax = axes[3][j]\n",
        "  ax.plot(sigma_chains[:, j], alpha=.3, c=colors[3])\n",
        "  ax.hlines(noise_scale, 0, 1000, zorder=4, color=colors[3], label=r\"$\\sigma$\".format(i))\n",
        "  ax.legend(loc='upper right')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQVcO1O9YSA"
      },
      "source": [
        "# Histogram of samples of w\n",
        "fig, axes = plt.subplots(4, num_chains, figsize=(4 * num_chains, 8))\n",
        "for j in range(num_chains):\n",
        "  for i in range(3):\n",
        "    ax = axes[i][j]\n",
        "    sns.distplot(w_chains[:, j, i], color=colors[i], norm_hist=True, ax=ax, hist_kws={'alpha': .3})\n",
        "    #sns.displot(w_chains[:, j, i], color=colors[i], ax=ax , kde=True)\n",
        "  for i in range(3):\n",
        "    ax = axes[i][j]\n",
        "    ymax = ax.get_ylim()[1]\n",
        "    ax.vlines(true_w[i], 0, ymax, color=colors[i], label=\"$w_{}$\".format(i), linewidth=3)\n",
        "    ax.set_ylim(0, ymax)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "  ax = axes[3][j]\n",
        "  sns.distplot(sigma_chains[:, j], color=colors[3], norm_hist=True, ax=ax, hist_kws={'alpha': .3})\n",
        "  #sns.displot(sigma_chains[:, j], color=colors[3], kde=True)\n",
        "  ymax = ax.get_ylim()[1]\n",
        "  ax.vlines(noise_scale, 0, ymax, color=colors[3], label=r\"$\\sigma$\".format(i), linewidth=3)\n",
        "  ax.set_ylim(0, ymax)\n",
        "  ax.legend(loc='upper right')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV-g5yiph-91"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10JkLjMSJBRW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}