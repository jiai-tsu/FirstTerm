{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_20210427.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "o-ERMWOCbL5n",
        "YnLDIM5cy2L_",
        "9QBR1Z9TyRK_",
        "iesSPGBcvgU8",
        "b9d0538e",
        "rKosQ7jQTbk2",
        "OlXF_xV4i5yD",
        "EP58PCBKV0o_",
        "CCtivAHtcRl7",
        "sm2CJ-cdbZaj",
        "ki9f7MIiunda",
        "XY2BsOXvv_4R",
        "sh7OpkjIxpeH",
        "0MCMuabKt79T",
        "MCSjRp70iNcu",
        "gmpaWSCacci_",
        "7Fd3FodCGKmy",
        "UiADcwBzGWGK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnijj-lZg_s"
      },
      "source": [
        "## 45. ガウス過程（GP : Gaussian Process）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQr8bUksVM3y"
      },
      "source": [
        "!pip install GPy\n",
        "\n",
        "!pip install gpflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqvGfYlCkvGJ"
      },
      "source": [
        "## サンプルコード置き場 : https://github.com/GPflow/docs/tree/develop/doc/source/notebooks\n",
        "# （colaboから読み込み可能）\n",
        "\n",
        "# ファイル -> ノートブックを開く -> 「GitHub」タブを選択\n",
        "#   -> 「GPflow」で検索\n",
        "#     -> レポジトリ ： GPflow/docs    ブランチ : develop    をそれぞれ選択"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-h2Z6eHFH7C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ERMWOCbL5n"
      },
      "source": [
        "### <font color=blue>**1.** </font> ガウス過程回帰（GPR）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnLDIM5cy2L_"
      },
      "source": [
        "#### <font color=green>**1.1.** </font> Gaussian Processes regression: basic introductory example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZbuO8XyFDI6"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html\n",
        "\n",
        "# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n",
        "#         Jake Vanderplas <vanderplas@astro.washington.edu>\n",
        "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41_CE2_1y-yq"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwdUwClYy-vU"
      },
      "source": [
        "def f(x):\n",
        "  \"\"\"The function to predict.\"\"\"\n",
        "  return x * np.sin(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VThCg910zZSj"
      },
      "source": [
        "# ----------------------------------------------------------------------\n",
        "#  First the noiseless case\n",
        "X1 = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n",
        "\n",
        "# Observations\n",
        "y1 = f(X1).ravel()\n",
        "\n",
        "# Mesh the input space for evaluations of the real function, the prediction and\n",
        "# its MSE\n",
        "x = np.atleast_2d(np.linspace(0, 10, 1000)).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8wu9MdzHZp"
      },
      "source": [
        "# Instantiate a Gaussian Process model\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\n",
        "gp1 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
        "\n",
        "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
        "gp1.fit(X1, y1)\n",
        "\n",
        "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
        "y_pred1, sigma1 = gp1.predict(x, return_std=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6lhVwJpy-rc"
      },
      "source": [
        "# Plot the function, the prediction and the 95% confidence interval based on\n",
        "# the MSE\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "plt.plot(x, f(x), 'r:', label=r'$f(x) = x\\,\\sin(x)$')\n",
        "plt.plot(X1, y1, 'r.', markersize=10, label='Observations')\n",
        "plt.plot(x, y_pred1, 'b-', label='Prediction')\n",
        "plt.fill(np.concatenate([x, x[::-1]]),\n",
        "         np.concatenate([y_pred1 - 1.9600 * sigma1,\n",
        "                        (y_pred1 + 1.9600 * sigma1)[::-1]]),\n",
        "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.ylim(-10, 15) ###\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JckFNRpHy-m-"
      },
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# now the noisy case\n",
        "X2 = np.linspace(0.1, 9.9, 20)\n",
        "X2 = np.atleast_2d(X2).T\n",
        "\n",
        "# Observations and noise\n",
        "y2 = f(X2).ravel()\n",
        "dy = 0.5 + 1.0 * np.random.random(y2.shape)\n",
        "noise = np.random.normal(0, dy)\n",
        "y2 += noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zenHlvc9y-kr"
      },
      "source": [
        "# Instantiate a Gaussian Process model\n",
        "gp2 = GaussianProcessRegressor(kernel=kernel, alpha=dy ** 2,\n",
        "                              n_restarts_optimizer=10)\n",
        "\n",
        "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
        "gp2.fit(X2, y2)\n",
        "\n",
        "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
        "y_pred2, sigma2 = gp2.predict(x, return_std=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fg1bpBkdK5A"
      },
      "source": [
        "# Plot the function, the prediction and the 95% confidence interval based on\n",
        "# the MSE\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "plt.plot(x, f(x), 'r:', label=r'$f(x) = x\\,\\sin(x)$')\n",
        "plt.errorbar(X2.ravel(), y2, dy, fmt='r.', markersize=10, label='Observations')\n",
        "plt.plot(x, y_pred2, 'b-', label='Prediction')\n",
        "plt.fill(np.concatenate([x, x[::-1]]),\n",
        "         np.concatenate([y_pred2 - 1.9600 * sigma2,\n",
        "                        (y_pred2 + 1.9600 * sigma2)[::-1]]),\n",
        "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$f(x)$')\n",
        "plt.ylim(-10, 15) ###\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8CKKAj9dK2u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t98vmnIodK0p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBR1Z9TyRK_"
      },
      "source": [
        "#### <font color=green>**1.2.** </font> Gaussian process regression (GPR) with noise-level estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFheUpvlE9z2"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy.html\n",
        "\n",
        "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "#\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l6MsZ4gyWfl"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, WhiteKernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Lsb3ceyWcY"
      },
      "source": [
        "rng = np.random.RandomState(0)\n",
        "X = rng.uniform(0, 5, 20)[:, np.newaxis]\n",
        "y = 0.5 * np.sin(3 * X[:, 0]) + rng.normal(0, 0.5, X.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNGaXV6VyWZr"
      },
      "source": [
        "# First run\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "kernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "    + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
        "gp = GaussianProcessRegressor(kernel=kernel,\n",
        "                              alpha=0.0).fit(X, y)\n",
        "X_ = np.linspace(0, 5, 100)\n",
        "y_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)\n",
        "plt.plot(X_, y_mean, 'k', lw=3, zorder=9)\n",
        "plt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),\n",
        "                 y_mean + np.sqrt(np.diag(y_cov)),\n",
        "                 alpha=0.5, color='k')\n",
        "plt.plot(X_, 0.5*np.sin(3*X_), 'r', lw=3, zorder=9)\n",
        "plt.scatter(X[:, 0], y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))\n",
        "plt.title(\"Initial: %s\\nOptimum: %s\\nLog-Marginal-Likelihood: %s\"\n",
        "          % (kernel, gp.kernel_,\n",
        "             gp.log_marginal_likelihood(gp.kernel_.theta)))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITvuPCL7yWXK"
      },
      "source": [
        "# Second run\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
        "    + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))\n",
        "gp = GaussianProcessRegressor(kernel=kernel,\n",
        "                              alpha=0.0).fit(X, y)\n",
        "X_ = np.linspace(0, 5, 100)\n",
        "y_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)\n",
        "plt.plot(X_, y_mean, 'k', lw=3, zorder=9)\n",
        "plt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),\n",
        "                 y_mean + np.sqrt(np.diag(y_cov)),\n",
        "                 alpha=0.5, color='k')\n",
        "plt.plot(X_, 0.5*np.sin(3*X_), 'r', lw=3, zorder=9)\n",
        "plt.scatter(X[:, 0], y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))\n",
        "plt.title(\"Initial: %s\\nOptimum: %s\\nLog-Marginal-Likelihood: %s\"\n",
        "          % (kernel, gp.kernel_,\n",
        "             gp.log_marginal_likelihood(gp.kernel_.theta)))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9dnDH7St7gl"
      },
      "source": [
        "# Plot LML landscape\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "theta0 = np.logspace(-2, 3, 49)\n",
        "theta1 = np.logspace(-2, 0, 50)\n",
        "Theta0, Theta1 = np.meshgrid(theta0, theta1)\n",
        "LML = [[gp.log_marginal_likelihood(np.log([0.36, Theta0[i, j], Theta1[i, j]]))\n",
        "        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]\n",
        "LML = np.array(LML).T\n",
        "\n",
        "vmin, vmax = (-LML).min(), (-LML).max()\n",
        "vmax = 50\n",
        "level = np.around(np.logspace(np.log10(vmin), np.log10(vmax), 50), decimals=1)\n",
        "plt.contour(Theta0, Theta1, -LML,\n",
        "            levels=level, norm=LogNorm(vmin=vmin, vmax=vmax))\n",
        "plt.colorbar()\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xlabel(\"Length-scale\")\n",
        "plt.ylabel(\"Noise-level\")\n",
        "plt.title(\"Log-marginal-likelihood\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR7tXUDzyWUX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv5Y5i1vkjY8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iesSPGBcvgU8"
      },
      "source": [
        "#### <font color=green>**1.3.** </font> Comparison of kernel ridge and Gaussian process regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhDYgZr4E3eB"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html\n",
        "\n",
        "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAojCEfIunB6"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNpYUyFum_b"
      },
      "source": [
        "rng = np.random.RandomState(0)\n",
        "\n",
        "# Generate sample data\n",
        "X = 15 * rng.rand(100, 1)\n",
        "y = np.sin(X).ravel()\n",
        "y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECtBWF57um8k"
      },
      "source": [
        "# Fit KernelRidge with parameter selection based on 5-fold cross validation\n",
        "param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n",
        "              \"kernel\": [ExpSineSquared(l, p)\n",
        "                         for l in np.logspace(-2, 2, 10)\n",
        "                         for p in np.logspace(0, 2, 10)]}\n",
        "kr = GridSearchCV(KernelRidge(), param_grid=param_grid)\n",
        "stime = time.time()\n",
        "kr.fit(X, y)\n",
        "print(\"Time for KRR fitting: %.3f\" % (time.time() - stime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpZqVTjQum5K"
      },
      "source": [
        "gp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) \\\n",
        "    + WhiteKernel(1e-1)\n",
        "gpr = GaussianProcessRegressor(kernel=gp_kernel)\n",
        "stime = time.time()\n",
        "gpr.fit(X, y)\n",
        "print(\"Time for GPR fitting: %.3f\" % (time.time() - stime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuhj-7Ztum23"
      },
      "source": [
        "# Predict using kernel ridge\n",
        "X_plot = np.linspace(0, 20, 10000)[:, None]\n",
        "stime = time.time()\n",
        "y_kr = kr.predict(X_plot)\n",
        "print(\"Time for KRR prediction: %.3f\" % (time.time() - stime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIMnVPM-vyp2"
      },
      "source": [
        "# Predict using gaussian process regressor\n",
        "stime = time.time()\n",
        "y_gpr = gpr.predict(X_plot, return_std=False)\n",
        "print(\"Time for GPR prediction: %.3f\" % (time.time() - stime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfRAyfRGvym-"
      },
      "source": [
        "stime = time.time()\n",
        "y_gpr, y_std = gpr.predict(X_plot, return_std=True)\n",
        "print(\"Time for GPR prediction with standard-deviation: %.3f\"\n",
        "      % (time.time() - stime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl0SeRQQum02"
      },
      "source": [
        "# Plot results\n",
        "plt.figure(figsize=(12, 8)) ###\n",
        "lw = 2\n",
        "plt.scatter(X, y, c='k', label='data')\n",
        "plt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')\n",
        "plt.plot(X_plot, y_kr, color='turquoise', lw=lw,\n",
        "         label='KRR (%s)' % kr.best_params_)\n",
        "plt.plot(X_plot, y_gpr, color='darkorange', lw=lw,\n",
        "         label='GPR (%s)' % gpr.kernel_)\n",
        "plt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color='darkorange',\n",
        "                 alpha=0.2)\n",
        "plt.xlabel('data')\n",
        "plt.ylabel('target')\n",
        "plt.xlim(0, 20)\n",
        "plt.ylim(-4, 4)\n",
        "plt.title('GPR versus Kernel Ridge')\n",
        "plt.legend(loc=\"best\",  scatterpoints=1, prop={'size': 8})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFfpZb0qumy6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Cj8cTbumsq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9d0538e"
      },
      "source": [
        "#### <font color=green>**1.4.** </font> Basic (Gaussian likelihood) GP regression model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsDU-20mshI"
      },
      "source": [
        "We focus here on the implementation of the models in GPflow; for more intuition on these models, see [A Practical Guide to Gaussian Processes](https://drafts.distill.pub/gp/) and [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:41.979311Z",
          "iopub.status.busy": "2021-04-08T13:07:41.977866Z",
          "iopub.status.idle": "2021-04-08T13:07:44.780945Z",
          "shell.execute_reply": "2021-04-08T13:07:44.781439Z"
        },
        "id": "00ef38ed"
      },
      "source": [
        "import gpflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from gpflow.utilities import print_summary\n",
        "\n",
        "# The lines below are specific to the notebook format\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7afab839"
      },
      "source": [
        "`X` and `Y` denote the input and output values. **NOTE:** `X` and `Y` must be two-dimensional NumPy arrays, $N \\times 1$ or $N \\times D$, where $D$ is the number of input dimensions/features, with the same number of rows as $N$ (one for each data point):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:44.788279Z",
          "iopub.status.busy": "2021-04-08T13:07:44.787464Z",
          "iopub.status.idle": "2021-04-08T13:07:44.924578Z",
          "shell.execute_reply": "2021-04-08T13:07:44.924101Z"
        },
        "id": "933e6a41"
      },
      "source": [
        "data = np.genfromtxt(\"https://raw.githubusercontent.com/GPflow/docs/develop/doc/source/notebooks/basics/data/regression_1D.csv\", delimiter=\",\")\n",
        "X = data[:, 0].reshape(-1, 1)\n",
        "Y = data[:, 1].reshape(-1, 1)\n",
        "\n",
        "_ = plt.plot(X, Y, \"kx\", mew=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e43e6d6"
      },
      "source": [
        "We will consider the following probabilistic model:\n",
        "\\begin{equation}\n",
        "Y_i = f(X_i) + \\varepsilon_i\\,,\n",
        "\\end{equation}\n",
        "where $f \\sim \\mathcal{GP}(\\mu(\\cdot), k(\\cdot, \\cdot'))$, and $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2 I)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b12e0a2"
      },
      "source": [
        "##### Choose a kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:44.941782Z",
          "iopub.status.busy": "2021-04-08T13:07:44.941285Z",
          "iopub.status.idle": "2021-04-08T13:07:44.951632Z",
          "shell.execute_reply": "2021-04-08T13:07:44.951210Z"
        },
        "id": "a27ec3a5"
      },
      "source": [
        "'''Several kernels (covariance functions) are implemented in GPflow. \n",
        "You can easily combine them to create new ones (see Manipulating kernels). \n",
        "You can also implement new covariance functions, as shown in the Kernel design notebook. \n",
        "Here, we will use a simple one:\n",
        "'''\n",
        "\n",
        "k = gpflow.kernels.Matern52() ###\n",
        "\n",
        "# 用意されている kernel : https://gpflow.readthedocs.io/en/develop/gpflow/kernels/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfjLTPlBm-U8"
      },
      "source": [
        "'''For more advanced kernels see the advanced kernel notebook\n",
        "(including kernels defined on subspaces).\n",
        "A summary of the kernel can be obtained by\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:44.957487Z",
          "iopub.status.busy": "2021-04-08T13:07:44.956987Z",
          "iopub.status.idle": "2021-04-08T13:07:44.964446Z",
          "shell.execute_reply": "2021-04-08T13:07:44.964054Z"
        },
        "id": "0ba2139e"
      },
      "source": [
        "print_summary(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRUhrthncIt"
      },
      "source": [
        "'''The Matern 5/2 kernel has two parameters: \n",
        "`lengthscales`, which encodes the \"wiggliness\" of the GP, and `variance`, which tunes the amplitude. \n",
        "They are both set to 1.0 as the default value. \n",
        "For more details on the meaning of the other columns, see Manipulating kernels.\n",
        "\n",
        "## Choose a mean function (optional)\n",
        "It is common to choose $\\mu = 0$, which is the GPflow default.\n",
        "However, if there is a clear pattern (such as a mean value of `Y` that is \n",
        "far away from 0, or a linear trend in the data), mean functions can  be beneficial. \n",
        "Some simple ones are provided in the `gpflow.mean_functions` module.\n",
        "Here's how to define a linear mean function:\n",
        "`meanf = gpflow.mean_functions.Linear()`\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54cd5e28"
      },
      "source": [
        "##### Construct a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:44.970149Z",
          "iopub.status.busy": "2021-04-08T13:07:44.969692Z",
          "iopub.status.idle": "2021-04-08T13:07:44.988930Z",
          "shell.execute_reply": "2021-04-08T13:07:44.988400Z"
        },
        "id": "e72262e0"
      },
      "source": [
        "'''A GPflow model is created by instantiating one of the GPflow model classes, in this case GPR. \n",
        "We'll make a kernel `k` and instantiate a GPR object using the generated data and the kernel. \n",
        "We'll also set the variance of the likelihood to a sensible initial guess.\n",
        "'''\n",
        "\n",
        "m = gpflow.models.GPR(data=(X, Y), kernel=k, mean_function=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:44.994310Z",
          "iopub.status.busy": "2021-04-08T13:07:44.993848Z",
          "iopub.status.idle": "2021-04-08T13:07:45.000894Z",
          "shell.execute_reply": "2021-04-08T13:07:45.000376Z"
        },
        "id": "9fe96e55"
      },
      "source": [
        "# A summary of the model can be obtained by\n",
        "print_summary(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:45.006698Z",
          "iopub.status.busy": "2021-04-08T13:07:45.005934Z",
          "iopub.status.idle": "2021-04-08T13:07:45.012838Z",
          "shell.execute_reply": "2021-04-08T13:07:45.011996Z"
        },
        "id": "c5b60e01"
      },
      "source": [
        "# The first two lines correspond to the kernel parameters, \n",
        "# and the third one gives the likelihood parameter (the noise variance $\\tau^2$ in our model).\n",
        "\n",
        "# You can access those values and manually set them to sensible initial guesses.\n",
        "# For example:\n",
        "m.likelihood.variance.assign(0.01)\n",
        "m.kernel.lengthscales.assign(0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf44106f"
      },
      "source": [
        "##### Optimize the model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SbH6Op7oLGA"
      },
      "source": [
        "To obtain meaningful predictions, you need to tune the model parameters (that is, the parameters of the kernel, the likelihood, and the mean function if applicable) to the data at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hY-UmXjoOrD"
      },
      "source": [
        "There are several optimizers available in GPflow. Here we use the `Scipy` optimizer, which by default implements the L-BFGS-B algorithm. \\\n",
        "(You can select other algorithms by using the `method=` keyword argument to its `minimize` method; see [the SciPy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) for details of available options.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:45.018344Z",
          "iopub.status.busy": "2021-04-08T13:07:45.017843Z",
          "iopub.status.idle": "2021-04-08T13:07:45.020589Z",
          "shell.execute_reply": "2021-04-08T13:07:45.020129Z"
        },
        "lines_to_next_cell": 2,
        "id": "5b28ebe0"
      },
      "source": [
        "opt = gpflow.optimizers.Scipy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb7c806"
      },
      "source": [
        "In order to train the model, we need to maximize the log marginal likelihood.\\\n",
        "GPflow models define a `training_loss` that can be passed to the `minimize` method of an optimizer; in this case it is simply the negative log marginal likelihood.\\\n",
        "We also need to specify the variables to train with\n",
        "`m.trainable_variables`, and the number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:45.070770Z",
          "iopub.status.busy": "2021-04-08T13:07:45.059420Z",
          "iopub.status.idle": "2021-04-08T13:07:46.924546Z",
          "shell.execute_reply": "2021-04-08T13:07:46.924937Z"
        },
        "id": "e75d3aea"
      },
      "source": [
        "opt_logs = opt.minimize(m.training_loss, m.trainable_variables, options=dict(maxiter=100))\n",
        "print_summary(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AsI90CXojko"
      },
      "source": [
        "'''Notice how the value column has changed.\n",
        "\n",
        "The local optimum found by Maximum Likelihood might not be the one you want \n",
        "(for example, it might be overfitting or oversmooth). \n",
        "\n",
        "This depends on the initial values of the hyperparameters, and is specific to each dataset. \n",
        "As an alternative to Maximum Likelihood, Markov Chain Monte Carlo (MCMC) is also available.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72c11cb1"
      },
      "source": [
        "##### Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3R8cNeJot6N"
      },
      "source": [
        "We can now use the model to make some predictions at the new points `Xnew`.\\\n",
        "You might be interested in predicting two different quantities: the latent function values `f(Xnew)` (the denoised signal), or the values of new observations `y(Xnew)` (signal + noise).\\\n",
        "Because we are dealing with Gaussian probabilistic models, the predictions typically produce a mean and variance as output.\\\n",
        "Alternatively, you can obtain samples of `f(Xnew)` or the log density of the new data points `(Xnew, Ynew)`.\n",
        "\n",
        "GPflow models have several prediction methods:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31028b5c"
      },
      "source": [
        " - `m.predict_f` returns the mean and marginal variance of $f$ at the points `Xnew`.\n",
        "\n",
        " - `m.predict_f` with argument `full_cov=True` returns the mean and the full covariance matrix of $f$ at the points `Xnew`.\n",
        "\n",
        " - `m.predict_f_samples` returns samples of the latent function.\n",
        "\n",
        " - `m.predict_y` returns the mean and variance of a new data point (that is, it includes the noise variance).\n",
        "\n",
        " - `m.predict_log_density` returns the log density of the observations `Ynew` at `Xnew`.\n",
        "\n",
        "We use `predict_f` and `predict_f_samples` to plot 95% confidence intervals and samples from the posterior distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:46.932318Z",
          "iopub.status.busy": "2021-04-08T13:07:46.931833Z",
          "iopub.status.idle": "2021-04-08T13:07:47.163700Z",
          "shell.execute_reply": "2021-04-08T13:07:47.164060Z"
        },
        "lines_to_next_cell": 2,
        "id": "a638c9fa"
      },
      "source": [
        "## generate test points for prediction\n",
        "xx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)  # test points must be of shape (N, D)\n",
        "\n",
        "## predict mean and variance of latent GP at test points\n",
        "mean, var = m.predict_f(xx)\n",
        "\n",
        "## generate 10 samples from posterior\n",
        "tf.random.set_seed(1)  # for reproducibility\n",
        "samples = m.predict_f_samples(xx, 10)  # shape (10, 100, 1)\n",
        "\n",
        "## plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(X, Y, \"kx\", mew=2)\n",
        "plt.plot(xx, mean, \"C0\", lw=2)\n",
        "plt.fill_between(\n",
        "    xx[:, 0],\n",
        "    mean[:, 0] - 1.96 * np.sqrt(var[:, 0]),\n",
        "    mean[:, 0] + 1.96 * np.sqrt(var[:, 0]),\n",
        "    color=\"C0\",\n",
        "    alpha=0.2,\n",
        ")\n",
        "\n",
        "plt.plot(xx, samples[:, :, 0].numpy().T, \"C0\", linewidth=0.5)\n",
        "_ = plt.xlim(-0.1, 1.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8pQADMgo9h0"
      },
      "source": [
        "'''## GP regression in higher dimensions\n",
        "\n",
        "Very little changes when the input space has more than one dimension. \n",
        "By default, the `lengthscales` is an isotropic (scalar) parameter. \n",
        "It is generally recommended that you allow to tune a different lengthscale \n",
        "for each dimension (Automatic Relevance Determination, ARD): \n",
        "simply initialize `lengthscales` with an array of length $D$ corresponding to the input dimension of `X`.  \n",
        "See Manipulating kernels for further information.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4StrMpPdLbt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhgjGZQtlmRh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKosQ7jQTbk2"
      },
      "source": [
        "#### <font color=green>**1.5.** </font> kernel ごとの比較"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AMj_Cb_EuIb"
      },
      "source": [
        "## 参考 : Illustration of prior and posterior Gaussian process for different kernels\n",
        "##       https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkeVT6CNdMWS"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n",
        "                                              ExpSineSquared, DotProduct,\n",
        "                                              ConstantKernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD49PIKgdMT-"
      },
      "source": [
        "kernels = [RBF(length_scale=1.0, \n",
        "               length_scale_bounds=(0.1, 10.0)),\n",
        "           RationalQuadratic(length_scale=1.0, \n",
        "                             alpha=0.1),\n",
        "           ExpSineSquared(length_scale=1.0, \n",
        "                          periodicity=1.0,\n",
        "                          length_scale_bounds=(0.1, 10.0),\n",
        "                          periodicity_bounds=(0.1, 10.0)),\n",
        "           Matern(length_scale=1.0, \n",
        "                  length_scale_bounds=(1e-1, 10.0),\n",
        "                  nu=1.0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnsAClnrdMR4"
      },
      "source": [
        "X_line = np.linspace(0, 5, 100)\n",
        "rn = np.random.RandomState(4)\n",
        "\n",
        "X_train = rn.uniform(0, 5, 80)[:, np.newaxis]   ## 60 -> 80\n",
        "Y_train = np.tan((X_train[:, 0] - 2.5) **2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljcIOcCkdMPX"
      },
      "source": [
        "def draw_graph(gauss, L):\n",
        "  Y_mean, Y_std = gauss.predict(X_line[:, np.newaxis], return_std=True)\n",
        "  plt.plot(X_line, Y_mean, 'b', lw=2, zorder=1)\n",
        "  plt.fill_between(X_line, Y_mean - Y_std, Y_mean + Y_std,\n",
        "                     alpha=0.2, color='k')\n",
        "  Y_samples = gauss.sample_y(X_line[:, np.newaxis], 5)\n",
        "  plt.plot(X_line, Y_samples, lw=0.5)\n",
        "  plt.xlim(0, 5)\n",
        "  plt.ylim(-3, 3)\n",
        "  plt.scatter(X_train[:L, 0], Y_train[:L], c='r', s=50, zorder=10, edgecolors=(0, 0, 0))\n",
        "  plt.plot(X_line, np.tan((X_line- 2.5)**2), 'r', lw=2, zorder=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msXtqjlLdMC9"
      },
      "source": [
        "for kernel in kernels:\n",
        "  gp = GaussianProcessRegressor(kernel=kernel)\n",
        "  plt.figure(figsize=(50, 5))\n",
        "\n",
        "  plt.subplot(1, 5, 1)\n",
        "  draw_graph(gp,0)\n",
        "\n",
        "  for i in range(1,5):    ## range(1,4) -> range(1,5)\n",
        "    gp.fit(X_train[:20*i], Y_train[:20*i])\n",
        "    plt.subplot(1, 5, 1+i)\n",
        "    draw_graph(gp,20*i)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tUDJR_idMM_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5Uz6Y-5dMK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlXF_xV4i5yD"
      },
      "source": [
        "#### <font color=green>**1.6.** </font> scikit-learn ver. GPR on Mauna Loa CO2 data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9Fa2axEEkBX"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html\n",
        "\n",
        "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "#\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSqxWWURV4C3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels \\\n",
        "    import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z13Qn42VV4Au"
      },
      "source": [
        "def load_mauna_loa_atmospheric_co2():\n",
        "  ml_data = fetch_openml(data_id=41187, as_frame=False)\n",
        "  months = []\n",
        "  ppmv_sums = []\n",
        "  counts = []\n",
        "\n",
        "  y = ml_data.data[:, 0]\n",
        "  m = ml_data.data[:, 1]\n",
        "  month_float = y + (m - 1) / 12\n",
        "  ppmvs = ml_data.target\n",
        "\n",
        "  for month, ppmv in zip(month_float, ppmvs):\n",
        "    if not months or month != months[-1]:\n",
        "      months.append(month)\n",
        "      ppmv_sums.append(ppmv)\n",
        "      counts.append(1)\n",
        "    else:\n",
        "      # aggregate monthly sum to produce average\n",
        "      ppmv_sums[-1] += ppmv\n",
        "      counts[-1] += 1\n",
        "\n",
        "  months = np.asarray(months).reshape(-1, 1)\n",
        "  avg_ppmvs = np.asarray(ppmv_sums) / counts\n",
        "  return months, avg_ppmvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "597kDvwwV3-T"
      },
      "source": [
        "X, y = load_mauna_loa_atmospheric_co2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HM9X4M2V38T"
      },
      "source": [
        "# Kernel with parameters given in GPML book\n",
        "k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend\n",
        "k2 = 2.4**2 * RBF(length_scale=90.0) \\\n",
        "    * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # seasonal component\n",
        "\n",
        "# medium term irregularity\n",
        "k3 = 0.66**2 \\\n",
        "    * RationalQuadratic(length_scale=1.2, alpha=0.78)\n",
        "k4 = 0.18**2 * RBF(length_scale=0.134) \\\n",
        "    + WhiteKernel(noise_level=0.19**2)  # noise terms\n",
        "\n",
        "kernel_gpml = k1 + k2 + k3 + k4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAVO4JSpjaus"
      },
      "source": [
        "gp = GaussianProcessRegressor(kernel=kernel_gpml, alpha=0,\n",
        "                              optimizer=None, normalize_y=True)\n",
        "gp.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4_J2KopjasI"
      },
      "source": [
        "print(\"GPML kernel: %s\" % gp.kernel_)\n",
        "print(\"Log-marginal-likelihood: %.3f\"\n",
        "      % gp.log_marginal_likelihood(gp.kernel_.theta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhyUrrthlZob"
      },
      "source": [
        "X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\n",
        "y_pred, y_std = gp.predict(X_, return_std=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbJNh-F6lZoz"
      },
      "source": [
        "# Illustration\n",
        "plt.figure(figsize=(18,12))\n",
        "plt.scatter(X, y, c='k', s=10)\n",
        "plt.plot(X_, y_pred, linewidth=1)\n",
        "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
        "                 alpha=0.5, color='pink')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBQTCQp_laWU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VWAw4xujfaG"
      },
      "source": [
        "# Kernel with optimized parameters\n",
        "k1_2 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
        "k2_2 = 2.0**2 * RBF(length_scale=100.0) \\\n",
        "    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n",
        "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
        "\n",
        "# medium term irregularities\n",
        "k3_2 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "k4_2 = 0.1**2 * RBF(length_scale=0.1) \\\n",
        "    + WhiteKernel(noise_level=0.1**2,\n",
        "                  noise_level_bounds=(1e-5, np.inf))  # noise terms\n",
        "\n",
        "kernel_2 = k1_2 + k2_2 + k3_2 + k4_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aenVuY_jfV7"
      },
      "source": [
        "gp2 = GaussianProcessRegressor(kernel=kernel_2, alpha=0,\n",
        "                              normalize_y=True)\n",
        "gp2.fit(X, y)\n",
        "\n",
        "print(\"\\nLearned kernel: %s\" % gp2.kernel_)\n",
        "print(\"Log-marginal-likelihood: %.3f\"\n",
        "      % gp2.log_marginal_likelihood(gp2.kernel_.theta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCW1ZETpjf34"
      },
      "source": [
        "X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\n",
        "y_pred2, y_std2 = gp2.predict(X_, return_std=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5_2fgaV3yu"
      },
      "source": [
        "# Illustration\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.scatter(X, y, c='k')\n",
        "plt.plot(X_, y_pred2)\n",
        "plt.fill_between(X_[:, 0], y_pred2 - y_std2, y_pred2 + y_std2,\n",
        "                 alpha=0.5, color='k')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btaJe96fnZV_"
      },
      "source": [
        "plt.figure(figsize=(18,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(X, y, c='k', s=5)\n",
        "plt.plot(X_, y_pred, linewidth=1)\n",
        "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
        "                 alpha=0.5, color='pink')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Kernel with parameters given in GPML book\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(X, y, c='k', s=5)\n",
        "plt.plot(X_, y_pred2, linewidth=1)\n",
        "plt.fill_between(X_[:, 0], y_pred2 - y_std2, y_pred2 + y_std2,\n",
        "                 alpha=0.5, color='pink')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Kernel with optimized parameters\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do_9i1HKoNig"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "#plt.scatter(X, y, c='k', s=5)\n",
        "plt.plot(X_, y_pred, color='r')\n",
        "plt.plot(X_, y_pred2, color='b')\n",
        "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
        "                 alpha=0.3, color='green')\n",
        "plt.fill_between(X_[:, 0], y_pred2 - y_std2, y_pred2 + y_std2,\n",
        "                 alpha=0.5, color='orange')\n",
        "\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M09d4K-tcRWk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6g1gbsbXhB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP58PCBKV0o_"
      },
      "source": [
        "#### <font color=green>**1.7.** </font> GPflow ver. Fitting a Gaussian process kernel (Mauna Loa CO2 data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c1zqRPSEhA6"
      },
      "source": [
        "## 出典 : https://peterroelants.github.io/posts/gaussian-process-kernel-fitting/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Aq0CiEUTiGS"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import warnings\n",
        "from itertools import islice\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tqdm import tqdm\n",
        "import bokeh\n",
        "import bokeh.io\n",
        "import bokeh.plotting\n",
        "import bokeh.models\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdIYR-K6pZDn"
      },
      "source": [
        "warnings.simplefilter(\"ignore\")\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "bokeh.io.output_notebook(hide_banner=True)\n",
        "\n",
        "\n",
        "tfb = tfp.bijectors\n",
        "tfd = tfp.distributions\n",
        "tfk = tfp.math.psd_kernels\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RLrmLhBV4ux"
      },
      "source": [
        "# Load the data from the Scripps CO2 program website. \n",
        "co2_df = pd.read_csv(\n",
        "    'https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv',\n",
        "    #'./monthly_in_situ_co2_mlo.csv', \n",
        "    header=54, # Data starts here\n",
        "    skiprows=[55, 56], # Headers consist of multiple rows\n",
        "    usecols=[3, 4], # Only keep the 'Date' and 'CO2' columns\n",
        "    names=['Date', 'CO2'],\n",
        "    na_values='-99.99',  # NaNs are denoted as '-99.99'\n",
        "    dtype=np.float64\n",
        ")\n",
        "\n",
        "# Drop missing values\n",
        "co2_df.dropna(inplace=True)\n",
        "# Remove whitespace from column names\n",
        "co2_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAMZKN9EV4s4"
      },
      "source": [
        "# Plot data\n",
        "fig = bokeh.plotting.figure(\n",
        "    width=600, height=300, \n",
        "    x_range=(1958, 2020), y_range=(310, 420))\n",
        "fig.xaxis.axis_label = 'Date'\n",
        "fig.yaxis.axis_label = 'CO₂ (ppm)'\n",
        "fig.add_layout(bokeh.models.Title(\n",
        "    text='In situ air measurements at Mauna Loa, Observatory, Hawaii',\n",
        "    text_font_style=\"italic\"), 'above')\n",
        "fig.add_layout(bokeh.models.Title(\n",
        "    text='Atmospheric CO₂ concentrations', \n",
        "    text_font_size=\"14pt\"), 'above')\n",
        "fig.line(co2_df.Date, co2_df.CO2,\n",
        "         legend_label='All data',\n",
        "         line_width=2, line_color='midnightblue')\n",
        "fig.legend.location = 'top_left'\n",
        "fig.toolbar.autohide = True\n",
        "bokeh.plotting.show(fig)\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwpwU37V4qC"
      },
      "source": [
        "# Split the data into observed and to predict\n",
        "date_split_predict = 2008\n",
        "df_observed = co2_df[co2_df.Date < date_split_predict]\n",
        "print('{} measurements in the observed set'.format(len(df_observed)))\n",
        "df_predict = co2_df[co2_df.Date >= date_split_predict]\n",
        "print('{} measurements in the test set'.format(len(df_predict)))\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP-AeSFkV4l7"
      },
      "source": [
        "# Define mean function which is the means of observations\n",
        "observations_mean = tf.constant(\n",
        "    [np.mean(df_observed.CO2.values)], dtype=tf.float64)\n",
        "mean_fn = lambda _: observations_mean\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz1lME5zfCFc"
      },
      "source": [
        "# Define the kernel with trainable parameters. \n",
        "# Note we transform some of the trainable variables to ensure\n",
        "#  they stay positive.\n",
        "\n",
        "# Use float64 because this means that the kernel matrix will have \n",
        "#  less numerical issues when computing the Cholesky decomposition\n",
        "\n",
        "# Constrain to make sure certain parameters are strictly positive\n",
        "constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3TDOn93fCBx"
      },
      "source": [
        "# Smooth kernel hyperparameters\n",
        "smooth_amplitude = tfp.util.TransformedVariable(\n",
        "    initial_value=10., bijector=constrain_positive, dtype=np.float64,\n",
        "    name='smooth_amplitude')\n",
        "smooth_length_scale = tfp.util.TransformedVariable(\n",
        "    initial_value=10., bijector=constrain_positive, dtype=np.float64,\n",
        "    name='smooth_length_scale')\n",
        "\n",
        "# Smooth kernel\n",
        "smooth_kernel = tfk.ExponentiatedQuadratic(\n",
        "    amplitude=smooth_amplitude, \n",
        "    length_scale=smooth_length_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_mImaYpfI6o"
      },
      "source": [
        "# Local periodic kernel hyperparameters\n",
        "periodic_amplitude = tfp.util.TransformedVariable(\n",
        "    initial_value=5.0, bijector=constrain_positive, dtype=np.float64,\n",
        "    name='periodic_amplitude')\n",
        "periodic_length_scale = tfp.util.TransformedVariable(\n",
        "    initial_value=1.0, bijector=constrain_positive, dtype=np.float64,\n",
        "    name='periodic_length_scale')\n",
        "periodic_period = tfp.util.TransformedVariable(\n",
        "    initial_value=1.0, bijector=constrain_positive, dtype=np.float64,\n",
        "    name='periodic_period')\n",
        "periodic_local_length_scale = tfp.util.TransformedVariable(\n",
        "    initial_value=1.0, bijector=constrain_positive, dtype=np.float64,\n",
        "    name='periodic_local_length_scale')\n",
        "\n",
        "# Local periodic kernel\n",
        "local_periodic_kernel = (\n",
        "    tfk.ExpSinSquared(\n",
        "        amplitude=periodic_amplitude, \n",
        "        length_scale=periodic_length_scale,\n",
        "        period=periodic_period) * \n",
        "    tfk.ExponentiatedQuadratic(\n",
        "        length_scale=periodic_local_length_scale))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drzYqg_9fI4F"
      },
      "source": [
        "# Short-medium term irregularities kernel hyperparameters\n",
        "irregular_amplitude = tfp.util.TransformedVariable(\n",
        "    initial_value=1., bijector=constrain_positive, dtype=np.float64,\n",
        "    name='irregular_amplitude')\n",
        "irregular_length_scale = tfp.util.TransformedVariable(\n",
        "    initial_value=1., bijector=constrain_positive, dtype=np.float64,\n",
        "    name='irregular_length_scale')\n",
        "irregular_scale_mixture = tfp.util.TransformedVariable(\n",
        "    initial_value=1., bijector=constrain_positive, dtype=np.float64,\n",
        "    name='irregular_scale_mixture')\n",
        "\n",
        "# Short-medium term irregularities kernel\n",
        "irregular_kernel = tfk.RationalQuadratic(\n",
        "    amplitude=irregular_amplitude,\n",
        "    length_scale=irregular_length_scale,\n",
        "    scale_mixture_rate=irregular_scale_mixture)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKdhIMQsfI19"
      },
      "source": [
        "# Noise variance of observations\n",
        "# Start out with a medium-to high noise\n",
        "observation_noise_variance = tfp.util.TransformedVariable(\n",
        "    initial_value=1, bijector=constrain_positive, dtype=np.float64,\n",
        "    name='observation_noise_variance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8u_1H5KV4jq"
      },
      "source": [
        "trainable_variables = [v.variables[0] for v in [\n",
        "    smooth_amplitude,\n",
        "    smooth_length_scale,\n",
        "    periodic_amplitude,\n",
        "    periodic_length_scale,\n",
        "    periodic_period,\n",
        "    periodic_local_length_scale,\n",
        "    irregular_amplitude,\n",
        "    irregular_length_scale,\n",
        "    irregular_scale_mixture,\n",
        "    observation_noise_variance\n",
        "]]\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojqnk7pBV4hl"
      },
      "source": [
        "# Sum all kernels to single kernel containing all characteristics\n",
        "kernel = (smooth_kernel + local_periodic_kernel + irregular_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8yfmIG3V4fP"
      },
      "source": [
        "# Define mini-batch data iterator\n",
        "batch_size = 128\n",
        "\n",
        "batched_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (df_observed.Date.values.reshape(-1, 1), df_observed.CO2.values))\n",
        "    .shuffle(buffer_size=len(df_observed))\n",
        "    .repeat(count=None)\n",
        "    .batch(batch_size)\n",
        ")\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzKOBQ7PV4dQ"
      },
      "source": [
        "@tf.function(autograph=False, experimental_compile=False)  # Use tf.function for more effecient function evaluation\n",
        "def gp_loss_fn(index_points, observations):\n",
        "  \"\"\"Gaussian process negative-log-likelihood loss function.\"\"\"\n",
        "  gp = tfd.GaussianProcess(\n",
        "      mean_fn=mean_fn,\n",
        "      kernel=kernel,\n",
        "      index_points=index_points,\n",
        "      observation_noise_variance=observation_noise_variance\n",
        "      )\n",
        "    \n",
        "  negative_log_likelihood = -gp.log_prob(observations)\n",
        "  return negative_log_likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5dr8HFzfapb"
      },
      "source": [
        "# Fit hyperparameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BB1yIfjV4bZ"
      },
      "source": [
        "# Training loop\n",
        "batch_nlls = []  # Batch NLL for plotting\n",
        "full_ll = []  # Full data NLL for plotting\n",
        "nb_iterations = 10001\n",
        "for i, (index_points_batch, observations_batch) in tqdm(enumerate(islice(batched_dataset, nb_iterations)), file=sys.stdout):\n",
        "  # Run optimization for single batch\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = gp_loss_fn(index_points_batch, observations_batch)\n",
        "  grads = tape.gradient(loss, trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, trainable_variables))\n",
        "  batch_nlls.append((i, loss.numpy()))\n",
        "  # Evaluate on all observations\n",
        "  if i % 100 == 0:\n",
        "    # Evaluate on all observed data\n",
        "    ll = gp_loss_fn(\n",
        "        index_points=df_observed.Date.values.reshape(-1, 1),\n",
        "        observations=df_observed.CO2.values)\n",
        "    full_ll.append((i, ll.numpy()))\n",
        "\n",
        "# 2分半くらいかかる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtxUiIhoV4ZM"
      },
      "source": [
        "# Plot NLL over iterations\n",
        "fig = bokeh.plotting.figure(width=600, height=400, \n",
        "                            x_range=(0, nb_iterations), y_range=(50, 200))\n",
        "fig.add_layout(bokeh.models.Title(\n",
        "    text='Negative Log-Likelihood (NLL) during training', \n",
        "    text_font_size=\"14pt\"), 'above')\n",
        "fig.xaxis.axis_label = 'iteration'\n",
        "fig.yaxis.axis_label = 'NLL batch'\n",
        "\n",
        "# First plot\n",
        "fig.line(*zip(*batch_nlls), legend_label='Batch data',\n",
        "         line_width=2, line_color='midnightblue')\n",
        "\n",
        "# Seoncd plot\n",
        "# Setting the second y axis range name and range\n",
        "fig.extra_y_ranges = {'fig1ax2': bokeh.models.Range1d(start=130, end=250)}\n",
        "fig.line(*zip(*full_ll), legend_label='All observed data',\n",
        "         line_width=2, line_color='red', y_range_name='fig1ax2')\n",
        "# Adding the second axis to the plot.  \n",
        "fig.add_layout(bokeh.models.LinearAxis(\n",
        "    y_range_name='fig1ax2', axis_label='NLL all'), 'right')\n",
        "\n",
        "fig.legend.location = 'top_right'\n",
        "fig.toolbar.autohide = True\n",
        "bokeh.plotting.show(fig)\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoXJtwNDV4W2"
      },
      "source": [
        "# Show values of parameters found\n",
        "variables = [\n",
        "    smooth_amplitude,\n",
        "    smooth_length_scale,\n",
        "    periodic_amplitude,\n",
        "    periodic_length_scale,\n",
        "    periodic_period,\n",
        "    periodic_local_length_scale,\n",
        "    irregular_amplitude,\n",
        "    irregular_length_scale,\n",
        "    irregular_scale_mixture,\n",
        "    observation_noise_variance\n",
        "]\n",
        "\n",
        "data = list([(var.variables[0].name[:-2], var.numpy()) for var in variables])\n",
        "df_variables = pd.DataFrame(\n",
        "    data, columns=['Hyperparameters', 'Value'])\n",
        "display(HTML(df_variables.to_html(\n",
        "    index=False, float_format=lambda x: f'{x:.4f}')))\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSlg1tHuV4UF"
      },
      "source": [
        "# Posterior GP using fitted kernel and observed data\n",
        "gp_posterior_predict = tfd.GaussianProcessRegressionModel(\n",
        "    mean_fn=mean_fn,\n",
        "    kernel=kernel,\n",
        "    index_points=df_predict.Date.values.reshape(-1, 1),\n",
        "    observation_index_points=df_observed.Date.values.reshape(-1, 1),\n",
        "    observations=df_observed.CO2.values,\n",
        "    observation_noise_variance=observation_noise_variance)\n",
        "\n",
        "# Posterior mean and standard deviation\n",
        "posterior_mean_predict = gp_posterior_predict.mean()\n",
        "posterior_std_predict = gp_posterior_predict.stddev()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqyLIwWvV4Ro"
      },
      "source": [
        "# Plot posterior predictions\n",
        "\n",
        "# Get posterior predictions\n",
        "μ = posterior_mean_predict.numpy()\n",
        "σ = posterior_std_predict.numpy()\n",
        "\n",
        "# Plot\n",
        "fig = bokeh.plotting.figure(\n",
        "    width=600, height=400,\n",
        "    x_range=(2008, 2021), y_range=(380, 415))\n",
        "fig.xaxis.axis_label = 'Date'\n",
        "fig.yaxis.axis_label = 'CO₂ (ppm)'\n",
        "fig.add_layout(bokeh.models.Title(\n",
        "    text='Posterior predictions conditioned on observations before 2008.',\n",
        "    text_font_style=\"italic\"), 'above')\n",
        "fig.add_layout(bokeh.models.Title(\n",
        "    text='Atmospheric CO₂ concentrations', \n",
        "    text_font_size=\"14pt\"), 'above')\n",
        "fig.circle(\n",
        "    co2_df.Date, co2_df.CO2, legend_label='True data',\n",
        "    size=2, line_color='midnightblue')\n",
        "fig.line(\n",
        "    df_predict.Date.values, μ, legend_label='μ (predictions)',\n",
        "    line_width=2, line_color='firebrick')\n",
        "# Prediction interval\n",
        "band_x = np.append(\n",
        "    df_predict.Date.values, df_predict.Date.values[::-1])\n",
        "band_y = np.append(\n",
        "    (μ + 2*σ), (μ - 2*σ)[::-1])\n",
        "fig.patch(\n",
        "    band_x, band_y, color='firebrick', alpha=0.4, \n",
        "    line_color='firebrick', legend_label='2σ')\n",
        "\n",
        "fig.legend.location = 'top_left'\n",
        "fig.toolbar.autohide = True\n",
        "bokeh.plotting.show(fig)\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0zJis1iV4Po"
      },
      "source": [
        "# Version info\n",
        "'''Python: 3.8.5\n",
        "Numpy: 1.18.5\n",
        "Pandas: 1.1.3\n",
        "TensorFlow: 2.3.1\n",
        "TensorFlow Probability: 0.11.1\n",
        "Bokeh: 2.2.2\n",
        "'''\n",
        "\n",
        "print('Python: {}.{}.{}'.format(*sys.version_info[:3]))\n",
        "print('Numpy: {}'.format(np.__version__))\n",
        "print('Pandas: {}'.format(pd.__version__))\n",
        "print('TensorFlow: {}'.format(tf.__version__))\n",
        "print('TensorFlow Probability: {}'.format(tfp.__version__))\n",
        "print('Bokeh: {}'.format(bokeh.__version__))\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB4PNVk3V4HV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT6KIIIqV4FF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCtivAHtcRl7"
      },
      "source": [
        "### <font color=blue>**2.** </font> ガウス過程　関連度自動決定（GPARD）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptNDeaGJWktQ"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL6mCRLQdMI9"
      },
      "source": [
        "diabetes = load_diabetes()\n",
        "\n",
        "sc = preprocessing.StandardScaler()\n",
        "sc.fit(diabetes.data)\n",
        "diabetes_data = sc.transform(diabetes.data)\n",
        "diabetes_target = diabetes.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRD4o-KdMGm"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(diabetes_data, diabetes.target)\n",
        "\n",
        "X=[i for i in range(10)]\n",
        "Y_train = Y_train[:, np.newaxis]\n",
        "n_features=len(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UICbyBf6dL-8"
      },
      "source": [
        "import GPy\n",
        "\n",
        "# カーネルを定義する。\n",
        "kernel = GPy.kern.RBF(input_dim=n_features, variance=0.01, ARD=True)\n",
        "# kernel = GPy.kern.Matern52(n_features, ARD=True)\n",
        "\n",
        "# ガウス過程を用いた回帰を行う。\n",
        "model = GPy.models.GPRegression(X_train, Y_train, kernel)\n",
        "\n",
        "# 最適化（MAP推定）を行う。\n",
        "model.optimize(messages=True, max_iters=1e5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luhJ-jrQdL8q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習後のスケール長を取り出す。\n",
        "ls = list(model.kern.lengthscale)\n",
        "\n",
        "# その逆数が説明変数の寄与の大きさ\n",
        "weights = [1 / v for v in ls]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.ylabel(\"$1/l_m$\")\n",
        "plt.xlabel(\"$m$\")\n",
        "xs = list(range(len(weights)))\n",
        "plt.bar(xs, weights)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VSb1ov5dL2o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9YNyW3tccQX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm2CJ-cdbZaj"
      },
      "source": [
        "### <font color=blue>**3.** </font> ガウス過程分類（GPC）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki9f7MIiunda"
      },
      "source": [
        "#### <font color=green>**3.1.** </font> Gaussian process classification (GPC) on iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1VFRiLjD3Zv"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_iris.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6y0CFM3uwkh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqIt-5bzuwhp"
      },
      "source": [
        "# import some data to play with\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "y = np.array(iris.target, dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3hy0c6-uwfW"
      },
      "source": [
        "kernel = 1.0 * RBF([1.0])\n",
        "gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n",
        "kernel = 1.0 * RBF([1.0, 1.0])\n",
        "gpc_rbf_anisotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrklLd6ku5f-"
      },
      "source": [
        "h = .02  # step size in the mesh\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA081TXkunGA"
      },
      "source": [
        "titles = [\"Isotropic RBF\", \"Anisotropic RBF\"]\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):\n",
        "  # Plot the predicted probabilities. For that, we will assign a color to\n",
        "  # each point in the mesh [x_min, m_max]x[y_min, y_max].\n",
        "  plt.subplot(1, 2, i + 1)\n",
        "  Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "  # Put the result into a color plot\n",
        "  Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n",
        "  plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin=\"lower\")\n",
        "\n",
        "  # Plot also the training points\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=np.array([\"r\", \"g\", \"b\"])[y],\n",
        "              edgecolors=(1, 1, 1)  ## (0, 0, 0) -> (1, 1, 1)\n",
        "              )\n",
        "  plt.xlabel('Sepal length')\n",
        "  plt.ylabel('Sepal width')\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.title(\"%s, LML: %.3f\" %\n",
        "              (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta)))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTa5Rumdu5dd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSIC9eSSunD-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY2BsOXvv_4R"
      },
      "source": [
        "#### <font color=green>**3.2.** </font> Iso-probability lines for Gaussian Processes classification (GPC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh0l37CgD6_R"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_isoprobability.html\n",
        "\n",
        "# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n",
        "# Adapted to GaussianProcessClassifier:\n",
        "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSG4Q6tnwsiM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import DotProduct, ConstantKernel as C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOlP7F28wsdT"
      },
      "source": [
        "def g(x):\n",
        "  \"\"\"The function to predict (classification will then consist in predicting\n",
        "  whether g(x) <= 0 or not)\"\"\"\n",
        "  return 5. - x[:, 1] - .5 * x[:, 0] ** 2."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjdBLWVlwsaz"
      },
      "source": [
        "# A few constants\n",
        "lim = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5vOuudgwsXc"
      },
      "source": [
        "# Design of experiments\n",
        "X = np.array([[-4.61611719, -6.00099547],\n",
        "              [4.10469096, 5.32782448],\n",
        "              [0.00000000, -0.50000000],\n",
        "              [-6.17289014, -4.6984743],\n",
        "              [1.3109306, -6.93271427],\n",
        "              [-5.03823144, 3.10584743],\n",
        "              [-2.87600388, 6.74310541],\n",
        "              [5.21301203, 4.26386883]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNbLNVaHwsVN"
      },
      "source": [
        "# Observations\n",
        "y = np.array(g(X) > 0, dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi5xri1Yw6vg"
      },
      "source": [
        "# Instantiate and fit Gaussian Process Model\n",
        "kernel = C(0.1, (1e-5, np.inf)) * DotProduct(sigma_0=0.1) ** 2\n",
        "gp = GaussianProcessClassifier(kernel=kernel)\n",
        "gp.fit(X, y)\n",
        "print(\"Learned kernel: %s \" % gp.kernel_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Qn24axw6tV"
      },
      "source": [
        "# Evaluate real function and the predicted probability\n",
        "res = 50\n",
        "x1, x2 = np.meshgrid(np.linspace(- lim, lim, res),\n",
        "                     np.linspace(- lim, lim, res))\n",
        "xx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n",
        "\n",
        "y_true = g(xx)\n",
        "y_prob = gp.predict_proba(xx)[:, 1]\n",
        "y_true = y_true.reshape((res, res))\n",
        "y_prob = y_prob.reshape((res, res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx-7y1QywJe8"
      },
      "source": [
        "# Plot the probabilistic classification iso-values\n",
        "fig = plt.figure(1, figsize=(10,10))  ###\n",
        "ax = fig.gca()\n",
        "ax.axes.set_aspect('equal')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "\n",
        "cax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8, extent=(-lim, lim, -lim, lim))\n",
        "norm = plt.matplotlib.colors.Normalize(vmin=0., vmax=0.9)\n",
        "cb = plt.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)\n",
        "cb.set_label(r'${\\rm \\mathbb{P}}\\left[\\widehat{G}(\\mathbf{x}) \\leq 0\\right]$')\n",
        "plt.clim(0, 1)\n",
        "\n",
        "plt.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)\n",
        "plt.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)\n",
        "\n",
        "plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')\n",
        "\n",
        "cs = plt.contour(x1, x2, y_prob, [0.666], colors='b', linestyles='solid')\n",
        "plt.clabel(cs, fontsize=11)\n",
        "\n",
        "cs = plt.contour(x1, x2, y_prob, [0.5], colors='k', linestyles='dashed')\n",
        "plt.clabel(cs, fontsize=11)\n",
        "\n",
        "cs = plt.contour(x1, x2, y_prob, [0.334], colors='r', linestyles='solid')\n",
        "plt.clabel(cs, fontsize=11)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyrxrL8Bw6q2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FooAtuOKwJcw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh7OpkjIxpeH"
      },
      "source": [
        "#### <font color=green>**3.3.** </font> Probabilistic predictions with Gaussian process classification (GPC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "941fkjDmEAO6"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc.html\n",
        "\n",
        "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "#\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvQJxDzTwJab"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTBX1g-qwF3m"
      },
      "source": [
        "# Generate data\n",
        "train_size = 50\n",
        "rng = np.random.RandomState(0)\n",
        "X = rng.uniform(0, 5, 100)[:, np.newaxis]\n",
        "y = np.array(X[:, 0] > 2.5, dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bw9N_GlwF0c"
      },
      "source": [
        "# Specify Gaussian Processes with fixed and optimized hyperparameters\n",
        "gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0),\n",
        "                                   optimizer=None)\n",
        "gp_fix.fit(X[:train_size], y[:train_size])\n",
        "\n",
        "gp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
        "gp_opt.fit(X[:train_size], y[:train_size])\n",
        "\n",
        "print(\"Log Marginal Likelihood (initial): %.3f\"\n",
        "      % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta))\n",
        "print(\"Log Marginal Likelihood (optimized): %.3f\"\n",
        "      % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta))\n",
        "\n",
        "print(\"Accuracy: %.3f (initial) %.3f (optimized)\"\n",
        "      % (accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),\n",
        "         accuracy_score(y[:train_size], gp_opt.predict(X[:train_size]))))\n",
        "print(\"Log-loss: %.3f (initial) %.3f (optimized)\"\n",
        "      % (log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),\n",
        "         log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCO8qyBHwFyZ"
      },
      "source": [
        "# Plot posteriors\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "plt.scatter(X[:train_size, 0], y[:train_size], c='k', label=\"Train data\",\n",
        "            edgecolors=(0, 0, 0))\n",
        "plt.scatter(X[train_size:, 0], y[train_size:], c='g', label=\"Test data\",\n",
        "            edgecolors=(0, 0, 0))\n",
        "X_ = np.linspace(0, 5, 100)\n",
        "plt.plot(X_, gp_fix.predict_proba(X_[:, np.newaxis])[:, 1], 'r',\n",
        "         label=\"Initial kernel: %s\" % gp_fix.kernel_)\n",
        "plt.plot(X_, gp_opt.predict_proba(X_[:, np.newaxis])[:, 1], 'b',\n",
        "         label=\"Optimized kernel: %s\" % gp_opt.kernel_)\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Class 1 probability\")\n",
        "plt.xlim(0, 5)\n",
        "plt.ylim(-0.25, 1.5)\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q62K1vS7t7pJ"
      },
      "source": [
        "# Plot LML landscape\n",
        "plt.figure(figsize=(12,8))  ###\n",
        "theta0 = np.logspace(0, 8, 30)\n",
        "theta1 = np.logspace(-1, 1, 29)\n",
        "Theta0, Theta1 = np.meshgrid(theta0, theta1)\n",
        "LML = [[gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))\n",
        "        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]\n",
        "LML = np.array(LML).T\n",
        "plt.plot(np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1],\n",
        "         'ko', zorder=10)\n",
        "plt.plot(np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1],\n",
        "         'ko', zorder=10)\n",
        "plt.pcolor(Theta0, Theta1, LML)\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Magnitude\")\n",
        "plt.ylabel(\"Length-scale\")\n",
        "plt.title(\"Log-marginal-likelihood\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMKMBMJ0t7rx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBeNWU3It7mT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MCMuabKt79T"
      },
      "source": [
        "#### <font color=green>**3.4.** </font> Illustration of Gaussian process classification (GPC) on the XOR dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHH-JzJZuGf4"
      },
      "source": [
        "## 出典 : https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpc_xor.html\n",
        "\n",
        "# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
        "#\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHwQ8UPuGc2"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF, DotProduct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onvAjQRBuGaB"
      },
      "source": [
        "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
        "                     np.linspace(-3, 3, 50))\n",
        "rng = np.random.RandomState(0)\n",
        "X = rng.randn(200, 2)\n",
        "Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MOMIPNcLasr"
      },
      "source": [
        "'''\n",
        "排他的論理和 xor （exclusive or）とは、\n",
        "1 xor 1 = 0 \n",
        "1 xor 0 = 1\n",
        "0 xor 1 = 1\n",
        "0 xor 0 = 0\n",
        "と定められるものである。\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkkcysAOuGXP"
      },
      "source": [
        "kernels = [1.0 * RBF(length_scale=1.0), 1.0 * DotProduct(sigma_0=1.0)**2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzVMHKnLuGUk"
      },
      "source": [
        "# fit the model\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, kernel in enumerate(kernels):\n",
        "  clf = GaussianProcessClassifier(kernel=kernel, warm_start=True).fit(X, Y)\n",
        "\n",
        "  # plot the decision function for each datapoint on the grid\n",
        "  Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\n",
        "  Z = Z.reshape(xx.shape)\n",
        "\n",
        "  plt.subplot(1, 2, i + 1)\n",
        "  image = plt.imshow(Z, interpolation='nearest',\n",
        "                       extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "                       aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)\n",
        "  contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2,\n",
        "                           colors=['k'])\n",
        "  plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,\n",
        "                edgecolors=(0, 0, 0))\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "  plt.axis([-3, 3, -3, 3])\n",
        "  plt.colorbar(image)\n",
        "  plt.title(\"%s\\n Log-Marginal-Likelihood:%.3f\"\n",
        "          % (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),\n",
        "            fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0jtNkjWuGSd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nsqpYm6t7uA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSjRp70iNcu"
      },
      "source": [
        "#### <font color=green>**3.5.** </font> Basic (binary) GP classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWgPkIWGuiox"
      },
      "source": [
        "## 出典 : https://gpflow.readthedocs.io/en/develop/notebooks/basics/classification.html\n",
        "\n",
        "'''This notebook shows how to build a GP classification model using variational inference.\n",
        "Here we consider binary (two-class, 0 vs. 1) classification only (there is a separate notebook on multiclass classification.\n",
        "We first look at a one-dimensional example, and then show how you can adapt this when the input space is two-dimensional.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:06:37.187740Z",
          "iopub.status.busy": "2021-04-08T13:06:37.185480Z",
          "iopub.status.idle": "2021-04-08T13:06:50.681467Z",
          "shell.execute_reply": "2021-04-08T13:06:50.682691Z"
        },
        "id": "d9e97a9a"
      },
      "source": [
        "import numpy as np\n",
        "import gpflow\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "393f7dea"
      },
      "source": [
        "##### One-dimensional example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuTFhsZfuu_c"
      },
      "source": [
        "## First of all, let's have a look at the data. `X` and `Y` denote the input and output values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0khOJTCu47Z"
      },
      "source": [
        "**NOTE:** `X` and `Y` must be two-dimensional NumPy arrays, $N \\times 1$ or $N \\times D$, where $D$ is the number of input dimensions/features, with the same number of rows as $N$ (one for each data point):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:06:50.692126Z",
          "iopub.status.busy": "2021-04-08T13:06:50.687886Z",
          "iopub.status.idle": "2021-04-08T13:06:51.189322Z",
          "shell.execute_reply": "2021-04-08T13:06:51.188513Z"
        },
        "id": "e6c06c51"
      },
      "source": [
        "X = np.genfromtxt(\"https://raw.githubusercontent.com/GPflow/docs/develop/doc/source/notebooks/basics/data/classif_1D_X.csv\").reshape(-1, 1)\n",
        "Y = np.genfromtxt(\"https://raw.githubusercontent.com/GPflow/docs/develop/doc/source/notebooks/basics/data/classif_1D_Y.csv\").reshape(-1, 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "_ = plt.plot(X, Y, \"C3x\", ms=8, mew=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lafwnNn4vBse"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdUaIzR4vBM8"
      },
      "source": [
        "## Reminders on GP classification\n",
        "\n",
        "# For a binary classification model using GPs, we can simply use a `Bernoulli` likelihood. \n",
        "# The details of the generative model are as follows:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41391cc5"
      },
      "source": [
        "__1. Define the latent GP:__ we start from a Gaussian process $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot'))$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:06:51.198194Z",
          "iopub.status.busy": "2021-04-08T13:06:51.194686Z",
          "iopub.status.idle": "2021-04-08T13:06:52.971775Z",
          "shell.execute_reply": "2021-04-08T13:06:52.972449Z"
        },
        "lines_to_next_cell": 1,
        "id": "410718e1"
      },
      "source": [
        "# build the kernel and covariance matrix\n",
        "k = gpflow.kernels.Matern52(variance=20.0)\n",
        "x_grid = np.linspace(0, 6, 200).reshape(-1, 1)\n",
        "K = k(x_grid)\n",
        "\n",
        "# sample from a multivariate normal\n",
        "rng = np.random.RandomState(6)\n",
        "\n",
        "L = np.linalg.cholesky(K)\n",
        "f_grid = np.dot(L, rng.randn(200, 5))\n",
        "plt.plot(x_grid, f_grid, \"C0\", linewidth=1)\n",
        "_ = plt.plot(x_grid, f_grid[:, 1], \"C0\", linewidth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce58b3b2"
      },
      "source": [
        "__2. Squash them to $[0, 1]$:__ the samples of the GP are mapped to $[0, 1]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Nm3iqrvhbY"
      },
      "source": [
        "By default, GPflow uses the standard normal cumulative distribution function (inverse probit function): $p(x) = \\Phi(f(x)) = \\frac{1}{2} (1 + \\operatorname{erf}(x / \\sqrt{2}))$.\\\n",
        "This choice has the advantage that predictive mean, variance and density can be computed analytically, but any choice of invlink is possible, e.g. the logit $p(x) = \\frac{\\exp(f(x))}{1 + \\exp(f(x))}$.\\\n",
        "Simply pass another function as the `invlink` argument to the `Bernoulli` likelihood class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:06:52.987082Z",
          "iopub.status.busy": "2021-04-08T13:06:52.985928Z",
          "iopub.status.idle": "2021-04-08T13:06:53.469845Z",
          "shell.execute_reply": "2021-04-08T13:06:53.480995Z"
        },
        "id": "70acc693"
      },
      "source": [
        "def invlink(f):\n",
        "    return gpflow.likelihoods.Bernoulli().invlink(f).numpy()\n",
        "\n",
        "\n",
        "p_grid = invlink(f_grid)\n",
        "plt.plot(x_grid, p_grid, \"C1\", linewidth=1)\n",
        "_ = plt.plot(x_grid, p_grid[:, 1], \"C1\", linewidth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1767dcb3"
      },
      "source": [
        "__3. Sample from a Bernoulli:__ for each observation point $X_i$, the class label $Y_i \\in \\{0, 1\\}$ is generated by sampling from a Bernoulli distribution $Y_i \\sim \\mathcal{B}(g(X_i))$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:06:53.488426Z",
          "iopub.status.busy": "2021-04-08T13:06:53.486192Z",
          "iopub.status.idle": "2021-04-08T13:06:54.066535Z",
          "shell.execute_reply": "2021-04-08T13:06:54.067713Z"
        },
        "id": "4bf2dd0f"
      },
      "source": [
        "# Select some input locations\n",
        "ind = rng.randint(0, 200, (30,))\n",
        "X_gen = x_grid[ind]\n",
        "\n",
        "# evaluate probability and get Bernoulli draws\n",
        "p = p_grid[ind, 1:2]\n",
        "Y_gen = rng.binomial(1, p)\n",
        "\n",
        "# plot\n",
        "plt.plot(x_grid, p_grid[:, 1], \"C1\", linewidth=2)\n",
        "plt.plot(X_gen, p, \"C1o\", ms=6)\n",
        "_ = plt.plot(X_gen, Y_gen, \"C3x\", ms=8, mew=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuReVuumvqH5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB6j-2vfvqBL"
      },
      "source": [
        "#### Implementation with GPflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bddf56f"
      },
      "source": [
        "For the model described above, the posterior $f(x)|Y$ (say $p$) is not Gaussian any more and does not have a closed-form expression.\\\n",
        "A common approach is then to look for the best approximation of this posterior by a tractable distribution (say $q$) such as a Gaussian distribution.\\\n",
        "In variational inference, the quality of an approximation is measured by the Kullback-Leibler divergence $\\mathrm{KL}[q \\| p]$.\n",
        "For more details on this model, see Nickisch and Rasmussen (2008)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGYLvDp8vw_x"
      },
      "source": [
        "The inference problem is thus turned into an optimization problem: finding the best parameters for $q$.\\\n",
        "In our case, we introduce $U \\sim \\mathcal{N}(q_\\mu, q_\\Sigma)$, and we choose $q$ to have the same distribution as $f | f(X) = U$.\\\n",
        "The parameters $q_\\mu$ and $q_\\Sigma$ can be seen as parameters of $q$, which can be optimized in order to minimise  $\\mathrm{KL}[q \\| p]$.\n",
        "\n",
        "This variational inference model is called `VGP` in GPflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:06:54.078745Z",
          "iopub.status.busy": "2021-04-08T13:06:54.077943Z",
          "iopub.status.idle": "2021-04-08T13:07:09.602168Z",
          "shell.execute_reply": "2021-04-08T13:07:09.602763Z"
        },
        "id": "99601050"
      },
      "source": [
        "m = gpflow.models.VGP(\n",
        "    (X, Y), likelihood=gpflow.likelihoods.Bernoulli(), kernel=gpflow.kernels.Matern52()\n",
        ")\n",
        "\n",
        "opt = gpflow.optimizers.Scipy()\n",
        "opt.minimize(m.training_loss, variables=m.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:09.701663Z",
          "iopub.status.busy": "2021-04-08T13:07:09.666002Z",
          "iopub.status.idle": "2021-04-08T13:07:09.769917Z",
          "shell.execute_reply": "2021-04-08T13:07:09.770559Z"
        },
        "id": "01b1ffc5"
      },
      "source": [
        "# We can now inspect the result of the optimization with `gpflow.utilities.print_summary(m)`:\n",
        "gpflow.utilities.print_summary(m, fmt=\"notebook\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0RedCdEv9yl"
      },
      "source": [
        "# In this table, the first two lines are associated with the kernel parameters, \n",
        "# and the last two correspond to the variational parameters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2e9719"
      },
      "source": [
        "**NOTE:** In practice, $q_\\Sigma$ is actually parameterized by its lower-triangular square root $q_\\Sigma = q_\\text{sqrt} q_\\text{sqrt}^T$ in order to ensure its positive-definiteness.\n",
        "\n",
        "For more details on how to handle models in GPflow (getting and setting parameters, fixing some of them during optimization, using priors, and so on), see Manipulating GPflow models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwN26uOEwGC2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e86r0BQwF_x"
      },
      "source": [
        "#### Predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac8ce475"
      },
      "source": [
        "Finally, we will see how to use model predictions to plot the resulting model.\\\n",
        "We will replicate the figures of the generative model above, but using the approximate posterior distribution given by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:09.775384Z",
          "iopub.status.busy": "2021-04-08T13:07:09.773878Z",
          "iopub.status.idle": "2021-04-08T13:07:10.774587Z",
          "shell.execute_reply": "2021-04-08T13:07:10.775202Z"
        },
        "id": "699c17d2"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# bubble fill the predictions\n",
        "mu, var = m.predict_f(x_grid)\n",
        "\n",
        "plt.fill_between(\n",
        "    x_grid.flatten(),\n",
        "    np.ravel(mu + 2 * np.sqrt(var)),\n",
        "    np.ravel(mu - 2 * np.sqrt(var)),\n",
        "    alpha=0.3,\n",
        "    color=\"C0\",\n",
        ")\n",
        "\n",
        "# plot samples\n",
        "tf.random.set_seed(6)\n",
        "samples = m.predict_f_samples(x_grid, 10).numpy().squeeze().T\n",
        "\n",
        "plt.plot(x_grid, samples, \"C0\", lw=1)\n",
        "\n",
        "# plot p-samples\n",
        "p = invlink(samples)\n",
        "plt.plot(x_grid, p, \"C1\", lw=1)\n",
        "\n",
        "# plot data\n",
        "plt.plot(X, Y, \"C3x\", ms=8, mew=2)\n",
        "plt.ylim((-3, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9c97b7d"
      },
      "source": [
        "##### Two-dimensional example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:10.779467Z",
          "iopub.status.busy": "2021-04-08T13:07:10.778400Z",
          "iopub.status.idle": "2021-04-08T13:07:11.278561Z",
          "shell.execute_reply": "2021-04-08T13:07:11.277725Z"
        },
        "id": "561494b0"
      },
      "source": [
        "# In this section we will use the following data:\n",
        "X = np.loadtxt(\"https://raw.githubusercontent.com/GPflow/docs/develop/doc/source/notebooks/basics/data/banana_X_train\", delimiter=\",\")\n",
        "Y = np.loadtxt(\"https://raw.githubusercontent.com/GPflow/docs/develop/doc/source/notebooks/basics/data/banana_Y_train\", delimiter=\",\").reshape(-1, 1)\n",
        "mask = Y[:, 0] == 1\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(X[mask, 0], X[mask, 1], \"oC0\", mew=0, alpha=0.5)\n",
        "_ = plt.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], \"oC1\", mew=0, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:11.282679Z",
          "iopub.status.busy": "2021-04-08T13:07:11.281995Z",
          "iopub.status.idle": "2021-04-08T13:07:29.094576Z",
          "shell.execute_reply": "2021-04-08T13:07:29.095037Z"
        },
        "id": "e2716076"
      },
      "source": [
        "# The model definition is the same as above; \n",
        "# the only important difference is that we now specify that the kernel operates over a two-dimensional input space:\n",
        "m = gpflow.models.VGP(\n",
        "    (X, Y), kernel=gpflow.kernels.SquaredExponential(), likelihood=gpflow.likelihoods.Bernoulli()\n",
        ")\n",
        "\n",
        "opt = gpflow.optimizers.Scipy()\n",
        "opt.minimize(\n",
        "    m.training_loss, variables=m.trainable_variables, options=dict(maxiter=25), method=\"L-BFGS-B\"\n",
        ")\n",
        "# in practice, the optimization needs around 250 iterations to converge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a35ea3db"
      },
      "source": [
        "We can now plot the predicted decision boundary between the two classes.\\\n",
        "To do so, we can equivalently plot the contour lines $E[f(x)|Y]=0$, or $E[g(f(x))|Y]=0.5$.\\\n",
        "We will do the latter, because it allows us to introduce the `predict_y` function, which returns the mean and variance at test points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:07:29.101418Z",
          "iopub.status.busy": "2021-04-08T13:07:29.100735Z",
          "iopub.status.idle": "2021-04-08T13:07:29.976162Z",
          "shell.execute_reply": "2021-04-08T13:07:29.976891Z"
        },
        "id": "5b4ac5ff"
      },
      "source": [
        "x_grid = np.linspace(-3, 3, 40)\n",
        "xx, yy = np.meshgrid(x_grid, x_grid)\n",
        "Xplot = np.vstack((xx.flatten(), yy.flatten())).T\n",
        "\n",
        "p, _ = m.predict_y(Xplot)  # here we only care about the mean\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.plot(X[mask, 0], X[mask, 1], \"oC0\", mew=0, alpha=0.5)\n",
        "plt.plot(X[np.logical_not(mask), 0], X[np.logical_not(mask), 1], \"oC1\", mew=0, alpha=0.5)\n",
        "\n",
        "_ = plt.contour(\n",
        "    xx,\n",
        "    yy,\n",
        "    p.numpy().reshape(*xx.shape),\n",
        "    [0.5],  # plot the p=0.5 contour line only\n",
        "    colors=\"k\",\n",
        "    linewidths=1.8,\n",
        "    zorder=100,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APGePoNQl9WB"
      },
      "source": [
        "## References\n",
        "# Hannes Nickisch and Carl Edward Rasmussen. 'Approximations for binary Gaussian process classification'. \n",
        "# Journal of Machine Learning Research 9(Oct):2035--2078, 2008."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7rAx2r0dLmt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP3DRt_OZg0V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmpaWSCacci_"
      },
      "source": [
        "### <font color=blue>**4.** </font> ガウス過程　潜在変数モデル（SVGP）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fd3FodCGKmy"
      },
      "source": [
        "#### <font color=green>**4.1.** </font> Stochastic Variational Inference for scalability with SVGP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39f43557"
      },
      "source": [
        "One of the main criticisms of Gaussian processes is their scalability to large datasets.\\\n",
        "In this notebook, we illustrate how to use the state-of-the-art Stochastic Variational Gaussian Process (SVGP) (*Hensman, et. al. 2013*) to overcome this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:04.070345Z",
          "iopub.status.busy": "2021-04-08T13:16:04.068623Z",
          "iopub.status.idle": "2021-04-08T13:16:06.942188Z",
          "shell.execute_reply": "2021-04-08T13:16:06.941403Z"
        },
        "lines_to_next_cell": 1,
        "id": "f7dd67f7"
      },
      "source": [
        "%matplotlib inline\n",
        "import itertools\n",
        "import numpy as np\n",
        "import time\n",
        "import gpflow\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from gpflow.ci_utils import ci_niter\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "# for reproducibility of this notebook:\n",
        "rng = np.random.RandomState(123)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NAGJ8cUrSMu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCYPd0iNrSI1"
      },
      "source": [
        "## Generating data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXpgALCGqAiA"
      },
      "source": [
        "For this notebook example, we generate 10,000 noisy observations from a test function:\n",
        "\\begin{equation}\n",
        "f(x) = \\sin(3\\pi x) + 0.3\\cos(9\\pi x) + \\frac{\\sin(7 \\pi x)}{2}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:06.948907Z",
          "iopub.status.busy": "2021-04-08T13:16:06.948220Z",
          "iopub.status.idle": "2021-04-08T13:16:06.951758Z",
          "shell.execute_reply": "2021-04-08T13:16:06.952283Z"
        },
        "id": "a23f495a"
      },
      "source": [
        "def func(x):\n",
        "    return np.sin(x * 3 * 3.14) + 0.3 * np.cos(x * 9 * 3.14) + 0.5 * np.sin(x * 7 * 3.14)\n",
        "\n",
        "\n",
        "N = 10000  # Number of training observations\n",
        "\n",
        "X = rng.rand(N, 1) * 2 - 1  # X values\n",
        "Y = func(X) + 0.2 * rng.randn(N, 1)  # Noisy Y values\n",
        "data = (X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:06.972270Z",
          "iopub.status.busy": "2021-04-08T13:16:06.971025Z",
          "iopub.status.idle": "2021-04-08T13:16:07.101867Z",
          "shell.execute_reply": "2021-04-08T13:16:07.102466Z"
        },
        "id": "17ba98bd"
      },
      "source": [
        "# We plot the data along with the noiseless generating function:\n",
        "plt.plot(X, Y, \"x\", alpha=0.2)\n",
        "Xt = np.linspace(-1.1, 1.1, 1000)[:, None]\n",
        "Yt = func(Xt)\n",
        "_ = plt.plot(Xt, Yt, c=\"k\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcWwDQH8rVSw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM4vQhg_rVO7"
      },
      "source": [
        "## Building the model\n",
        "'''The main idea behind SVGP is to approximate the true GP posterior \n",
        "with a GP conditioned on a small set of \"inducing\" values. \n",
        "This smaller set can be thought of as summarizing the larger dataset. \n",
        "For this example, we will select a set of 50 inducing locations that are initialized from the training dataset:\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:07.120167Z",
          "iopub.status.busy": "2021-04-08T13:16:07.119655Z",
          "iopub.status.idle": "2021-04-08T13:16:07.161602Z",
          "shell.execute_reply": "2021-04-08T13:16:07.161990Z"
        },
        "id": "0e73ed3a"
      },
      "source": [
        "M = 50  # Number of inducing locations\n",
        "\n",
        "kernel = gpflow.kernels.SquaredExponential()\n",
        "Z = X[:M, :].copy()  # Initialize inducing locations to the first M inputs in the dataset\n",
        "\n",
        "m = gpflow.models.SVGP(kernel, gpflow.likelihoods.Gaussian(), Z, num_data=N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98-npHnqra2_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B90FNXgrawj"
      },
      "source": [
        "## Likelihood computation: batch vs. minibatch\n",
        "# First we showcase the model's performance using the whole dataset to compute the ELBO."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:07.166308Z",
          "iopub.status.busy": "2021-04-08T13:16:07.165714Z",
          "iopub.status.idle": "2021-04-08T13:16:07.168215Z",
          "shell.execute_reply": "2021-04-08T13:16:07.167786Z"
        },
        "id": "ee39c54e"
      },
      "source": [
        "elbo = tf.function(m.elbo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:07.175572Z",
          "iopub.status.busy": "2021-04-08T13:16:07.175025Z",
          "iopub.status.idle": "2021-04-08T13:16:10.814803Z",
          "shell.execute_reply": "2021-04-08T13:16:10.814409Z"
        },
        "id": "82f2a472"
      },
      "source": [
        "# TensorFlow re-traces & compiles a `tf.function`-wrapped method at every call \n",
        "# if the arguments are numpy arrays instead of tf.Tensors. Hence:\n",
        "tensor_data = tuple(map(tf.convert_to_tensor, data))\n",
        "elbo(tensor_data)  # run it once to trace & compile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:10.820736Z",
          "iopub.status.busy": "2021-04-08T13:16:10.820125Z",
          "iopub.status.idle": "2021-04-08T13:16:18.099386Z",
          "shell.execute_reply": "2021-04-08T13:16:18.099885Z"
        },
        "id": "d9c11814"
      },
      "source": [
        "%%timeit\n",
        "elbo(tensor_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:18.109039Z",
          "iopub.status.busy": "2021-04-08T13:16:18.108561Z",
          "iopub.status.idle": "2021-04-08T13:16:18.126286Z",
          "shell.execute_reply": "2021-04-08T13:16:18.126665Z"
        },
        "id": "357854c4"
      },
      "source": [
        "# We can speed up this calculation by using minibatches of the data. \n",
        "# For this example, we use minibatches of size 100.\n",
        "\n",
        "minibatch_size = 100\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).repeat().shuffle(N)\n",
        "\n",
        "train_iter = iter(train_dataset.batch(minibatch_size))\n",
        "\n",
        "ground_truth = elbo(tensor_data).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:18.130930Z",
          "iopub.status.busy": "2021-04-08T13:16:18.130469Z",
          "iopub.status.idle": "2021-04-08T13:16:18.502373Z",
          "shell.execute_reply": "2021-04-08T13:16:18.502757Z"
        },
        "id": "f098a54e"
      },
      "source": [
        "%%timeit\n",
        "elbo(next(train_iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyGLZ92wrpKG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIhCZCmVrpHL"
      },
      "source": [
        "## Stochastical estimation of ELBO\n",
        "'''The minibatch estimate should be an unbiased estimator of the `ground_truth`. \n",
        "Here we show a histogram of the value from different evaluations, \n",
        "together with its mean and the ground truth. \n",
        "The small difference between the mean of the minibatch estimations and \n",
        "the ground truth shows that the minibatch estimator is working as expected.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:18.506788Z",
          "iopub.status.busy": "2021-04-08T13:16:18.506273Z",
          "iopub.status.idle": "2021-04-08T13:16:18.615957Z",
          "shell.execute_reply": "2021-04-08T13:16:18.615452Z"
        },
        "id": "f3a89712"
      },
      "source": [
        "evals = [elbo(minibatch).numpy() for minibatch in itertools.islice(train_iter, 100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:18.637792Z",
          "iopub.status.busy": "2021-04-08T13:16:18.632079Z",
          "iopub.status.idle": "2021-04-08T13:16:18.819936Z",
          "shell.execute_reply": "2021-04-08T13:16:18.820299Z"
        },
        "id": "fb7d9d80"
      },
      "source": [
        "plt.hist(evals, label=\"Minibatch estimations\")\n",
        "plt.axvline(ground_truth, c=\"k\", label=\"Ground truth\")\n",
        "plt.axvline(np.mean(evals), c=\"g\", ls=\"--\", label=\"Minibatch mean\")\n",
        "plt.legend()\n",
        "plt.title(\"Histogram of ELBO evaluations using minibatches\")\n",
        "print(\"Discrepancy between ground truth and minibatch estimate:\", ground_truth - np.mean(evals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-hqvdMxr0qq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE5U-s8br0mf"
      },
      "source": [
        "## Minibatches speed up computation\n",
        "'''The reason for using minibatches is that it decreases the time needed to make an optimization step, \n",
        "because estimating the objective is computationally cheaper with fewer data points. \n",
        "Here we plot the change in time required with the size of the minibatch. \n",
        "We see that smaller minibatches result in a cheaper estimate of the objective.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:18.825955Z",
          "iopub.status.busy": "2021-04-08T13:16:18.825493Z",
          "iopub.status.idle": "2021-04-08T13:16:24.315135Z",
          "shell.execute_reply": "2021-04-08T13:16:24.315753Z"
        },
        "id": "44ea3164"
      },
      "source": [
        "# Evaluate objective for different minibatch sizes\n",
        "minibatch_proportions = np.logspace(-2, 0, 10)\n",
        "times = []\n",
        "objs = []\n",
        "for mbp in minibatch_proportions:\n",
        "    batchsize = int(N * mbp)\n",
        "    train_iter = iter(train_dataset.batch(batchsize))\n",
        "    start_time = time.time()\n",
        "    objs.append([elbo(minibatch) for minibatch in itertools.islice(train_iter, 20)])\n",
        "    times.append(time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:24.334247Z",
          "iopub.status.busy": "2021-04-08T13:16:24.332790Z",
          "iopub.status.idle": "2021-04-08T13:16:24.590141Z",
          "shell.execute_reply": "2021-04-08T13:16:24.590814Z"
        },
        "id": "9d46dca5"
      },
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "ax1.plot(minibatch_proportions, times, \"x-\")\n",
        "ax1.set_xlabel(\"Minibatch proportion\")\n",
        "ax1.set_ylabel(\"Time taken\")\n",
        "\n",
        "ax2.plot(minibatch_proportions, np.array(objs), \"kx\")\n",
        "ax2.set_xlabel(\"Minibatch proportion\")\n",
        "ax2.set_ylabel(\"ELBO estimates\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Xi-6iusJrb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXS8qDVgsJnb"
      },
      "source": [
        "## Running stochastic optimization\n",
        "# First we create a utility function that plots the model's predictions:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:24.608520Z",
          "iopub.status.busy": "2021-04-08T13:16:24.607629Z",
          "iopub.status.idle": "2021-04-08T13:16:24.826018Z",
          "shell.execute_reply": "2021-04-08T13:16:24.826694Z"
        },
        "id": "a4f6a38f"
      },
      "source": [
        "def plot(title=\"\"):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.title(title)\n",
        "    pX = np.linspace(-1, 1, 100)[:, None]  # Test locations\n",
        "    pY, pYv = m.predict_y(pX)  # Predict Y values at test locations\n",
        "    plt.plot(X, Y, \"x\", label=\"Training points\", alpha=0.2)\n",
        "    (line,) = plt.plot(pX, pY, lw=1.5, label=\"Mean of predictive posterior\")\n",
        "    col = line.get_color()\n",
        "    plt.fill_between(\n",
        "        pX[:, 0],\n",
        "        (pY - 2 * pYv ** 0.5)[:, 0],\n",
        "        (pY + 2 * pYv ** 0.5)[:, 0],\n",
        "        color=col,\n",
        "        alpha=0.6,\n",
        "        lw=1.5,\n",
        "    )\n",
        "    Z = m.inducing_variable.Z.numpy()\n",
        "    plt.plot(Z, np.zeros_like(Z), \"k|\", mew=2, label=\"Inducing locations\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "\n",
        "plot(title=\"Predictions before training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edF3TxDWsTnz"
      },
      "source": [
        "'''Now we can train our model. \n",
        "For optimizing the ELBO, we use the Adam Optimizer (Kingma and Ba 2015) \n",
        "which is designed for stochastic objective functions. \n",
        "We create a `run_adam` utility function  to perform the optimization.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:24.853685Z",
          "iopub.status.busy": "2021-04-08T13:16:24.852820Z",
          "iopub.status.idle": "2021-04-08T13:16:24.854575Z",
          "shell.execute_reply": "2021-04-08T13:16:24.855394Z"
        },
        "id": "d9cd3c17"
      },
      "source": [
        "minibatch_size = 100\n",
        "\n",
        "# We turn off training for inducing point locations\n",
        "gpflow.set_trainable(m.inducing_variable, False)\n",
        "\n",
        "\n",
        "def run_adam(model, iterations):\n",
        "    \"\"\"\n",
        "    Utility function running the Adam optimizer\n",
        "    \n",
        "    :param model: GPflow model\n",
        "    :param interations: number of iterations\n",
        "    \"\"\"\n",
        "    # Create an Adam Optimizer action\n",
        "    logf = []\n",
        "    train_iter = iter(train_dataset.batch(minibatch_size))\n",
        "    training_loss = model.training_loss_closure(train_iter, compile=True)\n",
        "    optimizer = tf.optimizers.Adam()\n",
        "\n",
        "    @tf.function\n",
        "    def optimization_step():\n",
        "        optimizer.minimize(training_loss, model.trainable_variables)\n",
        "\n",
        "    for step in range(iterations):\n",
        "        optimization_step()\n",
        "        if step % 10 == 0:\n",
        "            elbo = -training_loss().numpy()\n",
        "            logf.append(elbo)\n",
        "    return logf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:24.859947Z",
          "iopub.status.busy": "2021-04-08T13:16:24.859521Z",
          "iopub.status.idle": "2021-04-08T13:16:56.374053Z",
          "shell.execute_reply": "2021-04-08T13:16:56.373405Z"
        },
        "id": "6a6679d1"
      },
      "source": [
        "# Now we run the optimization loop for 20,000 iterations.\n",
        "\n",
        "maxiter = ci_niter(20000)\n",
        "\n",
        "logf = run_adam(m, maxiter)\n",
        "plt.plot(np.arange(maxiter)[::10], logf)\n",
        "plt.xlabel(\"iteration\")\n",
        "_ = plt.ylabel(\"ELBO\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-08T13:16:56.388525Z",
          "iopub.status.busy": "2021-04-08T13:16:56.382728Z",
          "iopub.status.idle": "2021-04-08T13:16:56.582843Z",
          "shell.execute_reply": "2021-04-08T13:16:56.583529Z"
        },
        "id": "f7624e15"
      },
      "source": [
        "# Finally, we plot the model's predictions.\n",
        "plot(\"Predictions after training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajPmbf9tgRZt"
      },
      "source": [
        "## References:\n",
        "# Hensman, James, Nicolo Fusi, and Neil D. Lawrence. \"Gaussian processes for big data.\" Uncertainty in Artificial Intelligence (2013).\n",
        "# Kingma, Diederik P., and Jimmy Ba. \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fyBwrTfV06r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG9_WM-mbXBu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiADcwBzGWGK"
      },
      "source": [
        "#### <font color=green>**4.2.** </font> Oil Flow Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YihWBC_5Hc4Z"
      },
      "source": [
        "## datasetの説明 : https://inverseprobability.com/3PhaseData.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZKjhT8Aq-qY"
      },
      "source": [
        "from pylab import *\n",
        "import matplotlib.mlab \n",
        "import numpy\n",
        "\n",
        "X = loadtxt('https://raw.githubusercontent.com/jiai-tus/FirstTerm/main/20210427/OilFlowData/DataTrn.txt')\n",
        "Y = loadtxt('https://raw.githubusercontent.com/jiai-tus/FirstTerm/main/20210427/OilFlowData/DataTrnLbls.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xwm3nmoIPjJ"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPj25QLmuPmy"
      },
      "source": [
        "Y= [Y[i,0]*0 +Y[i,1]*1+Y[i,2]*2 for i in range(len(Y)) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaYCZ3ujsGw9"
      },
      "source": [
        "from sklearn import datasets, preprocessing\n",
        "sc=preprocessing.StandardScaler()\n",
        "sc.fit(X)\n",
        "X=sc.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwyE4t-dsM77"
      },
      "source": [
        "import GPy\n",
        "\n",
        "input_dim =2\n",
        "kernel = GPy.kern.Linear(input_dim)+ GPy.kern.RBF(input_dim, ARD=True)+ GPy.kern.Matern52(input_dim, ARD=True) + GPy.kern.Bias(input_dim)+ GPy.kern.White(input_dim)\n",
        "model = GPy.models.GPLVM(X, input_dim=input_dim, kernel=kernel)\n",
        "model.optimize(messages=True, max_iters=1e3)\n",
        "\n",
        "# 7分くらいかかる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmb0aRS4sYW0"
      },
      "source": [
        "model.plot_latent(labels=Y,figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRLX3Z7Px67Q"
      },
      "source": [
        "kernels = [GPy.kern.Linear(input_dim),\n",
        "           GPy.kern.RBF(input_dim, ARD=True),\n",
        "           GPy.kern.Matern52(input_dim, ARD=True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yieifz6YxJvx"
      },
      "source": [
        "for i in kernels:\n",
        "  model = GPy.models.GPLVM(X, input_dim=input_dim, kernel=i)\n",
        "  model.optimize(messages=True, max_iters=1e3)\n",
        "  model.plot_latent(labels=Y,figsize=(10,10))\n",
        "\n",
        "# total 14分くらいかかる"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzFSykZgHGtZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uqi3hqdHGmn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}